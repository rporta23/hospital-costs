---
title: "Final Report"
author: "Lindsay Knupp"
date: "2024-05-14"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dataset

Our data was collected from [Centers for Medicare & Medicaid Services](https://data.cms.gov/provider-compliance/cost-report/hospital-provider-cost-report) on April 03, 2024. It features information about hospitals through annual cost reports in $2020$. We were interested in understanding how certain characteristics of hospitals like location, number of full time equivalent employees, or number of beds, for example, affected the hospitals' total operating costs. To perform qualitative analyses, we categorized each hospital as above or below the median. 

While the data is roughly annual, certain hospitals reported for different fiscal year lengths. To normalize, we divided some of our variables by the length of their cost reporting period to obtain daily estimates like average number of inpatients per day or average salary expense per day. Variables that are reported as "averages per day" are denoted with the word "average" in the predictor table below. Some hospitals were listed multiple times with distinct reporting periods. We learned that this could correspond to a change in control of the hospital. For example, the hospital could have been sold and transitioned from a voluntary to a governmental hospital. Duplicate hospitals were left in the dataset and a dummy variable, `duplicate` was added to indicate its status.  

There were $13$ different categories of control ranging from "Voluntary Non-Profit-Church" to "Governmental-Federal". To reduce our number of categories, we re-binned this variable to only include the broad categories: "Voluntary", "Proprietary", and "Governmental". We followed a similar procedure for the $12$ different categories of provider type ranging from "Children" to "Cancer" to "General Long Term". In this case, we re-binned provider type to only distinguish between "General" and "Specialized" care. We classified "General Short Term", "General Long Term", and "Religious Non-Medical Health Care Institution" as "General" care and classified "Cancer", "Psychiatric", "Rehabilitation",
"Children", "Reserved for Future Use", "Other", "Extended Neoplastic Disease Care", "Indian Health Services", and "Rural Emergency Hospital" as "Specialized" care. 

To improve accuracy on methods like Lasso regression, we scaled our numerical variables using the `scale()` function which centered and scaled our data appropriately. The following table includes the ranges of the predictors and response before they were re-scaled. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(knitr)
library(tidyverse)
library(kableExtra)
```

\renewcommand{\arraystretch}{2}
```{r,warning=FALSE,message=FALSE,echo=FALSE}
Variables = c("Number of Beds","FTE employees on payroll","Total hospital days",
                   "Total discharges","Total income","Total assets","Salaries","Inpatients",
                   "Rural versus Urban","Type of control","Type of provider","Duplicate hospital","Total costs","Costs bin")


Range = c("[1-2,791]","[0.05-26,941.09]",
          "[1-772,819]","[0.0027-462.63]","[-$6,129,919, $11,516,626]",
           "[-$636,856,458, $29,465,487,958] ","[$128.51, $9,032,294.85]","[0.0033-2123.13]",
           "[2487 rural, 3225 urban]","[2927 voluntary, 1728 proprietary, 1057 governmental]",
           "[4779 general, 993 specialized]","[132 duplicates, 5580 non duplicates ]","[$2,718.28, $16,000,980.58]","[2856 above median, 2856 below median]")

Descriptions = c("Total number of available beds including adult beds, pediatric beds, birthing room, and newborn ICU beds",
                 "Average number of full time-equivalent employees",
                 "Total number of inpatient days (i.e. days all patients spent in the hospital)",
                 "Average number of discharges including deaths",
                 "Average income including net revenue from services given to patients",
                 "Total current assets",
                 "Average salary expenses",
                 "Average number of inpatients",
                 "Location of hospital defined as rural or urban",
                 "Type of control under which hospital is conducted",
                 "Type of services provided",
                 "Whether or not hospital was listed multiple times",
                 "Total hospital costs",
                 "Whether or not total hospital costs was above/below median")

summary = tibble("Variables" = Variables,"Pre-scaled Range" = Range,
                 "Descriptions" = Descriptions)

summary %>%
  kable(linesep = "") %>%
  kable_styling(latex_options = c("striped")) %>% 
  column_spec(1,width="4cm") %>%
  column_spec(2,width ="4cm") %>%
  column_spec(3,width="8cm")  %>%
  pack_rows("Predictors",1,12) %>%
  pack_rows("Response",13,14)
```

## Qualitative Outcomes 

For all of our qualitative outcomes, we were trying to predict whether a hospital's total costs were above or below the median. All of the methods' error rates were comparable except for LDA which had the highest misclassification rate at about $17\%$. KNN had no consistent choice of an optimal $k$ across simulations and its variability inspired our simulation study. 

### KNN 

We used $10$ fold cross validation to first choose an optimal number of neighbors, $k$ and found $k = 7$ to be optimal with an error rate of $0.0533$. The true error rate with $k=7$ was $0.0490$ and the true/false positive and negative rates are summarized in the table below. When plotting the cross validation error rates against the chosen $k$, we see a condensed U shape. As we continue to increase $k$ after our optimal choice highlighted in red, there is a continual increase in the error rates. Further, the error rates for the smaller $k$s like $k = \{1,2\}$ are larger but not as large as $k = 100$ which may suggest that the most flexible models do not suffer from over fitting. 

\renewcommand{\arraystretch}{1}
```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(latex2exp)

type = c("True positive", "True negative", "False positive", 
         "False negative")

values = c(0.912,0.993,0.00722,0.0884)

rates = tibble("Classification Rates" = type, "Values" = values)

rates %>%
  kable(linesep = "") %>%
  kable_styling()
```


```{r,warning=FALSE,message=FALSE,echo=FALSE}
knn_errors = read_csv("~/hospital-costs/lindsayknupp/knn_error_rates.csv")

df = cbind(knn_errors,"k" = seq(1,100))

df = df %>%
  mutate("1/K" = 1/k)

ggplot(df) +
  geom_point(aes(x=`1/K`,y=error)) + 
  geom_point(aes(x=1/7,y=error[7]),color="red") +
  ggtitle("Error rates using 10 fold CV") + 
  ylab("Error rates") + 
  xlab(TeX(r"(larger $K$ $\leftarrow \frac{1}{K} \rightarrow$ smaller $K$)"))

```



## Simulation Study 

We were interested in understanding how our data affected the optimal choice of $k$ in the $k$ -nearest neighbors algorithm. We already experienced some variability when running our model through on different computers. Therefore, we wanted to see if more simulated datasets would produce the same variablility. 

To replicate our $13$ predictors and $2$ response variables, we used a package called `faux` to simulate our numerical predictors from a multivariate normal distribution. 

We used a standard normal distribution to replicate our $13$ predictors. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(ggplot2)
library(readr)

results = read_csv("~/hospital-costs/lindsayknupp/sim_study_results.csv")

ggplot(results) +
  geom_histogram(aes(x=k),bins=15) +
  ggtitle("Optimal k over 200 datasets") +
  xlab("k") + 
  ylab("count")

ggplot(results) +
  geom_point(aes(x=k,y=error)) + 
  ggtitle("Error rates vs optimal k")

ggplot(results) + 
  geom_histogram(aes(x=var),bins=15)

ggplot(results) +
  geom_point(aes(x=k,y=var)) +
  ggtitle("Error variance vs optimal k")

```

