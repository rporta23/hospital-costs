---
title: "Final Report"
author: "Lindsay Knupp"
date: "2024-05-14"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Dataset

Our data was collected from [Centers for Medicare & Medicaid Services](https://data.cms.gov/provider-compliance/cost-report/hospital-provider-cost-report) on April 03, 2024. It features information about hospitals through annual cost reports in $2020$. We were interested in understanding how certain characteristics of hospitals like location, number of full time equivalent employees, or number of beds, for example, affected the hospitals' total operating costs. To perform qualitative analyses, we categorized each hospital as above or below the median. 

While the data is roughly annual, certain hospitals reported for different fiscal year lengths. To normalize, we divided some of our variables by the length of their cost reporting period to obtain daily estimates like average number of inpatients per day or average salary expense per day. Variables that are reported as "averages per day" are denoted with the word "average" in the predictor table below. Some hospitals were listed multiple times with distinct reporting periods. We learned that this could correspond to a change in control of the hospital. For example, the hospital could have been sold and transitioned from a voluntary to a governmental hospital. Duplicate hospitals were left in the dataset and a dummy variable, `duplicate` was added to indicate its status.  

There were $13$ different categories of control ranging from "Voluntary Non-Profit-Church" to "Governmental-Federal". To reduce our number of categories, we re-binned this variable to only include the broad categories: "Voluntary", "Proprietary", and "Governmental". We followed a similar procedure for the $12$ different categories of provider type ranging from "Children" to "Cancer" to "General Long Term". In this case, we re-binned provider type to only distinguish between "General" and "Specialized" care. We classified "General Short Term", "General Long Term", and "Religious Non-Medical Health Care Institution" as "General" care and classified "Cancer", "Psychiatric", "Rehabilitation",
"Children", "Reserved for Future Use", "Other", "Extended Neoplastic Disease Care", "Indian Health Services", and "Rural Emergency Hospital" as "Specialized" care. 

To improve accuracy on methods like Lasso regression, we scaled our numerical variables using the `scale()` function which centered and scaled our data appropriately. The following table includes the ranges of the predictors and response before they were re-scaled. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(knitr)
library(tidyverse)
library(kableExtra)
```

\renewcommand{\arraystretch}{2}
```{r,warning=FALSE,message=FALSE,echo=FALSE}
Variables = c("Number of Beds","FTE employees on payroll","Total hospital days",
                   "Total discharges","Total income","Total assets","Salaries","Inpatients",
                   "Rural versus Urban","Type of control","Type of provider","Duplicate hospital","Total costs","Costs bin")


Range = c("[1-2,791]","[0.05-26,941.09]",
          "[1-772,819]","[0.0027-462.63]","[-$6,129,919, $11,516,626]",
           "[-$636,856,458, $29,465,487,958] ","[$128.51, $9,032,294.85]","[0.0033-2123.13]",
           "[2487 rural, 3225 urban]","[2927 voluntary, 1728 proprietary, 1057 governmental]",
           "[4779 general, 993 specialized]","[132 duplicates, 5580 non duplicates ]","[$2,718.28, $16,000,980.58]","[2856 above median, 2856 below median]")

Descriptions = c("Total number of available beds including adult beds, pediatric beds, birthing room, and newborn ICU beds",
                 "Average number of full time-equivalent employees",
                 "Total number of inpatient days (i.e. days all patients spent in the hospital)",
                 "Average number of discharges including deaths",
                 "Average income including net revenue from services given to patients",
                 "Total current assets",
                 "Average salary expenses",
                 "Average number of inpatients",
                 "Location of hospital defined as rural or urban",
                 "Type of control under which hospital is conducted",
                 "Type of services provided",
                 "Whether or not hospital was listed multiple times",
                 "Total hospital costs",
                 "Whether or not total hospital costs was above/below median")

summary = tibble("Variables" = Variables,"Pre-scaled Range" = Range,
                 "Descriptions" = Descriptions)

summary %>%
  kable(linesep = "") %>%
  kable_styling(latex_options = c("striped")) %>% 
  column_spec(1,width="4cm") %>%
  column_spec(2,width ="4cm") %>%
  column_spec(3,width="8cm")  %>%
  pack_rows("Predictors",1,12) %>%
  pack_rows("Response",13,14)
```

# Qualitative Outcome Analyses

For all of our qualitative outcomes, we were trying to predict whether a hospital's total costs were above or below the median. All of the methods' error rates were comparable except for LDA which had the highest misclassification rate at about $17\%$. KNN had no consistent choice of an optimal $k$ across simulations and its variability inspired our simulation study. 

## KNN 

### Assumptions 
We assumed that hospitals with similar predictor values have similar total costs. 

### Results 
We used $10$ fold cross validation to first choose an optimal number of neighbors, $k$ and found $k = 7$ to be optimal with an error rate of $0.0533$ using Euclidean distance. The true error rate with $k=7$ was $0.0490$ and the true/false positive and negative rates are summarized in the table below. When plotting the cross validation error rates against the chosen $k$, we see a condensed U shape. This may suggest that large $k$ suffers from high inaccuracy but too small $k$ can lead to overfitting.

\renewcommand{\arraystretch}{1}
```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(latex2exp)

type = c("True positive", "True negative", "False positive", 
         "False negative")

values = c(0.912,0.993,0.00722,0.0884)

rates = tibble("Classification Rates" = type, "Values" = values)

rates %>%
  kable(linesep = "") %>%
  kable_styling()
```


```{r,warning=FALSE,message=FALSE,echo=FALSE}
knn_errors = read_csv("~/hospital-costs/lindsayknupp/knn_error_rates.csv")

df = cbind(knn_errors,"k" = seq(1,100))

df = df %>%
  mutate("1/K" = 1/k)

ggplot(df) +
  geom_point(aes(x=`1/K`,y=error)) + 
  geom_point(aes(x=1/7,y=error[7]),color="red") +
  ggtitle("Error rates using 10 fold CV") + 
  ylab("Error rates") + 
  xlab(TeX(r"(larger $K$ $\leftarrow \frac{1}{K} \rightarrow$ smaller $K$)"))

```


## Multiple Logistic Regression 

### Assumptions 
We assume that our predictors are not correlated with one another. 

### Results 
The coefficients and standard errors associated with our model can be found below. 
We found that `total_discharges`, `total_assets`, `salaries`, `rural`, and `provider_bin_Specialized` were the most statistically significant. Further, most predictors increased the probability of a hospital's total costs being above the median; unsurprisingly, `salaries` stood out the most. A one unit increase in `salaries` increased the log odds of an above median classification by $50.27$. It also produced a $z$ statistic of $21.001$ providing strong evidence of an association between salaries and total costs. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
simpleglm_coef = read_csv("~/hospital-costs/lindsayknupp/simpleglm_coef.csv")

simpleglm_coef = cbind(simpleglm_coef[,1],round(simpleglm_coef[,2:5],2))
colnames(simpleglm_coef)[5] = "p-value"

kable(simpleglm_coef) %>%
  kable_styling(latex_options = "striped")
```

Our estimated and true error rates were pretty close to one another with our cross validation error of $0.073$ and true test error of $0.0333$. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
type = c("True positive", "True negative", "False positive", 
         "False negative")

values = c(0.946,0.989,0.0108,0.0544)

rates = tibble("Classification Rates" = type, "Values" = values)

rates %>%
  kable(linesep = "") %>%
  kable_styling()
```

## Multiple Logistic Regression with Transformations 

### Assumptions 
We again assume that our predictors are not correlated with one another.

### Results 
We decided to transform `total_income`,`fte_employees_on_payroll`, `salaries`, and `total_days` to experiment with how less significant predictors in conjunction with `salaries` affected the response. We computed polynomial models up to degree $2$ for `total_days` and interaction terms between `total_income` & `fte_employees_on_payroll` and between `fte_employees_on_payroll` & `salaries`.

With our smaller model, all of our new predictors were statistically significant with extreme $z$ statistics. However, it is important to note that the standard errors associated with each coefficient were extremely high suggesting a poor fit. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
simpleglm_coef = read_csv("~/hospital-costs/lindsayknupp/transglm_coef.csv")

simpleglm_coef = cbind(simpleglm_coef[,1],round(simpleglm_coef[,2:5],2))
colnames(simpleglm_coef)[5] = "p-value"

kable(simpleglm_coef) %>%
  kable_styling(latex_options = "striped")
```

Compared to our original multiple logistic regression, our true error rate shot up from $0.033$ to $0.158$ and our cross validation error rate shot up from $0.073$ to $0.130$. Interestingly enough though, the model perfectly predicted hospitals whose total costs were below the median with a true negative rate of $100\%$. But, it did misclassify hospitals whose total costs were above the median with a false negative rate of $30.6\%$. Overall, the transformations performed worse than our original multiple logistic model. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
type = c("True positive", "True negative", "False positive", 
         "False negative")

values = c(0.694,1,0,0.306)

rates = tibble("Classification Rates" = type, "Values" = values)

rates %>%
  kable(linesep = "") %>%
  kable_styling()
```

## LDA 

### Assumptions 
We assume that our predictors are drawn from a multivariate normal distribution and both classes share a common covariance matrix. 

### Results
As stated in the introduction, LDA performed the worst with a cross validation error rate of $0.175$ and a true error rate of $0.165$. This may suggest that our original assumption of our predictors being sampled from a multivariate normal was incorrect or that our classes do not share a common covariance matrix. Further, it is clear that a linear decision boundary is not sufficient to classify hospitals' total costs. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
type = c("True positive", "True negative", "False positive", 
         "False negative")

values = c(0.762,0.913,0.0866,0.238)

rates = tibble("Classification Rates" = type, "Values" = values)

rates %>%
  kable(linesep = "") %>%
  kable_styling()
```


## QDA 

### Assumptions 
We still assume that our predictors are drawn from a multivariate normal distribution but drop the assumption that both classes share a common covariance matrix. 

### Results 
QDA did not perform much better than LDA with a cross validation error of $0.115$ and a true error rate of $0.119$. However, compared to LDA, QDA did a much better job of accurately classifying hospitals whose total costs were below the median reducing the false positive rate from $8.7\%$ to $1.4\%$. The false negative rates stayed fairly consistent hovering around $\sim 20\%$. Overall, QDA is adequate at predicting our class labels. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
type = c("True positive", "True negative", "False positive", 
         "False negative")

values = c(0.782,0.986,0.0144,0.218)

rates = tibble("Classification Rates" = type, "Values" = values)

rates %>%
  kable(linesep = "") %>%
  kable_styling()
```



# Simulation Study 

We were interested in understanding how our data affected the optimal choice of $k$ in the $k$ -nearest neighbors algorithm. We already experienced some variability when running our model through on different computers. Therefore, we wanted to see if more simulated datasets would produce the same variablility. 

To replicate our $13$ predictors and $2$ response variables, we used a package called `faux` to simulate our numerical predictors from a multivariate normal distribution. 

We used a standard normal distribution to replicate our $13$ predictors. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(ggplot2)
library(readr)

results = read_csv("~/hospital-costs/lindsayknupp/sim_study_results.csv")

ggplot(results) +
  geom_histogram(aes(x=k),bins=15) +
  ggtitle("Optimal k over 200 datasets") +
  xlab("k") + 
  ylab("count")

ggplot(results) +
  geom_point(aes(x=k,y=error)) + 
  ggtitle("Error rates vs optimal k")

ggplot(results) + 
  geom_histogram(aes(x=var),bins=15)

ggplot(results) +
  geom_point(aes(x=k,y=var)) +
  ggtitle("Error variance vs optimal k")

```

