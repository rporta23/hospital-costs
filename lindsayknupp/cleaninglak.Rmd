---
title: "Data Cleaning"
author: "Lindsay Knupp"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r}
hpc <- read_csv(here::here("hpc.csv"))
#summary(hpc)

hpcdf <- hpc |> 
  mutate(start = as.Date(`Fiscal Year Begin Date`),
         end = as.Date(`Fiscal Year End Date`)) |> 
  mutate(days = as.numeric(end - start)) |> 
  mutate(numBeds = `Total Bed Days Available`/days,
         id = row_number())
```

```{r}
hpc_clean <- hpcdf |> 
  janitor::clean_names() |> 
  select(provider_ccn, days, number_of_beds,
         total_costs, rural_versus_urban, provider_type,
         type_of_control,fte_employees_on_payroll,
         total_days = total_days_v_xviii_xix_unknown,
         total_discharges = total_discharges_v_xviii_xix_unknown,
         total_income, total_assets,
         salaries = total_salaries_from_worksheet_a) |> 
  na.omit() |> 
  mutate(rural = ifelse(rural_versus_urban == "R", 1, 0),
         control_bin = case_when(
           type_of_control < 3 ~ "Voluntary",
           (type_of_control >= 3 & type_of_control < 7) ~ "Proprietary",
           type_of_control >= 7 ~ "Governmental"
         ),
         provider_bin = ifelse(provider_type < 3 |
                                 provider_type == 6,
                               "General", "Specialized"
                               ),
         ) |> 
  select(- c(rural_versus_urban, type_of_control, provider_type))

duplicates <- hpc_clean |> 
  group_by(provider_ccn) |> 
  summarise(count = n()) |> 
  filter(count > 1)

dup <- map_lgl(hpc_clean$provider_ccn, `%in%`, duplicates$provider_ccn)

hpc_clean$duplicate <- as.numeric(dup)

hpc_normalize <- hpc_clean |> 
  mutate(
    total_costs = total_costs/days,
    inpatients = total_days/days,
    total_discharges = total_discharges/days,
    total_income = total_income/days,
    salaries = salaries/days
    )
```

```{r}
hpc_dummies <- hpc_normalize |> 
  select(-c(provider_ccn, days)) |> 
  mutate(
    costs_bin = ifelse(total_costs > median(total_costs), 1, 0)
    ) |> 
   tidytable::get_dummies() |>
  select(-c(control_bin_Voluntary, provider_bin_General)) |> 
  select(where(is.numeric))
```

Scaled hpc_dummies (quantiative predictors are scaled)
```{r}
quantitative_pred <- hpc_dummies %>%
  select(-c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized,costs_bin,duplicate)) %>%
  scale()

qualitative_pred <- hpc_dummies %>%
  select(c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized,costs_bin,duplicate))

scaled_hpc_dummies = cbind(quantitative_pred,qualitative_pred)

```

Codes for type of Control:

1 = Voluntary Non‐Profit‐Church

2 = Voluntary Non‐Profit‐Other

3 = Proprietary‐Individual

4 = Proprietary‐Corporation

5 = Proprietary‐Partnership

6 = Proprietary‐Other

7 = Governmental‐Federal

8 = Governmental‐City‐County

9 = Governmental‐County

10 = Governmental‐State

11 = Governmental‐Hospital District

12 = Governmental‐City

13 = Governmental‐Other

Codes for Provider Type:

1 = General Short Term (includes CAHs)

2 = General Long Term

3 = Cancer

4 = Psychiatric

5 = Rehabilitation

6 = Religious Non‐Medical Health Care Institution

7 = Children

8 = Reserved for Future Use

9 = Other

10 = Extended Neoplastic Disease Care

11 = Indian Health Services

12 = Rural Emergency Hospital.

# Exploratory Analysis

```{r}
ggplot(hpc_dummies, aes(x = total_costs)) +
  geom_histogram() +
  geom_vline(xintercept = median(hpc_dummies$total_costs), color = "red") + 
  geom_vline(xintercept = mean(hpc_dummies$total_costs), color = "blue")
```

Distribution of response total costs is extremely right-skewed.

Red line = median, blue line = mean

Summary Statistics:

```{r}
summary(hpc_clean)
```

Pairs plot:

```{r}
# pairs(hpc_dummies)
```

-   very high pairwise positive collinearity between:
    -   total_days and bed_days, and total_discharges
    -   salaries and employees on payroll
-   total_costs appears to have fairly strong positive linear relationships with:
    -   fte_employees_on_payroll
    -   total_days
    -   bed_days
    -   total_discharges
    -   salaries
-   total_costs appears to have weak or no relationship with:
    -   total_income
    -   total_assets
-   hard to discern any relationships for the categorical predictors
-   for binary response, the predictors which have positive relationships with total cost have a pattern such that reponse = 0 corresponds to a higher concentration of points with low values of the predictor response = 1 corresponds to a wider range of values for the predictor. This same pattern shows up also for total assets and total income, which did not look associated with continuous total cost.

```{r}
library(plotly)

ggplot(hpc_dummies) +
  geom_histogram(aes(x = total_income))

negative = hpc_dummies %>%
  filter(total_income < 0) 

positive = hpc_dummies %>%
  filter(total_income > 0) 

hpc_dummies %>%
  filter(total_income < 3000000) %>%
  mutate(status = ifelse(total_income < 0, "in the red","in the green")) %>%
  plot_ly() %>%
  add_histogram(
    x = ~total_income,
    color = ~status
  )

negative %>%
  mutate(provider = ifelse(provider_bin_Specialized == 0,"General health","Specialized")) %>%
ggplot() +
geom_histogram(aes(x=total_income,fill = provider),alpha = 0.3)

negative %>%
  summarise(num_Specialized = sum(provider_bin_Specialized),
         num_General = nrow(negative) - num_Specialized)

positive %>%
  mutate(provider = ifelse(provider_bin_Specialized == 0,"General health","Specialized")) %>%
ggplot() +
geom_histogram(aes(x=total_income,fill = provider),alpha = 0.3)

positive %>%
  summarise(num_Specialized = sum(provider_bin_Specialized),
         num_General = nrow(negative) - num_Specialized)

```

* There are about a quarter of hospitals whose operating costs are larger than their income from patients and other sources. The other 75% are operating in the positive in such that their operating costs are not larger than their income sources.
* Splitting up the hospitals that are operating in a deficit versus a surplus, I was interested to see if those operating in a deficit were the specialized hospitals. Specialized hospitals make up around 17.5% of hospitals in a deficit and around 46.8% of hospitals in a surplus. It should also be noted that overall, specialized hospitals only make up 923/5646 or around 16% of hosptials overall. 

# Train-test Split

```{r}
set.seed(1)

train_prop <- 0.9

n <- nrow(scaled_hpc_dummies)
n_train <- train_prop*n
n_test <- n - n_train

scaled_hpc_dummies$set <- "Train"
scaled_hpc_dummies$set[sample(n, n_test, replace = FALSE)] <- "Test"
  
df_train <- scaled_hpc_dummies |>  filter(set == "Train") |> select(-set)
df_test <- scaled_hpc_dummies |> filter(set == "Test") |> select(-set)
```

# Train set-fold assignment

```{r}
set.seed(1)

n_train <- nrow(df_train)

k <- 10
k.o <- k
kvals <- unique(round(n_train/(1L:floor(n_train/2))))
temp <- abs(kvals - k)
if (!any(temp == 0))  { k <- kvals[temp == min(temp)][1L] }
f <- ceiling(n_train/k)

df_train$fold <- sample(rep(1L:k, f), n_train)
```


# For all methods:

-   For each method you apply, use 10 fold cross-validation estimate for the test error.
-   discuss assumptions

# Quantitative Outcome Analyses

## Marginal simple linear regressions

```{r}
train = df_train %>%
  select(-c(costs_bin))

test = df_test %>%
  select(-c(costs_bin))

predictors <- select(train, -c(total_costs,fold))

# model summaries
# map(predictors, ~summary(lm(total_costs ~ .x, data = train)))

simple_models <- map(predictors, ~lm(total_costs ~ .x, data = train))
```

10-fold cv
```{r}
# Estimate with k-fold:

predictor_names <- colnames(predictors)
formulas <- str_c("total_costs", " ~ ", predictor_names)

compute_MSEP_lm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(formula, data = select(current_train_folds,-fold))
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs,fold) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

kfold_cv_lm <- function(train_data, formula, k = 10){
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

cv_marginal_lm <- map_dbl( formulas, ~kfold_cv_lm(train, formula = .x, k = 10) )
```

Test error 
```{r}
compute_testMSE_lm <- function(train_data,test_data, formula){
  
  marginaldf <- as.data.frame(matrix(nrow=1,ncol=2))
  colnames(marginaldf) <- c("Formula","Test_MSE")

  mod <- lm(formula, data = select(train_data,-fold))
  pred <- predict(mod, newdata = select( test_data, -c(total_costs) ) )
    
  testMSE <- mean( (test_data$total_costs - pred) ^ 2 )
  
  marginaldf$Formula = formula
  marginaldf$Test_MSE = testMSE
  
  return(marginaldf)
}

testMSE_marginal_lm <- map_df(formulas, ~compute_testMSE_lm(train_data = train,test_data = test,formula = .x))

```

```{r}
method <- c("Marginal Linear Regression")

formula <- testMSE_marginal_lm$Formula

cv_error <- c(cv_marginal_lm)

test_error <- c(testMSE_marginal_lm$Test_MSE)

error_summary <- tibble(method,formula, cv_error, test_error)

knitr::kable(error_summary)
```

## Multiple linear regression

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r}
train = df_train %>%
  select(-c(costs_bin))

test = df_test %>%
  select(-c(costs_bin))

transformation_lm <- summary(lm(total_costs ~ fte_employees_on_payroll*total_income, data = df_train))

polynomial_lm <- summary(lm(total_costs ~ salaries + I(salaries^2),data = df_train))

```

10-fold cv
```{r}
# Estimate with k-fold:

compute_MSEP_multiple_lm <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(total_costs ~ ., data = select(current_train_folds,-fold))
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs,fold) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

cv_multiple_lm <- mean(map_dbl(1:10, ~compute_MSEP_multiple_lm(train,.x)))
```

Test error 
```{r}
compute_testMSE_multiple_lm <- function(train_data,test_data){
  
  mod <- lm(total_costs ~ ., data = select(train_data,-fold))
  pred <- predict(mod, newdata = select( test_data, -c(total_costs) ) )
    
  testMSE <- mean( (test_data$total_costs - pred) ^ 2 )

  return(testMSE)
}

testMSE_multiple_lm <- compute_testMSE_multiple_lm(train,test)
```


## Regression Tree (with pruning)
```{r}
library(tree)

set.seed(1)

train = df_train %>%
  select(-costs_bin)

test = df_test %>%
  select(-costs_bin)

# Original tree 
tree.costs = tree(total_costs ~ ., data = select(train,-fold))
summary(tree.costs)
plot(tree.costs)
text(tree.costs)
# Original errors
tree.pred = predict(tree.costs,test)
testerror = mean(  (test$total_costs - tree.pred)^2  )

# Optimal number of nodes
cv.costs = cv.tree(tree.costs)
plot(cv.costs$size, cv.costs$dev, type = "b")
optimalnodes = cv.costs$size[which.min(cv.costs$dev)]
# Prune tree with optimal number of nodes 
prune.costs = prune.tree(tree.costs,best=optimalnodes)
# Compute new errors 
prune.pred = predict(prune.costs,test)
testerrorprune = mean(  (test$total_costs - prune.pred)^2  )


# Now, fit larger tree with pruning to see if we get better results 
tree.costs.full = tree(total_costs ~ ., data = select(train,-fold),
                        control = tree.control(nrow(train), mindev = 0))
cv.costs.full = cv.tree(tree.costs.full)
plot(cv.costs.full$size, cv.costs.full$dev, type = "b")
optimalnodesfull = cv.costs.full$size[which.min(cv.costs.full$dev)]

prune.costs.full = prune.tree(tree.costs.full,best=optimalnodesfull)
# Compute new errors 
prune.pred.full = predict(prune.costs.full,test)
testerrorprunefull = mean(  (test$total_costs - prune.pred.full)^2  )
```
Best number of terminal nodes is 9 but that is too large when using individual folds for cross validation error rates. 

k fold CV 
```{r}
compute_MSEP_regression_tree <- function(train_data,i){
    current_train_folds <- filter(train_data, fold != i)
    current_test_fold <- filter(train_data, fold == i)
    
    tree.costs = tree(total_costs ~ ., data = select(current_train_folds,-fold))
    prune.costs = prune.tree(tree.costs,best = 9)
    
    prune.pred = predict(prune.costs,select(current_test_fold,-fold))
    
    return (mean((current_test_fold$total_costs - prune.pred)^2) )
}

cv_regression_tree = mean(map_dbl(1:10,~compute_MSEP_regression_tree(train,.x)))

compute_testMSE_regression_tree <- function(train_data,test_data){
  
  tree.costs = tree(total_costs ~ ., data = select(train_data,-fold))
  prune.costs = prune.tree(tree.costs,best = 9)
  
  prune.pred = predict(prune.costs,test_data)
  
  return (mean((test_data$total_costs - prune.pred)^2) )
}

testMSE_regression_tree <- compute_testMSE_regression_tree(train,test)
```

## Bagging (with variable importance)

```{r}
library(randomForest)

set.seed(1)

train = df_train %>%
  select(-costs_bin) 

test = df_test %>%
  select(-costs_bin)
```

k-fold CV
```{r}
compute_MSEP_bagging <- function(train_data,i){
    current_train_folds <- filter(train_data, fold != i)
    current_test_fold <- filter(train_data, fold == i)
    
    bag.costs <- randomForest(total_costs ~ .,data=select(current_train_folds,-fold),
                              mtry = ncol(current_train_folds)-2,importance = TRUE  )
    bag.pred = predict(bag.costs,select(current_test_fold,-fold))
    
    return (mean((current_test_fold$total_costs - bag.pred)^2) )
}

# cv_bagging = mean(map_dbl(1:10,~compute_MSEP_bagging(train,.x)))
cv_bagging = 0.07421265
```

```{r}
bag.costs <- randomForest(total_costs ~ ., data = select(train,-fold), mtry = ncol(train)-2,
                          importance = TRUE  )

bag.pred = predict(bag.costs,test)

mean_decrease_Gini = importance(bag.costs,type=2)
varImpPlot(bag.costs)

testMSE_bagging = mean( (test$total_costs - bag.pred)^2 )
```


## Random Forest (with variable importance)

```{r}
library(randomForest)

set.seed(1)

train = df_train %>%
  select(-costs_bin) 

test = df_test %>%
  select(-costs_bin)
```

k-fold CV
```{r}
compute_MSEP_random_forest <- function(train_data,i){
    current_train_folds <- filter(train_data, fold != i)
    current_test_fold <- filter(train_data, fold == i)
    
    random_forest.costs <- randomForest(total_costs ~ .,data=select(current_train_folds,-fold),
                              mtry = sqrt(ncol(current_train_folds)-2),importance = TRUE  )
    random_forest.pred = predict(random_forest.costs,select(current_test_fold,-fold))
    
    return (mean((current_test_fold$total_costs - random_forest.pred)^2) )
}

# cv_random_forest = mean(map_dbl(1:10,~compute_MSEP_random_forest(train,.x)))
cv_random_forest = 0.06965231
```

```{r}
random_forest.costs <- randomForest(total_costs ~ ., data = select(train,-fold), mtry = sqrt(ncol(train)-2),
                          importance = TRUE  )

random_forest.pred = predict(random_forest.costs,test)

mean_decrease_Gini = importance(random_forest.costs,type=2)
varImpPlot(random_forest.costs)

testMSE_random_forest = mean( (test$total_costs - random_forest.pred)^2 )
```


## Boosting (including selecting the tuning parameter)

```{r}
library(gbm)

set.seed(1)

train = df_train %>%
  select(-costs_bin) 

test = df_test %>%
  select(-costs_bin) 
  
# boost.costs = gbm(costs_bin ~ ., data = select(train,-fold),
#                   distribution = "bernoulli",n.trees = 1000)
# summary(boost.costs)
# 
# boost.pred = round(predict(boost.costs,test,type="response"))
# error = mean(boost.pred != test$costs_bin)
# 
# plot(boost.costs,i="salaries")
# plot(boost.costs,i="fte_employees_on_payroll")
```

k-fold CV
```{r}
compute_MSEP_boosting <- function(train_data,lambda,i){
    current_train_folds <- filter(train_data, fold != i)
    current_test_fold <- filter(train_data, fold == i)
    
    boost.costs = gbm(total_costs ~ ., data = select(current_train_folds,-fold),
                  distribution = "gaussian",n.trees = 1000,shrinkage = lambda)
    boost.pred = predict(boost.costs,select(current_test_fold,-fold))
    
    return (mean( (current_test_fold$total_costs - boost.pred)^2 ))
}

kfold_cv_boosting <- function(train_data,lambda){
  cv_boosting = mean(map_dbl(1:10,~compute_MSEP_boosting(train_data,
                                                         lambda = lambda,.x)))
  return (cv_boosting)
}

lambda_grid = seq(0.01,0.1,0.005)
# Commented line out b/c it takes so long 
# lambda_errors = map_dbl(lambda_grid,~kfold_cv_boosting(train,.x))

# optimal_lambda = lambda_grid[which.min(lambda_errors)]
optimal_lambda = 0.01
# cv_boosting = min(lambda_errors)
cv_boosting = 0.07512334
```

Test Error 
```{r}
boost.costs = gbm(total_costs ~ ., data = select(train,-fold),
                  distribution = "gaussian",n.trees = 5000,shrinkage = optimal_lambda)

boost.pred = predict(boost.costs,test)
    
testMSE_boosting = mean( (test$total_costs-boost.pred)^2 )

```


## Neural Network

# Variable Selection

## Best Subset

```{r,warning = FALSE}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin,-fold)

regfit_full <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "exhaustive")
summary(regfit_full)
```

k-fold CV

```{r}
compute_MSEP_lm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(formula, data = select(current_train_folds,-fold))
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs,fold) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

kfold_cv_lm <- function(train_data, formula, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
  
}

compute_testMSE_lm <- function(train_data,test_data,formula){
  
  mod <- lm(formula, data = select(train_data,-fold))
  pred <- predict(mod, newdata = select( test_data, -c(total_costs) ) )
    
  testMSE <- mean( (test_data$total_costs - pred) ^ 2 )

  return(testMSE)
}

# testMSE_multiple_lm <- compute_testMSE_lm(train,test,"total_costs ~ salaries")
```

```{r}
train <- df_train %>%
  select(-(costs_bin))

test <- df_test %>%
  select(-costs_bin)

best_subset_cv <- tibble("avgMSEP" = rep(NA,12),"testMSE" = rep(NA,12))

# k-fold CV for 1 predictor: salaries
formula = "total_costs ~ salaries"
best_subset_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(train, formula = .x, k = 10))
best_subset_cv$testMSE[1] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
best_subset_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[2] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
best_subset_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[3] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
best_subset_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[4] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
best_subset_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[5] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
best_subset_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[6] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
best_subset_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[7] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
best_subset_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[8] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
best_subset_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[9] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
best_subset_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[10] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
best_subset_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[11] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
best_subset_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[12] <- compute_testMSE_lm(train,test,formula)

cv_best_subset = mean(best_subset_cv$avgMSEP)
testMSE_best_subset = mean(best_subset_cv$testMSE)
```


```{r}
method <- "Best Subset"

cv_error <- cv_best_subset

test_error <- testMSE_best_subset

```


## Forward Stepwise

```{r}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin)

test <- df_test %>%
  select(-costs_bin)

regfit_fwd <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "forward")
summary(regfit_fwd)
```

Forward stepwise CV

```{r}
forward_stepwise_cv <- tibble("avgMSEP" = rep(NA,12),"testMSE" = rep(NA,12))

# k-fold CV for 1 predictor: 
formula = "total_costs ~ salaries"
forward_stepwise_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(train, formula = .x, k = 10))
forward_stepwise_cv$testMSE[1] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
forward_stepwise_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[2] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
forward_stepwise_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[3] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
forward_stepwise_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[4] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
forward_stepwise_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[5] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
forward_stepwise_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[6] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
forward_stepwise_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[7] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
forward_stepwise_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[8] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
forward_stepwise_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[9] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
forward_stepwise_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[10] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
forward_stepwise_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[11] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
forward_stepwise_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[12] <- compute_testMSE_lm(train,test,formula)

cv_forward_stepwise = mean(forward_stepwise_cv$avgMSEP)
testMSE_forward_stepwise = mean(forward_stepwise_cv$testMSE)
```

## Backward Stepwise

```{r}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin)

test <- df_test %>%
  select(-costs_bin)

regfit_bwd <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "backward")
summary(regfit_bwd)
```

Backward stepwise CV

```{r}

backward_stepwise_cv <- tibble("avgMSEP" = rep(NA,12),"testMSE" = rep(NA,12))

# k-fold CV for 1 predictor: 
formula = "total_costs ~ salaries"
backward_stepwise_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(df_train, formula = .x, k = 10))
backward_stepwise_cv$testMSE[1] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
backward_stepwise_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[2] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
backward_stepwise_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[3] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
backward_stepwise_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[4] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
backward_stepwise_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[5] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
backward_stepwise_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[6] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
backward_stepwise_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[7] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
backward_stepwise_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[8] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
backward_stepwise_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[9] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
backward_stepwise_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[10] <- compute_testMSE_lm(train,test,formula)


# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
backward_stepwise_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[11] <- compute_testMSE_lm(train,test,formula)


# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
backward_stepwise_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[12] <- compute_testMSE_lm(train,test,formula)

cv_backward_stepwise = mean(backward_stepwise_cv$avgMSEP)
testMSE_backward_stepwise = mean(backward_stepwise_cv$testMSE)
```

(For the above 3 methods, determine which model of a given size is best by comparing the 10-fold cross validation estimate of the test error.)

## Ridge regression (find the best tuning parameter using cross-validation)

Compute cv MSEP for both ridge and lasso 
```{r}
compute_MSEP_ridge_lasso <- function(train_data,lambda,alpha,i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  train_design_mat = model.matrix(total_costs ~ ., data = select(current_train_folds,-fold))[, -1]
  test_design_mat = model.matrix(total_costs ~ .,data = select(current_test_fold,-fold))[,-1]
  
  trainx <- train_design_mat
  trainy <- current_train_folds$total_costs
  
  testx <- test_design_mat
  testy <- current_test_fold$total_costs
    
  mod <- glmnet(trainx,trainy,alpha=alpha,lambda = lambda)
  pred <- predict(mod, newx = testx)
    
  MSEP <- mean( (testy - pred) ^ 2 )
  return(MSEP)
}
```


```{r}
library(glmnet)

set.seed(1)

train <- df_train %>%
  select(-c(costs_bin))

test <- df_test %>%
  select(-c(costs_bin))

train_design_mat = model.matrix(total_costs ~ ., data = select(train,-fold))[, -1]
test_design_mat = model.matrix(total_costs ~ .,data = test)[,-1]

trainx <- train_design_mat
trainy <- train$total_costs

testx <- test_design_mat
testy <- test$total_costs

# Ridge model 
lambda_grid <- 10 ^ seq(10, -2, length = 100)
ridge_reg <- glmnet(trainx, trainy, alpha = 0, lambda = lambda_grid) 

# Perform 10-fold cross validation 
cv.out <- cv.glmnet(trainx, trainy, alpha = 0)
bestmean <- min(cv.out$cvm) # lowest error 
bestlambda <- cv.out$lambda[which(cv.out$cvm == bestmean)]
cv_ridge <- mean(map_dbl(1:10, ~compute_MSEP_ridge_lasso(train,bestlambda,0,.x)))

# Use optimal lambda to find test error 
best_lambda_ridge_reg <- glmnet(trainx,trainy,alpha = 0,lambda = bestlambda)
pred = predict(best_lambda_ridge_reg, newx = testx)
testMSE_ridge = mean( (pred-testy)^2 )
```

## Lasso (find the best tuning parameter using cross-validation)

```{r}
library(glmnet)

set.seed(1)

train <- df_train %>%
  select(-c(costs_bin))

test <- df_test %>%
  select(-c(costs_bin))

train_design_mat = model.matrix(total_costs ~ ., data = select(train,-fold))[, -1]
test_design_mat = model.matrix(total_costs ~ .,data = test)[,-1]

trainx <- train_design_mat
trainy <- train$total_costs

testx <- test_design_mat
testy <- test$total_costs

# Lasso model 
lambda_grid <- 10 ^ seq(10, -2, length = 100)
lasso_reg <- glmnet(trainx, trainy, alpha = 1, lambda = lambda_grid) 

# Perform 10-fold cross validation 
cv.out <- cv.glmnet(trainx, trainy, alpha = 1)
bestmean <- min(cv.out$cvm) # lowest error 
bestlambda <- cv.out$lambda[which(cv.out$cvm == bestmean)]
cv_lasso <- mean(map_dbl(1:10, ~compute_MSEP_ridge_lasso(train,bestlambda,1,.x)))

# Use optimal lambda to find test error 
optimal_lambda_lasso_reg <- glmnet(trainx,trainy,alpha = 0,lambda = bestlambda)
pred = predict(optimal_lambda_lasso_reg, newx = testx)
testMSE_lasso = mean( (pred-testy)^2 )

```

## Principal Components Regression (PCR)

```{r}
library(pls)

set.seed(1)
train <- df_train %>%
  select(-c(costs_bin,fold))

test <- df_test %>%
  select(-c(costs_bin))

pcr.fit <-pcr(total_costs ~ ., data = train, validation = "CV",segments = 10)
MSEP = MSEP(pcr.fit)
(which.min(MSEP$val[1,1, ] ))
cv_pcr = MSEP$val[1,1,]
cv_pcr = mean(cv_pcr[-1])

validationplot(pcr.fit, val.type = "MSEP")

pcr.pred <-predict(pcr.fit, test, ncomp = 5)
testMSE_pcr = mean((pcr.pred- test$total_costs)^2)
```

# Quantitative summary
```{r}

method <- c("Marginal Linear Regression")

formula <- testMSE_marginal_lm$Formula

cv_error <- c(cv_marginal_lm)

test_error <- c(testMSE_marginal_lm$Test_MSE)

marginal_summary <- tibble(method,formula, cv_error, test_error)


method <- c("Multiple Linear Regression","Best Subset","Forward Stepwise",
            "Backward Stepwise","Ridge Regression","Lasso","PCR","Regression Trees",
            "Bagging","Random Forest","Boosting")

formula <- rep(NA,11)

cv_error <- c(cv_multiple_lm,cv_best_subset,cv_forward_stepwise,
              cv_backward_stepwise,cv_ridge,cv_lasso,cv_pcr,cv_regression_tree,
              cv_bagging,cv_random_forest,cv_boosting)

test_error <- c(testMSE_multiple_lm,testMSE_best_subset,testMSE_forward_stepwise,
                testMSE_backward_stepwise,testMSE_ridge,testMSE_lasso,
                testMSE_pcr,testMSE_regression_tree,testMSE_bagging,
                testMSE_random_forest,testMSE_boosting)

rest_of_methods <- tibble(method,formula,cv_error,test_error)

error_summary <- rbind(marginal_summary,rest_of_methods)

knitr::kable(error_summary)
```


# Qualitative Outcome Analyses

## KNN

```{r}
library(class)

set.seed(1)

train <- df_train %>%
  select(-c(total_costs))

compute_error_knn <- function(train_data, i, neighbors){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)

  knn_pred <- knn(
      train = select(current_train_folds, -c(costs_bin,fold)),
      test = select(current_test_fold, -c(costs_bin,fold)),
      cl = current_train_folds$costs_bin,
      k = neighbors,
      prob = TRUE
    )
  
  error = mean(current_test_fold$costs_bin != knn_pred)
  return (error)
}

# for fold number = 1,2,3,4,5,6,7,8,9,10 and then avg all the errors together
kfold_cv_knn <- function(train_data,neighbors, k = 10) {
  error_vec <- map_dbl(1:k, ~compute_error_knn(train, .x,neighbors))
  avg_error <- mean(error_vec)
  return(avg_error)
}

neighbors <- 1:100 
# Commented lines out b/c it takes so long
# knn_error_rates = map_dbl(neighbors, ~kfold_cv_knn(train,neighbors = .x))

# optimal_n <- which.min(knn_error_rates)
# optimal_error_rate <- knn_error_rates[optimal_n]

optimal_n <- 5
optimal_error_rate <- 0.05057881

cv_knn = optimal_error_rate
```

### Compare true test error rate

```{r}
set.seed(1)

train <- df_train %>%
  select(-c(total_costs,fold))

test <- df_test %>%
  select(-c(total_costs))

model_info = tibble(.rows=1)

knn_pred <- knn(
      train = select(train,-costs_bin),
      test = select(test,-costs_bin),
      cl = train$costs_bin,
      k = optimal_n,
      prob = TRUE
    )
  
(contingency = table(knn_pred,test$costs_bin))

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error = mean(test$costs_bin != knn_pred)

model_info

testMSE_knn = model_info$error
truepos_knn = model_info$truepos
trueneg_knn = model_info$trueneg
falsepos_knn = model_info$falsepos
falseneg_knn = model_info$falseneg
```

## Simple/Multiple logistic regression

```{r}
set.seed(1)

predictors <- select(df_train, -c(total_costs, costs_bin,fold))

train = df_train %>%
  select(-c(total_costs))

test = df_test %>%
  select(-c(total_costs))

# model summaries
# map(predictors, ~summary(glm(costs_bin ~ .x, data = select(train,-fold),family=binomial)))

# simple_logistic_models <- map(predictors, ~glm(costs_bin ~ .x, data = select(train,-fold),family=binomial))
```

10-fold cv

```{r}
# Estimate with k-fold:
compute_MSEP_glm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- glm(formula, data = select(current_train_folds,-fold),family = binomial)
  pred <- round(predict(mod, newdata = select( current_test_fold, -c(fold,costs_bin) ),type = "response" ))
    
  error_rate <- mean( (current_test_fold$costs_bin != pred) )
  return(error_rate)
}

formula = c("costs_bin ~ .")
error_rates <- map_dbl(1:10, ~compute_MSEP_glm(train, formula, .x))
cv_glm = mean(error_rates)

# test_error_vec <- map_dbl( formulas, ~compute_MSEP_glm(train, formula = .x, i = 10) )
```

Test error 
```{r}
mod <- glm("costs_bin ~ .", data = select(train,-fold),family = binomial)
pred <- round(predict(mod, newdata = select( test, -c(costs_bin) ),type = "response" ))
  
model_info = tibble(.rows=1)
contingency = table(pred,test$costs_bin)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error_rate <- mean( (test$costs_bin != pred) )

testMSE_glm = model_info$error_rate
truepos_glm = model_info$truepos
trueneg_glm = model_info$trueneg
falsepos_glm = model_info$falsepos
falseneg_glm = model_info$falseneg
```

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

## LDA

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

test = df_test %>%
  select(-total_costs)

compute_error_lda <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  lda_model <- MASS::lda(costs_bin ~ ., data = select(current_train_folds,-fold))
  lda_pred <- predict(lda_model, newdata = select(current_test_fold, -c(fold,costs_bin)))$class
  
  contingency = table(lda_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != lda_pred)
  return (model_info)
}

kfold_cv_lda = map_df(1:10, ~compute_error_lda(train,.x))
cv_lda = (mean(kfold_cv_lda$error))

# table(c(1,2),c(3,4))
# table(current_test_fold$costs_bin,lda_pred)
```

Test error 
```{r}
lda_model <- MASS::lda(costs_bin ~ ., data = select(train,-fold))
lda_pred <- predict(lda_model, newdata = select(test, -c(costs_bin)))$class
  
model_info = tibble(.rows=1)
contingency = table(pred,test$costs_bin)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error_rate <- mean( (test$costs_bin != lda_pred) )

testMSE_lda = model_info$error_rate
truepos_lda = model_info$truepos
trueneg_lda = model_info$trueneg
falsepos_lda = model_info$falsepos
falseneg_lda = model_info$falseneg
```


## QDA

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

test = df_test %>%
  select(-total_costs)

compute_error_qda <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  qda_model <- MASS::qda(costs_bin ~ ., data = select(current_train_folds,-fold))
  qda_pred <- predict(qda_model, newdata = select(current_test_fold, -c(fold,costs_bin)))$class
  
  contingency = table(qda_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != qda_pred)
  return (model_info)
}

kfold_cv_qda = map_df(1:10, ~compute_error_qda(train,.x))
cv_qda = (mean(kfold_cv_qda$error))

# table(c(1,2),c(3,4))
# table(current_test_fold$costs_bin,lda_pred)
```

Test error 
```{r}

qda_model <- MASS::qda(costs_bin ~ ., data = select(train,-fold))
qda_pred <- predict(qda_model, newdata = select(test, -c(costs_bin)))$class

model_info = tibble(.rows=1)
contingency = table(pred,test$costs_bin)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error_rate <- mean( (test$costs_bin != lda_pred) )

testMSE_qda = model_info$error_rate
truepos_qda = model_info$truepos
trueneg_qda = model_info$trueneg
falsepos_qda = model_info$falsepos
falseneg_qda = model_info$falseneg
```

## Naive Bayes (at least two kernels)

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

test = df_test %>%
  select(-total_costs)

compute_error_nb <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  nb_model <- e1071::naiveBayes(costs_bin ~ ., data = select(current_train_folds,-fold))
  nb_pred <- predict(nb_model, newdata = select(current_test_fold, -c(fold,costs_bin)))
  
  contingency = table(nb_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != nb_pred)
  return (model_info)
}

kfold_cv_nb = map_df(1:10, ~compute_error_nb(train,.x))
cv_nb = (mean(kfold_cv_nb$error))
```

Test error 
```{r}
nb_model <- e1071::naiveBayes(costs_bin ~ ., data = select(train,-fold))
nb_pred <- predict(nb_model, newdata = select(test, -c(costs_bin)))

model_info = tibble(.rows=1)
contingency = table(pred,test$costs_bin)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error_rate <- mean( (test$costs_bin != lda_pred) )

testMSE_nb = model_info$error_rate
truepos_nb = model_info$truepos
trueneg_nb = model_info$trueneg
falsepos_nb = model_info$falsepos
falseneg_nb = model_info$falseneg
```

2nd kernel

```{r}
set.seed(1)


train = df_train %>% 
  select(-total_costs)

test = df_test %>% 
  select(-total_costs)

compute_error_nb_kde <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  current_train_matrix = current_train_folds %>%
    select(-c(fold,costs_bin)) %>%
    as.matrix()
  
  current_train_matrixy = current_train_folds %>%
    select(costs_bin) %>%
    as.matrix()
  
  current_test_matrix = current_test_fold %>%
    select(-c(fold,costs_bin)) %>%
    as.matrix()
  
  model_info = tibble(.rows=1)
  
  nb_kde_model <-  naivebayes::nonparametric_naive_bayes(y = as.factor(current_train_matrixy),
                                       x = current_train_matrix
    )
  nb_kde_pred <- predict(nb_kde_model, newdata = current_test_matrix )

  # nb_kde_model <- e1071::naiveBayes(costs_bin ~ ., data = select(current_train_folds,-fold))
  # nb_kde_pred <- predict(nb_kde_model, newdata = select(current_test_fold, -c(fold,costs_bin)))
  
  contingency = table(nb_kde_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos
  model_info$error = mean(current_test_fold$costs_bin != nb_kde_pred)
  return (model_info)
}

kfold_cv_nb_kde = map_df(1:10, ~compute_error_nb_kde(train,.x))
cv_nb_kde = (mean(kfold_cv_nb_kde$error))
```

Test error 
```{r}
current_train_matrix = train %>%
  select(-c(fold,costs_bin)) %>%
  as.matrix()

current_train_matrixy = train %>%
  select(costs_bin) %>%
  as.matrix()

current_test_matrix = test %>%
  select(-c(costs_bin)) %>%
  as.matrix()

model_info = tibble(.rows=1)

nb_kde_model <-  naivebayes::nonparametric_naive_bayes(y = as.factor(current_train_matrixy),
                                     x = current_train_matrix
  )
nb_kde_pred <- predict(nb_kde_model, newdata = current_test_matrix )

# nb_kde_model <- e1071::naiveBayes(costs_bin ~ ., data = select(current_train_folds,-fold))
# nb_kde_pred <- predict(nb_kde_model, newdata = select(current_test_fold, -c(fold,costs_bin)))

contingency = table(nb_kde_pred,test$costs_bin)
#           Actual:           
#           below   above
# Predicted:
# below 
# above

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos
model_info$error = mean(test$costs_bin != nb_kde_pred)

testMSE_nb_kde = model_info$error
truepos_nb_kde = model_info$truepos
trueneg_nb_kde = model_info$trueneg
falsepos_nb_kde = model_info$falsepos
falseneg_nb_kde = model_info$falseneg
```

## Decision Tree (with pruning)

```{r}
library(tree)

set.seed(1)

train = df_train %>%
  select(-total_costs) %>%
  mutate(costs_bin = as.factor(costs_bin))

test = df_test %>%
  select(-total_costs) %>%
  mutate(costs_bin = as.factor(costs_bin))

# Original tree 
tree.costs = tree(costs_bin ~ ., data = select(train,-fold))
summary(tree.costs)
plot(tree.costs)
text(tree.costs)
# Original errors
tree.pred = predict(tree.costs,test,type="class")
testerror = mean(  (test$costs_bin != tree.pred)  )

# Optimal number of nodes
cv.costs = cv.tree(tree.costs)
plot(cv.costs$size, cv.costs$dev, type = "b")
optimalnodes = cv.costs$size[which.min(cv.costs$dev)]
# Prune tree with optimal number of nodes 
prune.costs = prune.tree(tree.costs,best=optimalnodes)
# Compute new errors 
prune.pred = predict(prune.costs,test,type="class")
testerrorprune = mean(  (test$costs_bin != prune.pred)  )


# Now, fit larger tree with pruning to see if we get better results 
tree.costs.full = tree(costs_bin ~ ., data = select(train,-fold),
                        control = tree.control(nrow(train), mindev = 0))
cv.costs.full = cv.tree(tree.costs.full)
plot(cv.costs.full$size, cv.costs.full$dev, type = "b")
optimalnodesfull = cv.costs.full$size[which.min(cv.costs.full$dev)]

prune.costs.full = prune.tree(tree.costs.full,best=optimalnodesfull)
# Compute new errors 
prune.pred.full = predict(prune.costs.full,test,type="class")
testerrorprunefull = mean(  (test$costs_bin != prune.pred.full) )
```
Best number of terminal nodes is 6.

k fold CV 
```{r}
compute_MSEP_regression_tree <- function(train_data,i){
    current_train_folds <- filter(train_data, fold != i)
    current_test_fold <- filter(train_data, fold == i)
    
    tree.costs = tree(costs_bin ~ ., data = select(current_train_folds,-fold))
    prune.costs = prune.tree(tree.costs,best = 6)
    
    prune.pred = predict(prune.costs,select(current_test_fold,-fold),type="class")
    
    return (mean((current_test_fold$costs_bin != prune.pred)) )
}

cv_decision_tree = mean(map_dbl(1:10,~compute_MSEP_regression_tree(train,.x)))
```

Test error 
```{r}
tree.costs = tree(costs_bin ~ ., data = select(train,-fold))
prune.costs = prune.tree(tree.costs,best = 6)

prune.pred = predict(prune.costs,test,type="class")

model_info = tibble(.rows=1)

contingency = table(prune.pred,test$costs_bin)
#           Actual:           
#           below   above
# Predicted:
# below 
# above

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos
model_info$error = mean(test$costs_bin != prune.pred)

testMSE_decision_tree = model_info$error
truepos_decision_tree = model_info$truepos
trueneg_decision_tree = model_info$trueneg
falsepos_decision_tree = model_info$falsepos
falseneg_decision_tree = model_info$falseneg

# testMSE_regression_tree <- compute_testMSE_regression_tree(train,test)
```

## Bagging (with variable importance)

```{r}
library(randomForest)

set.seed(1)

train = df_train %>%
  select(-total_costs) %>%
  mutate(costs_bin = as.factor(costs_bin))

test = df_test %>%
  select(-total_costs) %>%
  mutate(costs_bin = as.factor(costs_bin))
```

k-fold CV
```{r}
compute_MSEP_bagging <- function(train_data,i){
    current_train_folds <- filter(train_data, fold != i)
    current_test_fold <- filter(train_data, fold == i)
    
    bag.costs <- randomForest(costs_bin ~ ., data = select(current_train_folds,-fold),
                              mtry = ncol(current_train_folds)-2,importance = TRUE  )
    bag.pred = predict(bag.costs,select(current_test_fold,-fold),type="class")
    
    return (mean(current_test_fold$costs_bin != bag.pred) )
}

cv_bagging = mean(map_dbl(1:10,~compute_MSEP_bagging(train,.x)))
```

```{r}
bag.costs <- randomForest(costs_bin ~ ., data = select(train,-fold), mtry = ncol(train)-2,importance = TRUE  )

bag.pred = predict(bag.costs,test,type="class")

mean_decrease_Gini = importance(bag.costs,type=2)
varImpPlot(bag.costs)

contingency = table(bag.pred,test$costs_bin)
model_info = tibble(.rows=1)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos
model_info$error = mean(test$costs_bin != bag.pred)

testMSE_bagging = model_info$error
truepos_bagging = model_info$truepos
trueneg_bagging = model_info$trueneg
falsepos_bagging = model_info$falsepos
falseneg_bagging = model_info$falseneg

```

## Random Forest (with variable importance)

```{r}
library(randomForest)

set.seed(1)

train = df_train %>%
  select(-total_costs) %>%
  mutate(costs_bin = as.factor(costs_bin))

test = df_test %>%
  select(-total_costs) %>%
  mutate(costs_bin = as.factor(costs_bin))
```

k-fold CV
```{r}
compute_MSEP_random_forest <- function(train_data,i){
    current_train_folds <- filter(train_data, fold != i)
    current_test_fold <- filter(train_data, fold == i)
    
    random_forest.costs <- randomForest(costs_bin ~ ., data = select(current_train_folds,-fold),
                              mtry = sqrt(ncol(current_train_folds)-2),importance = TRUE  )
    random_forest.pred = predict(random_forest.costs,select(current_test_fold,-fold),type="class")
    
    return (mean(current_test_fold$costs_bin != random_forest.pred) )
}

# cv_random_forest = mean(map_dbl(1:10,~compute_MSEP_random_forest(train,.x)))
cv_random_forest = 0.03680463
```

```{r}
random_forest.costs <- randomForest(costs_bin ~ ., data = select(train,-fold),
                                    mtry = sqrt(ncol(current_train_folds)-2),
                                    importance = TRUE  )

random_forest.pred = predict(random_forest.costs,test,type="class")

mean_decrease_Gini = importance(random_forest.costs,type=2)
varImpPlot(random_forest.costs)

contingency = table(random_forest.pred,test$costs_bin)
model_info = tibble(.rows=1)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos
model_info$error = mean(test$costs_bin != random_forest.pred)

testMSE_random_forest = model_info$error
truepos_random_forest = model_info$truepos
trueneg_random_forest = model_info$trueneg
falsepos_random_forest = model_info$falsepos
falseneg_random_forest = model_info$falseneg

```

## Boosting (including selecting the tuning parameter)

```{r}
library(gbm)

set.seed(1)

train = df_train %>%
  select(-total_costs) 

test = df_test %>%
  select(-total_costs) 
  
# boost.costs = gbm(costs_bin ~ ., data = select(train,-fold),
#                   distribution = "bernoulli",n.trees = 1000)
# summary(boost.costs)
# 
# boost.pred = round(predict(boost.costs,test,type="response"))
# error = mean(boost.pred != test$costs_bin)
# 
# plot(boost.costs,i="salaries")
# plot(boost.costs,i="fte_employees_on_payroll")
```

k-fold CV
```{r}
compute_MSEP_boosting <- function(train_data,lambda,i){
    current_train_folds <- filter(train_data, fold != i)
    current_test_fold <- filter(train_data, fold == i)
    
    boost.costs = gbm(costs_bin ~ ., data = select(current_train_folds,-fold),
                  distribution = "bernoulli",n.trees = 1000,shrinkage = lambda)
    boost.pred = round(predict(boost.costs,select(current_test_fold,-fold),
                               type="response"))
    
    return (mean(current_test_fold$costs_bin != boost.pred) )
}

kfold_cv_boosting <- function(train_data,lambda){
  cv_boosting = mean(map_dbl(1:10,~compute_MSEP_boosting(train_data,
                                                         lambda = lambda,.x)))
  return (cv_boosting)
}

lambda_grid = seq(0.01,0.1,0.005)
# Commented line out b/c it takes so long 
# lambda_errors = map_dbl(lambda_grid,~kfold_cv_boosting(train,.x))

# optimal_lambda = lambda_grid[which.min(lambda_errors)]
optimal_lambda = 0.08
# cv_boosting = min(lambda_errors)
cv_boosting = 0.03601762
```

Test Error 
```{r}
boost.costs = gbm(costs_bin ~ ., data = select(train,-fold),
                  distribution = "bernoulli",n.trees = 5000,shrinkage = optimal_lambda)

boost.pred = round(predict(boost.costs,test,type="response"))
    
contingency = table(boost.pred,test$costs_bin)

model_info = tibble(.rows=1)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos
model_info$error = mean(test$costs_bin != boost.pred)

testMSE_boosting = model_info$error
truepos_boosting = model_info$truepos
trueneg_boosting = model_info$trueneg
falsepos_boosting = model_info$falsepos
falseneg_boosting = model_info$falseneg

```

## Neural Network

```{r}
train = df_train %>%
  select(-total_costs)

test = df_test %>%
  select(-total_costs)


# Neural Network Model 
modelnn <- keras_model_sequential()
modelnn %>%
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = "softmax")

modelnn %>% 
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(learning_rate = 0.01), 
    metrics = "categorical_accuracy"
    )
```


10-fold CV
```{r}
compute_MSEP_neural_network = function(train_data,nn,i){
  current_train_folds.x = filter(train_data, fold != i) %>% select(-c(costs_bin,fold))
  current_train_folds.x = array_reshape(current_train_folds.x,
                                               c(nrow(current_train_folds.x),ncol(current_train_folds.x)))
  current_train_folds.y = filter(train_data, fold != i) %>% 
    select(costs_bin) %>%
    to_categorical(,num_classes=2)
  
  current_test_fold.x = filter(train_data, fold == i) %>% select(-c(costs_bin,fold))
  current_test_fold.x = array_reshape(current_test_fold.x,
                                               c(nrow(current_test_fold.x),ncol(current_test_fold.x)))
  current_test_fold.y = filter(train_data, fold == i) %>% 
    select(costs_bin) %>%
    to_categorical(,num_classes=2)
  
  history = nn %>%
  fit(current_train_folds.x,
      current_train_folds.y,
      epochs = 10,
      batch_size = 32,
      validation_data = list(current_test_fold.x,current_test_fold.y)
      )
  
  error = 1- history[["metrics"]][["val_categorical_accuracy"]][10]
  return(error)
} 

cv_neural_network = mean(map_dbl(1:10,~compute_MSEP_neural_network(train,modelnn,.x)))

```

Test error 
```{r}
library(keras)

set.seed(1)

train_x = df_train %>%
  select(-c(total_costs,costs_bin,fold)) 
train_x = array_reshape(train_x,c(nrow(train_x),ncol(train_x)))

train_y = df_train %>%
  select(costs_bin) %>%
  to_categorical(,num_classes=2)

test_x = df_test %>%
  select(-c(total_costs,costs_bin))
test_x = array_reshape(test_x,c(nrow(test_x),ncol(test_x)))

test_y = df_test %>%
  select(costs_bin) %>%
  to_categorical(,num_classes=2)


modelnn <- keras_model_sequential()
modelnn %>%
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = "softmax")

modelnn %>% 
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(learning_rate = 0.01), 
    metrics = "categorical_accuracy"
    )

history = modelnn %>%
  fit(train_x,
      train_y,
      epochs = 10,
      batch_size = 32,
      validation_data = list(test_x,test_y)
      )

plot(history, smooth = FALSE)

# Using the metrics from the validation set 
avg_test_error = 1- history[["metrics"]][["val_categorical_accuracy"]][10]


# Using the predict function to predict the class labels 
nn.pred_one_hot = modelnn %>% predict(test_x,batch_size = 32) %>% round
nn.pred = nn.pred_one_hot[,2]

contingency = table(nn.pred,df_test$costs_bin)
truepos_neural_net = contingency[2,2] / (contingency[1,2] + contingency[2,2])
trueneg_neural_net = contingency[1,1] / (contingency[2,1] + contingency[1,1])
falsepos_neural_net = 1 - trueneg_neural_net
falseneg_neural_net = 1 - truepos_neural_net
error_neural_net = mean(df_test$costs_bin != nn.pred)

testMSE_neural_net = avg_test_error
```

## Qualitative summary
```{r}

method <- c("KNN","Multiple Logistic Regression","LDA","QDA","Naive Bayes (Gaussian)",
            "Naive Bayes (KDE)","Decision Tree","Basgging","Random Forest",
            "Boosting","Neural Net")

cv_error <- c(cv_knn,cv_glm,cv_lda,cv_qda,cv_nb,cv_nb_kde,cv_decision_tree,
              cv_bagging,cv_random_forest,cv_boosting)

error_rates <- c(testMSE_knn,testMSE_glm,testMSE_lda,testMSE_qda,testMSE_nb,
                testMSE_nb_kde,testMSE_decision_tree,testMSE_bagging,
                testMSE_random_forest,testMSE_boosting,testMSE_neural_net)

true_pos <- c(truepos_knn,truepos_glm,truepos_lda,truepos_qda,truepos_nb,
              truepos_nb_kde,truepos_decision_tree,truepos_bagging,
              truepos_random_forest,truepos_boosting,truepos_neural_net)

true_neg <- c(trueneg_knn,trueneg_glm,trueneg_lda,trueneg_qda,trueneg_nb,
              trueneg_nb_kde,trueneg_decision_tree,trueneg_bagging,
              trueneg_random_forest,trueneg_boosting,trueneg_neural_net)

false_pos <- c(falsepos_knn,falsepos_glm,falsepos_lda,falsepos_qda,falsepos_nb,
              falsepos_nb_kde,falsepos_decision_tree,falsepos_bagging,
              falsepos_random_forest,falsepos_boosting,falsepos_neural_net)

false_neg <- c(falseneg_knn,falseneg_glm,falseneg_lda,falseneg_qda,falseneg_nb,
              falseneg_nb_kde,falseneg_decision_tree,falseneg_bagging,
              falseneg_random_forest,falseneg_boosting,falseneg_neural_net)

error_summary <- tibble(method,cv_error,error_rates,true_pos,true_neg,
                        false_pos,false_neg)

knitr::kable(error_summary)
```

# Bootstrap CI

# Simulation Study

```{r}
library(faux)

factor_scaled = scaled_hpc_dummie

sim = sim_df(scaled_hpc_dummies,500)

sim

```

