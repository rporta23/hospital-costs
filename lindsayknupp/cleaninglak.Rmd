---
title: "Data Cleaning"
author: "Rose Porta"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r}
hpc <- read_csv(here::here("hpc.csv"))
#summary(hpc)

hpcdf <- hpc |> 
  mutate(start = as.Date(`Fiscal Year Begin Date`),end = as.Date(`Fiscal Year End Date`)) |> 
  mutate(days = as.numeric(end - start)) |> 
  mutate(numBeds = `Total Bed Days Available`/days,id = row_number())
```

```{r}
hpc_clean <- hpcdf |> 
  janitor::clean_names() |> 
  select(provider_ccn, days, number_of_beds,
         total_costs, rural_versus_urban, provider_type, type_of_control,
         fte_employees_on_payroll, total_days = total_days_v_xviii_xix_unknown,
         total_discharges = total_discharges_v_xviii_xix_unknown,
         total_income, total_assets,
         salaries = total_salaries_from_worksheet_a) |> 
  na.omit() |> 
  mutate(rural = ifelse(rural_versus_urban == "R", 1, 0),
         control_bin = case_when(
           type_of_control < 3 ~ "Voluntary",
           (type_of_control >= 3 & type_of_control < 7) ~ "Proprietary",
           type_of_control >= 7 ~ "Governmental"
         ),
         provider_bin = ifelse(provider_type < 3 |
                                 provider_type == 6,
                               "General", "Specialized"
                               ),
         ) |> 
  select(- c(rural_versus_urban, type_of_control, provider_type))

duplicates <- hpc_clean |> 
  group_by(provider_ccn) |> 
  summarise(count = n()) |> 
  filter(count > 1)

dup <- map_lgl(hpc_clean$provider_ccn, `%in%`, duplicates$provider_ccn)

hpc1 <- hpc_clean |> 
  filter(!dup)

hpc2 <- hpc_clean |> 
  filter(dup) |> 
  group_by(provider_ccn) |> 
  summarise(
    days = sum(days),
    number_of_beds = mean(number_of_beds),
    total_costs = sum(total_costs),
    fte_employees_on_payroll = mean(fte_employees_on_payroll),
    total_days = sum(total_days),
    total_discharges = sum(total_discharges),
    total_income = sum(total_income),
    total_assets = mean(total_assets),
    salaries = sum(salaries),
    rural = max(rural),
    control_bin = max(control_bin),
    provider_bin = max(provider_bin)
  )

hpc_all <- bind_rows(hpc1, hpc2)

hpc_normalize <- hpc_all |> 
  mutate(
    total_costs = total_costs/days,
    inpatients = total_days/days,
    total_discharges = total_discharges/days,
    total_income = total_income/days,
    salaries = salaries/days
    )
```

```{r}
hpc_dummies <- hpc_normalize |> 
  select(-c(provider_ccn, days)) |> 
  mutate(
    costs_bin = ifelse(total_costs > median(total_costs), 1, 0)
    ) |> 
  tidytable::get_dummies(drop_first = TRUE) |> 
  select(where(is.numeric))
```

Codes for type of Control:

1 = Voluntary Non‐Profit‐Church

2 = Voluntary Non‐Profit‐Other

3 = Proprietary‐Individual

4 = Proprietary‐Corporation

5 = Proprietary‐Partnership

6 = Proprietary‐Other

7 = Governmental‐Federal

8 = Governmental‐City‐County

9 = Governmental‐County

10 = Governmental‐State

11 = Governmental‐Hospital District

12 = Governmental‐City

13 = Governmental‐Other

Codes for Provider Type:

1 = General Short Term (includes CAHs)

2 = General Long Term

3 = Cancer

4 = Psychiatric

5 = Rehabilitation

6 = Religious Non‐Medical Health Care Institution

7 = Children

8 = Reserved for Future Use

9 = Other

10 = Extended Neoplastic Disease Care

11 = Indian Health Services

12 = Rural Emergency Hospital.

# Exploratory Analysis

```{r}
ggplot(hpc_clean, aes(x = total_costs)) +
  geom_histogram() +
  geom_vline(xintercept = median(hpc_clean$total_costs), color = "red") + 
  geom_vline(xintercept = mean(hpc_clean$total_costs), color = "blue")
```

Distribution of response total costs is extremely right-skewed.

Red line = median, blue line = mean

Summary Statistics:

```{r}
summary(hpc_clean)
```

Pairs plot:

```{r}
hpc_dummies %>%
  select(-set) %>%
  pairs()
pairs(hpc_dummies)

# Try pairs plot with log of total costs
data_pairs <- hpc_dummies |> 
  select(-c(rural, costs_bin, control_bin_Governmental,
            control_bin_Voluntary, provider_bin_Specialized,
            set)
         ) |> 
  mutate(total_costs = log(total_costs))

pairs(data_pairs)
```

-   very high pairwise positive collinearity between:
    -   total_days and bed_days, and total_discharges
    -   salaries and employees on payroll
-   total_costs appears to have fairly strong positive linear relationships with:
    -   fte_employees_on_payroll
    -   total_days
    -   bed_days
    -   total_discharges
    -   salaries
-   total_costs appears to have weak or no relationship with:
    -   total_income
    -   total_assets
-   hard to discern any relationships for the categorical predictors
-   for binary response, the predictors which have positive relationships with total cost have a pattern such that reponse = 0 corresponds to a higher concentration of points with low values of the predictor response = 1 corresponds to a wider range of values for the predictor. This same pattern shows up also for total assets and total income, which did not look associated with continuous total cost.

# Train-test Split

```{r}
set.seed(1)

train_prop <- 0.9

n <- nrow(hpc_dummies)
n_train <- train_prop*n
n_test <- n - n_train

hpc_dummies$set <- "Train"
hpc_dummies$set[sample(n, n_test, replace = FALSE)] <- "Test"
  
df_train <- hpc_dummies |>  filter(set == "Train") |> select(-set)
df_test <- hpc_dummies |> filter(set == "Test") |> select(-set)
```

# Train set-fold assignment 
```{r}
set.seed(1)

n_train <- nrow(df_train)

k <- 10
k.o <- k
kvals <- unique(round(n_train/(1L:floor(n_train/2))))
temp <- abs(kvals - k)
if (!any(temp == 0))  { k <- kvals[temp == min(temp)][1L] }
f <- ceiling(n_train/k)

df_train$fold <- sample(rep(1L:k, f), n_train)
```


# For all methods:

-   For each method you apply, use 10 fold cross-validation estimate for the test error.
-   discuss assumptions


# Quantitative Outcome Analyses

## Marginal simple linear regressions

```{r}
predictors <- select(df_train, -c(total_costs, costs_bin))

# model summaries
map(predictors, ~summary(lm(total_costs ~ .x, data = df_train)))

simple_models <- map(predictors, ~lm(total_costs ~ .x, data = df_train))
```

10-fold cv

```{r}
# Estimate with k-fold:

predictor_names <- colnames(predictors)

formulas <- str_c("total_costs", " ~ ", predictor_names)


train_data = df_train

formula = formulas[1]

i = 1

compute_MSEP_lm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(formula, data = current_train_folds)
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs, costs_bin) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

#compute_MSEP_lm(train_data, formula, i)

kfold_cv_lm <- function(train_data, formula, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
  
}

kfold_cv_lm(train_data, formula)

test_error_vec <- map_dbl( formulas, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

test_error_vec
```

## Multiple linear regression

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you
```{r}
qual_predictor_names = c(predictor_names[1:7],predictor_names[9])
predictor_names

multiple_lm <- summary(lm(total_costs ~ number_of_beds + fte_employees_on_payroll
                          + total_days + total_discharges + total_income 
                          + total_assets + salaries + rural + inpatients
                          + control_bin_Governmental + control_bin_Proprietary
                          + provider_bin_Specialized, data = df_train))

transformation_lm <- summary(lm(total_costs ~ fte_employees_on_payroll*total_income, data = df_train))

polynomial_lm <- summary(lm(total_costs ~ salaries + I(salaries^2),data = df_train))

ggplot(df_train) + 
  geom_point(aes(x=number_of_beds,y=total_costs))
```


## Regression Tree (with pruning)

## Bagging (with variable importance)

## Random Forest (with variable importance)

## Boosting (including selecting the tuning parameter)

## Neural Network

# Variable Selection

## Best Subset

```{r}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin,-fold)

regfit_full <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "exhaustive")
summary(regfit_full)
```

k-fold CV 
```{r}
compute_MSEP_lm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(formula, data = current_train_folds)
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs, costs_bin, fold) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

kfold_cv_lm <- function(train_data, formula, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
  
}
```

```{r}
best_subset_cv <- tibble("avgMSEP" = rep(NA,12))

# k-fold CV for 1 predictor: salaries
formula = "total_costs ~ salaries"
best_subset_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(df_train, formula = .x, k = 10))

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
best_subset_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
best_subset_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
best_subset_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
best_subset_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
best_subset_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
best_subset_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
best_subset_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
best_subset_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
best_subset_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
best_subset_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
best_subset_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
```

## Forward Stepwise
```{r}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin,-fold)

regfit_fwd <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "forward")
summary(regfit_fwd)
```

Forward stepwise CV
```{r}
forward_stepwise_cv <- tibble("avgMSEP" = rep(NA,12))

# k-fold CV for 1 predictor: 
formula = "total_costs ~ salaries"
forward_stepwise_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(df_train, formula = .x, k = 10))

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
forward_stepwise_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
forward_stepwise_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
forward_stepwise_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
forward_stepwise_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
forward_stepwise_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
forward_stepwise_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
forward_stepwise_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
forward_stepwise_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
forward_stepwise_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
forward_stepwise_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
forward_stepwise_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
```

## Backward Stepwise

```{r}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin,-fold)

regfit_bwd <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "backward")
summary(regfit_bwd)
```

Backward stepwise CV 
```{r}
backward_stepwise_cv <- tibble("avgMSEP" = rep(NA,12))

# k-fold CV for 1 predictor: 
formula = "total_costs ~ salaries"
backward_stepwise_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(df_train, formula = .x, k = 10))

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
backward_stepwise_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
backward_stepwise_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
backward_stepwise_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
backward_stepwise_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
backward_stepwise_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
backward_stepwise_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
backward_stepwise_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
backward_stepwise_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
backward_stepwise_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
backward_stepwise_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
backward_stepwise_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
```

(For the above 3 methods, determine which model of a given size is best by comparing the 10-fold cross validation estimate of the test error.)

## Ridge regression (find the best tuning parameter using cross-validation)

```{r}
library(glmnet)

set.seed(1)
train <- df_train %>%
  select(-c(costs_bin,fold))

numerical_pred <- train %>%
  select(-c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized))

qualitative_pred <- train %>%
  select(c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized,total_costs))

numerical_pred_mat = scale(model.matrix(total_costs ~ ., data = numerical_pred)[ , -1])
qualitative_pred_mat = model.matrix(total_costs ~ ., data = qualitative_pred)[ , -1]

design_mat = cbind(numerical_pred_mat,qualitative_pred_mat)

x <- design_mat
y <- train$total_costs

# Lasso model 
lambda_grid <- 10 ^ seq(10, -2, length = 100)
lasso_reg <- glmnet(x, y, alpha = 0, lambda = lambda_grid) 

# Perform 10-fold cross validation 
cv.out <- cv.glmnet(x, y, alpha = 0)
bestmean <- min(cv.out$cvm) # lowest error 
bestlambda <- cv.out$lambda[which(cv.out$cvm == bestmean)]
```

## Lasso (find the best tuning parameter using cross-validation)

```{r}
library(glmnet)

set.seed(1)
train <- df_train %>%
  select(-c(costs_bin,fold))

numerical_pred <- train %>%
  select(-c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized))

qualitative_pred <- train %>%
  select(c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized,total_costs))

numerical_pred_mat = scale(model.matrix(total_costs ~ ., data = numerical_pred)[ , -1])
qualitative_pred_mat = model.matrix(total_costs ~ ., data = qualitative_pred)[ , -1]

design_mat = cbind(numerical_pred_mat,qualitative_pred_mat)

x <- design_mat
y <- train$total_costs

# Lasso model 
lambda_grid <- 10 ^ seq(10, -2, length = 100)
lasso_reg <- glmnet(x, y, alpha = 1, lambda = lambda_grid) 

# Perform 10-fold cross validation 
cv.out <- cv.glmnet(x, y, alpha = 1)
bestmean <- min(cv.out$cvm) # lowest error 
bestlambda <- cv.out$lambda[which(cv.out$cvm == bestmean)]
```

## Principal Components Regression (PCR)

```{r}
library(pls)

set.seed(1)
train <- df_train %>%
  select(-c(costs_bin,fold))

test <- df_test %>%
  select(-c(costs_bin))

numerical_pred <- train %>%
  select(-c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized))

qualitative_pred <- train %>%
  select(c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized,total_costs))

numerical_pred_mat = scale(model.matrix(total_costs ~ ., data = numerical_pred)[ , -1])
qualitative_pred_mat = model.matrix(total_costs ~ ., data = qualitative_pred)[ , -1]

response_mat = train %>%
  select(total_costs)%>%
  as.matrix()

design_df = as.data.frame(cbind(numerical_pred_mat,qualitative_pred_mat,response_mat))

pcr.fit <-pcr(total_costs ~ ., data = design_df, scale = FALSE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")

pcr.pred <-predict(pcr.fit, test, ncomp = 12)
mean((pcr.pred- test$total_costs)^2)
```

# Qualitative Outcome Analyses

## KNN

```{r}
library(class)

set.seed(1)

train <- df_train %>%
  select(-c(total_costs))

compute_error_knn <- function(train_data, i, neighbors){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)

  knn_pred <- knn(
      train = select(current_train_folds, -c(costs_bin,fold)),
      test = select(current_test_fold, -c(costs_bin,fold)),
      cl = current_train_folds$costs_bin,
      k = neighbors,
      prob = TRUE
    )
  
  
  error = mean(current_test_fold$costs_bin != knn_pred)
  return (error)
}

# for fold number = 1,2,3,4,5,6,7,8,9,10 and then avg all the errors together
kfold_cv_knn <- function(train_data,neighbors, k = 10) {
  error_vec <- map_dbl(1:k, ~compute_error_knn(train, .x,neighbors))
  avg_error <- mean(error_vec)
  return(avg_error)
}

neighbors <- 1:100 
knn_error_rates = map_dbl(neighbors, ~kfold_cv_knn(train,neighbors = .x))

optimal_n <- which(knn_error_rates == min(knn_error_rates))
optimal_error_rate <- knn_error_rates[optimal_n]

optimal_n
optimal_error_rate
```

## Multiple logistic regression

```{r}
set.seed(1)

predictors <- select(df_train, -c(total_costs, costs_bin,fold))

train = df_train %>%
  select(-c(total_costs))

# model summaries
# map(predictors, ~summary(glm(costs_bin ~ .x, data = select(train,-fold),family=binomial)))

# simple_logistic_models <- map(predictors, ~glm(costs_bin ~ .x, data = select(train,-fold),family=binomial))
```

10-fold cv

```{r}
# Estimate with k-fold:

predictor_names <- colnames(predictors)

formulas <- str_c("costs_bin", " ~ ", predictor_names)

compute_MSEP_glm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- glm(formula, data = select(current_train_folds,-fold),family = binomial)
  pred <- predict(mod, newdata = select( current_test_fold, -c(fold,costs_bin) ) )
    
  MSEP <- mean( (current_test_fold$costs_bin - pred) ^ 2 )
  return(MSEP)
}

kfold_cv_glm <- function(train_data, formula, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_glm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

test_error_vec <- map_dbl( formulas, ~kfold_cv_lm(train, formula = .x, k = 10) )
test_error_vec

optimal_idx = which(test_error_vec == min(test_error_vec))
formulas[optimal_idx]
test_error_vec[optimal_idx]
```

```{r}

```


-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

## LDA

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

compute_error_lda <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  lda_model <- MASS::lda(costs_bin ~ ., data = select(current_train_folds,-fold))
  lda_pred <- predict(lda_model, newdata = select(current_test_fold, -c(fold,costs_bin)))$class
  
  contingency = table(lda_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != lda_pred)
  return (model_info)
}

kfold_cv_lda = map_df(1:10, ~compute_error_lda(train,.x))
(mean(kfold_cv_lda$error))

# table(c(1,2),c(3,4))
# table(current_test_fold$costs_bin,lda_pred)
```

## QDA

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

compute_error_qda <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  qda_model <- MASS::qda(costs_bin ~ ., data = select(current_train_folds,-fold))
  qda_pred <- predict(qda_model, newdata = select(current_test_fold, -c(fold,costs_bin)))$class
  
  contingency = table(qda_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != qda_pred)
  return (model_info)
}

kfold_cv_qda = map_df(1:10, ~compute_error_qda(train,.x))
(mean(kfold_cv_qda$error))

# table(c(1,2),c(3,4))
# table(current_test_fold$costs_bin,lda_pred)
```

## Naive Bayes (at least two kernels)

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

compute_error_nb <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  nb_model <- e1071::naiveBayes(costs_bin ~ ., data = select(current_train_folds,-fold))
  nb_pred <- predict(nb_model, newdata = select(current_test_fold, -c(fold,costs_bin)))
  
  contingency = table(nb_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != nb_pred)
  return (model_info)
}

kfold_cv_nb = map_df(1:10, ~compute_error_nb(train,.x))
(mean(kfold_cv_nb$error))
```

2nd kernel 
```{r}
set.seed(1)


train = df_train %>%
  select(-total_costs)

compute_error_nb_kde <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  current_train_matrix = current_train_folds %>%
    select(-c(fold,costs_bin)) %>%
    as.matrix()
  
  current_train_matrixy = current_train_folds %>%
    select(costs_bin) %>%
    as.matrix()
  
  current_test_matrix = current_test_fold %>%
    select(-c(fold,costs_bin)) %>%
    as.matrix()
  
  model_info = tibble(.rows=1)
  
  nb_kde_model <-  naivebayes::nonparametric_naive_bayes(y = as.factor(current_train_matrixy),
                                       x = current_train_matrix
    )
  nb_kde_pred <- predict(nb_kde_model, newdata = current_test_matrix )

  # nb_kde_model <- e1071::naiveBayes(costs_bin ~ ., data = select(current_train_folds,-fold))
  # nb_kde_pred <- predict(nb_kde_model, newdata = select(current_test_fold, -c(fold,costs_bin)))
  
  contingency = table(nb_kde_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos
  model_info$error = mean(current_test_fold$costs_bin != nb_kde_pred)
  return (model_info)
}

kfold_cv_nb_kde = map_df(1:10, ~compute_error_nb_kde(train,.x))
(mean(kfold_cv_nb_kde$error))
```



## Decision Tree (with pruning)

## Bagging (with variable importance)

## Random Forest (with variable importance)

## Boosting (including selecting the tuning parameter)

(For the above three methods, calculate the Gini index on each leaf of the final tree to examine the purity of the node.)

## Neural Network

## Calculate True/False Positive/Negative rates for each method

# Bootstrap CI

# Simulation Study
