---
title: "Data Cleaning"
author: "Rose Porta"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r}
hpc <- read_csv(here::here("hpc.csv"))
#summary(hpc)

hpcdf <- hpc |> 
  mutate(start = as.Date(`Fiscal Year Begin Date`),end = as.Date(`Fiscal Year End Date`)) |> 
  mutate(days = as.numeric(end - start)) |> 
  mutate(numBeds = `Total Bed Days Available`/days,id = row_number())
```

```{r}
hpc_clean <- hpcdf |> 
  janitor::clean_names() |> 
  select(provider_ccn, days, number_of_beds,
         total_costs, rural_versus_urban, provider_type, type_of_control,
         fte_employees_on_payroll, total_days = total_days_v_xviii_xix_unknown,
         total_discharges = total_discharges_v_xviii_xix_unknown,
         total_income, total_assets,
         salaries = total_salaries_from_worksheet_a) |> 
  na.omit() |> 
  mutate(rural = ifelse(rural_versus_urban == "R", 1, 0),
         control_bin = case_when(
           type_of_control < 3 ~ "Voluntary",
           (type_of_control >= 3 & type_of_control < 7) ~ "Proprietary",
           type_of_control >= 7 ~ "Governmental"
         ),
         provider_bin = ifelse(provider_type < 3 |
                                 provider_type == 6,
                               "General", "Specialized"
                               ),
         ) |> 
  select(- c(rural_versus_urban, type_of_control, provider_type))

duplicates <- hpc_clean |> 
  group_by(provider_ccn) |> 
  summarise(count = n()) |> 
  filter(count > 1)

dup <- map_lgl(hpc_clean$provider_ccn, `%in%`, duplicates$provider_ccn)

hpc1 <- hpc_clean |> 
  filter(!dup)

hpc2 <- hpc_clean |> 
  filter(dup) |> 
  group_by(provider_ccn) |> 
  summarise(
    days = sum(days),
    number_of_beds = mean(number_of_beds),
    total_costs = sum(total_costs),
    fte_employees_on_payroll = mean(fte_employees_on_payroll),
    total_days = sum(total_days),
    total_discharges = sum(total_discharges),
    total_income = sum(total_income),
    total_assets = mean(total_assets),
    salaries = sum(salaries),
    rural = max(rural),
    control_bin = max(control_bin),
    provider_bin = max(provider_bin)
  )

hpc_all <- bind_rows(hpc1, hpc2)

hpc_normalize <- hpc_all |> 
  mutate(
    total_costs = total_costs/days,
    inpatients = total_days/days,
    total_discharges = total_discharges/days,
    total_income = total_income/days,
    salaries = salaries/days
    )
```

```{r}
hpc_dummies <- hpc_normalize |> 
  select(-c(provider_ccn, days)) |> 
  mutate(
    costs_bin = ifelse(total_costs > median(total_costs), 1, 0)
    ) |> 
  tidytable::get_dummies(drop_first = TRUE) |> 
  select(where(is.numeric))
```

Scaled hpc_dummies (quantiative predictors are scaled)
```{r}
quantitative_pred <- hpc_dummies %>%
  select(-c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized,costs_bin)) %>%
  scale()

qualitative_pred <- hpc_dummies %>%
  select(c(rural,control_bin_Governmental,control_bin_Proprietary,provider_bin_Specialized,costs_bin))

scaled_hpc_dummies = cbind(quantitative_pred,qualitative_pred)

```

Codes for type of Control:

1 = Voluntary Non‐Profit‐Church

2 = Voluntary Non‐Profit‐Other

3 = Proprietary‐Individual

4 = Proprietary‐Corporation

5 = Proprietary‐Partnership

6 = Proprietary‐Other

7 = Governmental‐Federal

8 = Governmental‐City‐County

9 = Governmental‐County

10 = Governmental‐State

11 = Governmental‐Hospital District

12 = Governmental‐City

13 = Governmental‐Other

Codes for Provider Type:

1 = General Short Term (includes CAHs)

2 = General Long Term

3 = Cancer

4 = Psychiatric

5 = Rehabilitation

6 = Religious Non‐Medical Health Care Institution

7 = Children

8 = Reserved for Future Use

9 = Other

10 = Extended Neoplastic Disease Care

11 = Indian Health Services

12 = Rural Emergency Hospital.

# Exploratory Analysis

```{r}
ggplot(hpc_dummies, aes(x = total_costs)) +
  geom_histogram() +
  geom_vline(xintercept = median(hpc_dummies$total_costs), color = "red") + 
  geom_vline(xintercept = mean(hpc_dummies$total_costs), color = "blue")
```

Distribution of response total costs is extremely right-skewed.

Red line = median, blue line = mean

Summary Statistics:

```{r}
summary(hpc_clean)
```

Pairs plot:

```{r}
pairs(hpc_dummies)
```

-   very high pairwise positive collinearity between:
    -   total_days and bed_days, and total_discharges
    -   salaries and employees on payroll
-   total_costs appears to have fairly strong positive linear relationships with:
    -   fte_employees_on_payroll
    -   total_days
    -   bed_days
    -   total_discharges
    -   salaries
-   total_costs appears to have weak or no relationship with:
    -   total_income
    -   total_assets
-   hard to discern any relationships for the categorical predictors
-   for binary response, the predictors which have positive relationships with total cost have a pattern such that reponse = 0 corresponds to a higher concentration of points with low values of the predictor response = 1 corresponds to a wider range of values for the predictor. This same pattern shows up also for total assets and total income, which did not look associated with continuous total cost.

```{r}
library(plotly)

ggplot(hpc_dummies) +
  geom_histogram(aes(x = total_income))

negative = hpc_dummies %>%
  filter(total_income < 0) 

positive = hpc_dummies %>%
  filter(total_income > 0) 

hpc_dummies %>%
  filter(total_income < 3000000) %>%
  mutate(status = ifelse(total_income < 0, "in the red","in the green")) %>%
  plot_ly() %>%
  add_histogram(
    x = ~total_income,
    color = ~status
  )

negative %>%
  mutate(provider = ifelse(provider_bin_Specialized == 0,"General health","Specialized")) %>%
ggplot() +
geom_histogram(aes(x=total_income,fill = provider),alpha = 0.3)

negative %>%
  summarise(num_Specialized = sum(provider_bin_Specialized),
         num_General = nrow(negative) - num_Specialized)

positive %>%
  mutate(provider = ifelse(provider_bin_Specialized == 0,"General health","Specialized")) %>%
ggplot() +
geom_histogram(aes(x=total_income,fill = provider),alpha = 0.3)

positive %>%
  summarise(num_Specialized = sum(provider_bin_Specialized),
         num_General = nrow(negative) - num_Specialized)

```

* There are about a quarter of hospitals whose operating costs are larger than their income from patients and other sources. The other 75% are operating in the positive in such that their operating costs are not larger than their income sources.
* Splitting up the hospitals that are operating in a deficit versus a surplus, I was interested to see if those operating in a deficit were the specialized hospitals. Specialized hospitals make up around 17.5% of hospitals in a deficit and around 46.8% of hospitals in a surplus. It should also be noted that overall, specialized hospitals only make up 923/5646 or around 16% of hosptials overall. 

# Train-test Split

```{r}
set.seed(1)

train_prop <- 0.9

n <- nrow(scaled_hpc_dummies)
n_train <- train_prop*n
n_test <- n - n_train

scaled_hpc_dummies$set <- "Train"
scaled_hpc_dummies$set[sample(n, n_test, replace = FALSE)] <- "Test"
  
df_train <- scaled_hpc_dummies |>  filter(set == "Train") |> select(-set)
df_test <- scaled_hpc_dummies |> filter(set == "Test") |> select(-set)
```

# Train set-fold assignment

```{r}
set.seed(1)

n_train <- nrow(df_train)

k <- 10
k.o <- k
kvals <- unique(round(n_train/(1L:floor(n_train/2))))
temp <- abs(kvals - k)
if (!any(temp == 0))  { k <- kvals[temp == min(temp)][1L] }
f <- ceiling(n_train/k)

df_train$fold <- sample(rep(1L:k, f), n_train)
```


# For all methods:

-   For each method you apply, use 10 fold cross-validation estimate for the test error.
-   discuss assumptions

# Quantitative Outcome Analyses

## Marginal simple linear regressions

```{r}
train = df_train %>%
  select(-c(costs_bin))

test = df_test %>%
  select(-c(costs_bin))

predictors <- select(train, -c(total_costs,fold))

# model summaries
# map(predictors, ~summary(lm(total_costs ~ .x, data = train)))

simple_models <- map(predictors, ~lm(total_costs ~ .x, data = train))
```

10-fold cv
```{r}
# Estimate with k-fold:

predictor_names <- colnames(predictors)
formulas <- str_c("total_costs", " ~ ", predictor_names)

compute_MSEP_lm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(formula, data = select(current_train_folds,-fold))
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs,fold) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

kfold_cv_lm <- function(train_data, formula, k = 10){
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

cv_marginal_lm <- map_dbl( formulas, ~kfold_cv_lm(train, formula = .x, k = 10) )
```

Test error 
```{r}
compute_testMSE_lm <- function(train_data,test_data, formula){
  
  marginaldf <- as.data.frame(matrix(nrow=1,ncol=2))
  colnames(marginaldf) <- c("Formula","Test_MSE")

  mod <- lm(formula, data = select(train_data,-fold))
  pred <- predict(mod, newdata = select( test_data, -c(total_costs) ) )
    
  testMSE <- mean( (test_data$total_costs - pred) ^ 2 )
  
  marginaldf$Formula = formula
  marginaldf$Test_MSE = testMSE
  
  return(marginaldf)
}

# compute_testMSE_lm(train,test,"total_costs ~ total_days")

testMSE_marginal_lm <- map_df(formulas, ~compute_testMSE_lm(train_data = train,test_data = test,formula = .x))

```

```{r}
method <- c("Marginal Linear Regression")

formula <- testMSE_marginal_lm$Formula

cv_error <- c(cv_marginal_lm)

test_error <- c(testMSE_marginal_lm$Test_MSE)

error_summary <- tibble(method,formula, cv_error, test_error)

knitr::kable(error_summary)
```

## Multiple linear regression

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r}
train = df_train %>%
  select(-c(costs_bin))

test = df_test %>%
  select(-c(costs_bin))

transformation_lm <- summary(lm(total_costs ~ fte_employees_on_payroll*total_income, data = df_train))

polynomial_lm <- summary(lm(total_costs ~ salaries + I(salaries^2),data = df_train))

# ggplot(df_train) + 
#   geom_point(aes(x=number_of_beds,y=total_costs))
```

10-fold cv
```{r}
# Estimate with k-fold:

compute_MSEP_multiple_lm <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(total_costs ~ ., data = select(current_train_folds,-fold))
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs,fold) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

cv_multiple_lm <- mean(map_dbl(1:10, ~compute_MSEP_multiple_lm(train,.x)))
```

Test error 
```{r}
compute_testMSE_multiple_lm <- function(train_data,test_data){
  
  mod <- lm(total_costs ~ ., data = select(train_data,-fold))
  pred <- predict(mod, newdata = select( test_data, -c(total_costs) ) )
    
  testMSE <- mean( (test_data$total_costs - pred) ^ 2 )

  return(testMSE)
}

testMSE_multiple_lm <- compute_testMSE_multiple_lm(train,test)
```


```{r}
# method <- c("Multiple Linear Regression")
# 
# formula <- c("total_costs ~ .")
# 
# cv_error <- c(cv_multiple_lm)
# 
# test_error <- c(testMSE_multiple_lm)
```

## Regression Tree (with pruning)

## Bagging (with variable importance)

## Random Forest (with variable importance)

## Boosting (including selecting the tuning parameter)

## Neural Network

# Variable Selection

## Best Subset

```{r,warning = FALSE}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin,-fold)

regfit_full <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "exhaustive")
summary(regfit_full)
```

k-fold CV

```{r}
compute_MSEP_lm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(formula, data = select(current_train_folds,-fold))
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs,fold) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

kfold_cv_lm <- function(train_data, formula, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
  
}

compute_testMSE_lm <- function(train_data,test_data,formula){
  
  mod <- lm(formula, data = select(train_data,-fold))
  pred <- predict(mod, newdata = select( test_data, -c(total_costs) ) )
    
  testMSE <- mean( (test_data$total_costs - pred) ^ 2 )

  return(testMSE)
}

# testMSE_multiple_lm <- compute_testMSE_lm(train,test,"total_costs ~ salaries")
```

```{r}
train <- df_train %>%
  select(-(costs_bin))

test <- df_test %>%
  select(-costs_bin)

best_subset_cv <- tibble("avgMSEP" = rep(NA,12),"testMSE" = rep(NA,12))

# k-fold CV for 1 predictor: salaries
formula = "total_costs ~ salaries"
best_subset_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(train, formula = .x, k = 10))
best_subset_cv$testMSE[1] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
best_subset_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[2] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
best_subset_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[3] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
best_subset_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[4] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
best_subset_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[5] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
best_subset_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[6] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
best_subset_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[7] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
best_subset_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[8] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
best_subset_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[9] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
best_subset_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[10] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
best_subset_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[11] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
best_subset_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
best_subset_cv$testMSE[12] <- compute_testMSE_lm(train,test,formula)

cv_best_subset = mean(best_subset_cv$avgMSEP)
testMSE_best_subset = mean(best_subset_cv$testMSE)
```


```{r}
method <- "Best Subset"

cv_error <- cv_best_subset

test_error <- testMSE_best_subset

```


## Forward Stepwise

```{r}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin)

test <- df_test %>%
  select(-costs_bin)

regfit_fwd <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "forward")
summary(regfit_fwd)
```

Forward stepwise CV

```{r}
forward_stepwise_cv <- tibble("avgMSEP" = rep(NA,12),"testMSE" = rep(NA,12))

# k-fold CV for 1 predictor: 
formula = "total_costs ~ salaries"
forward_stepwise_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(train, formula = .x, k = 10))
forward_stepwise_cv$testMSE[1] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
forward_stepwise_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[2] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
forward_stepwise_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[3] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
forward_stepwise_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[4] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
forward_stepwise_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[5] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
forward_stepwise_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[6] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
forward_stepwise_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[7] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
forward_stepwise_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[8] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
forward_stepwise_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[9] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
forward_stepwise_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[10] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
forward_stepwise_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[11] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
forward_stepwise_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(train, formula = .x, k = 10) )
forward_stepwise_cv$testMSE[12] <- compute_testMSE_lm(train,test,formula)

cv_forward_stepwise = mean(forward_stepwise_cv$avgMSEP)
testMSE_forward_stepwise = mean(forward_stepwise_cv$testMSE)
```

## Backward Stepwise

```{r}
library(leaps)

set.seed(1)

train <- df_train %>%
  select(-costs_bin)

test <- df_test %>%
  select(-costs_bin)

regfit_bwd <- regsubsets(total_costs ~ ., train,nvmax = 12,method = "backward")
summary(regfit_bwd)
```

Backward stepwise CV

```{r}

backward_stepwise_cv <- tibble("avgMSEP" = rep(NA,12),"testMSE" = rep(NA,12))

# k-fold CV for 1 predictor: 
formula = "total_costs ~ salaries"
backward_stepwise_cv$avgMSEP[1] <- map_dbl(formula, ~kfold_cv_lm(df_train, formula = .x, k = 10))
backward_stepwise_cv$testMSE[1] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 2 predictors: 
formula = "total_costs ~ salaries + inpatients"
backward_stepwise_cv$avgMSEP[2] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[2] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 3 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized"
backward_stepwise_cv$avgMSEP[3] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[3] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 4 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll"
backward_stepwise_cv$avgMSEP[4] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[4] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 5 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets"
backward_stepwise_cv$avgMSEP[5] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[5] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 6 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary"
backward_stepwise_cv$avgMSEP[6] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[6] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 7 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges"
backward_stepwise_cv$avgMSEP[7] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[7] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 8 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges + total_income"
backward_stepwise_cv$avgMSEP[8] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[8] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 9 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds"
backward_stepwise_cv$avgMSEP[9] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[9] <- compute_testMSE_lm(train,test,formula)

# k-fold CV for 10 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental"
backward_stepwise_cv$avgMSEP[10] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[10] <- compute_testMSE_lm(train,test,formula)


# k-fold CV for 11 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural"
backward_stepwise_cv$avgMSEP[11] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[11] <- compute_testMSE_lm(train,test,formula)


# k-fold CV for 12 predictors: 
formula = "total_costs ~ salaries + inpatients + provider_bin_Specialized + fte_employees_on_payroll + total_assets + control_bin_Proprietary + total_discharges +
total_income + number_of_beds + control_bin_Governmental + rural + total_days"
backward_stepwise_cv$avgMSEP[12] <- map_dbl( formula, ~kfold_cv_lm(df_train, formula = .x, k = 10) )
backward_stepwise_cv$testMSE[12] <- compute_testMSE_lm(train,test,formula)

cv_backward_stepwise = mean(backward_stepwise_cv$avgMSEP)
testMSE_backward_stepwise = mean(backward_stepwise_cv$testMSE)
```

(For the above 3 methods, determine which model of a given size is best by comparing the 10-fold cross validation estimate of the test error.)

## Ridge regression (find the best tuning parameter using cross-validation)

```{r}
library(glmnet)

set.seed(1)

train <- df_train %>%
  select(-c(costs_bin,fold))

test <- df_test %>%
  select(-c(costs_bin))

train_design_mat = model.matrix(total_costs ~ ., data = train)[, -1]
test_design_mat = model.matrix(total_costs ~ .,data = test)[,-1]

trainx <- train_design_mat
trainy <- train$total_costs

testx <- test_design_mat
testy <- test$total_costs

# Ridge model 
lambda_grid <- 10 ^ seq(10, -2, length = 100)
ridge_reg <- glmnet(trainx, trainy, alpha = 0, lambda = lambda_grid) 

# Perform 10-fold cross validation 
cv.out <- cv.glmnet(trainx, trainy, alpha = 0)
bestmean <- min(cv.out$cvm) # lowest error 
bestlambda <- cv.out$lambda[which(cv.out$cvm == bestmean)]
cv_ridge = mean(cv.out$cvm)

# Use optimal lambda to find test error 
optimal_lambda_ridge_reg <- glmnet(x,y,alpha = 0,lambda = bestlambda)
pred = predict(optimal_lambda_ridge_reg, newx = testx)
testMSE_ridge = mean( (pred-testy)^2 )
```

## Lasso (find the best tuning parameter using cross-validation)

```{r}
library(glmnet)

set.seed(1)

train <- df_train %>%
  select(-c(costs_bin,fold))

test <- df_test %>%
  select(-c(costs_bin))

train_design_mat = model.matrix(total_costs ~ ., data = train)[, -1]
test_design_mat = model.matrix(total_costs ~ .,data = test)[,-1]

trainx <- train_design_mat
trainy <- train$total_costs

testx <- test_design_mat
testy <- test$total_costs

# Lasso model 
lambda_grid <- 10 ^ seq(10, -2, length = 100)
lasso_reg <- glmnet(trainx, trainy, alpha = 1, lambda = lambda_grid) 

# Perform 10-fold cross validation 
cv.out <- cv.glmnet(trainx, trainy, alpha = 1)
bestmean <- min(cv.out$cvm) # lowest error 
bestlambda <- cv.out$lambda[which(cv.out$cvm == bestmean)]
cv_lasso = mean(cv.out$cvm)

# Use optimal lambda to find test error 
optimal_lambda_lasso_reg <- glmnet(x,y,alpha = 0,lambda = bestlambda)
pred = predict(optimal_lambda_lasso_reg, newx = testx)
testMSE_lasso = mean( (pred-testy)^2 )

```

## Principal Components Regression (PCR)

```{r}
library(pls)

set.seed(1)
train <- df_train %>%
  select(-c(costs_bin,fold))

test <- df_test %>%
  select(-c(costs_bin))

pcr.fit <-pcr(total_costs ~ ., data = train, scale = FALSE, validation = "CV",segments = 10)
MSEP = MSEP(pcr.fit)
(which.min(MSEP$val[1,1, ] ))
cv_pcr = MSEP$val[1,1,]
cv_pcr = mean(cv_pcr[-1])

validationplot(pcr.fit, val.type = "MSEP")

pcr.pred <-predict(pcr.fit, newx = select(test,-total_costs), ncomp = 12)
testMSE_pcr = mean((pcr.pred- test$total_costs)^2)
```

# Quantitative summary
```{r}

method <- c("Marginal Linear Regression")

formula <- testMSE_marginal_lm$Formula

cv_error <- c(cv_marginal_lm)

test_error <- c(testMSE_marginal_lm$Test_MSE)

marginal_summary <- tibble(method,formula, cv_error, test_error)


method <- c("Multiple Linear Regression","Best Subset","Forward Stepwise",
            "Backward Stepwise","Ridge Regression","Lasso","PCR")

formula <- rep(NA,7)

cv_error <- c(cv_multiple_lm,cv_best_subset,cv_forward_stepwise,
              cv_backward_stepwise,cv_ridge,cv_lasso,cv_pcr)

test_error <- c(testMSE_multiple_lm,testMSE_best_subset,testMSE_forward_stepwise,
                testMSE_backward_stepwise,testMSE_ridge,testMSE_lasso,
                testMSE_pcr)

rest_of_methods <- tibble(method,formula,cv_error,test_error)

error_summary <- rbind(marginal_summary,rest_of_methods)

knitr::kable(error_summary)
```


# Qualitative Outcome Analyses

## KNN

```{r}
library(class)

set.seed(1)

train <- df_train %>%
  select(-c(total_costs))

compute_error_knn <- function(train_data, i, neighbors){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)

  knn_pred <- knn(
      train = select(current_train_folds, -c(costs_bin,fold)),
      test = select(current_test_fold, -c(costs_bin,fold)),
      cl = current_train_folds$costs_bin,
      k = neighbors,
      prob = TRUE
    )
  
  error = mean(current_test_fold$costs_bin != knn_pred)
  return (error)
}

# for fold number = 1,2,3,4,5,6,7,8,9,10 and then avg all the errors together
kfold_cv_knn <- function(train_data,neighbors, k = 10) {
  error_vec <- map_dbl(1:k, ~compute_error_knn(train, .x,neighbors))
  avg_error <- mean(error_vec)
  return(avg_error)
}

neighbors <- 1:100 
knn_error_rates = map_dbl(neighbors, ~kfold_cv_knn(train,neighbors = .x))

optimal_n <- which.min(knn_error_rates)
optimal_error_rate <- knn_error_rates[optimal_n]

optimal_n
optimal_error_rate

cv_knn = mean(knn_error_rates)
```

### Compare true test error rate

```{r}
set.seed(1)

train <- df_train %>%
  select(-c(total_costs,fold))

test <- df_test %>%
  select(-c(total_costs))

model_info = tibble(.rows=1)

knn_pred <- knn(
      train = select(train,-costs_bin),
      test = select(test,-costs_bin),
      cl = train$costs_bin,
      k = optimal_n,
      prob = TRUE
    )
  
(contingency = table(knn_pred,test$costs_bin))

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error = mean(test$costs_bin != knn_pred)

model_info

testMSE_knn = model_info$error
truepos_knn = model_info$truepos
trueneg_knn = model_info$trueneg
falsepos_knn = model_info$falsepos
falseneg_knn = model_info$falseneg
```

## Simple/Multiple logistic regression

```{r}
set.seed(1)

predictors <- select(df_train, -c(total_costs, costs_bin,fold))

train = df_train %>%
  select(-c(total_costs))

test = df_test %>%
  select(-c(total_costs))

# model summaries
# map(predictors, ~summary(glm(costs_bin ~ .x, data = select(train,-fold),family=binomial)))

# simple_logistic_models <- map(predictors, ~glm(costs_bin ~ .x, data = select(train,-fold),family=binomial))
```

10-fold cv

```{r}
# Estimate with k-fold:
compute_MSEP_glm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- glm(formula, data = select(current_train_folds,-fold),family = binomial)
  pred <- round(predict(mod, newdata = select( current_test_fold, -c(fold,costs_bin) ),type = "response" ))
    
  error_rate <- mean( (current_test_fold$costs_bin != pred) )
  return(error_rate)
}

formula = c("costs_bin ~ .")
error_rates <- map_dbl(1:10, ~compute_MSEP_glm(train, formula, .x))
cv_glm = mean(error_rates)

# test_error_vec <- map_dbl( formulas, ~compute_MSEP_glm(train, formula = .x, i = 10) )
```

Test error 
```{r}
mod <- glm("costs_bin ~ .", data = select(train,-fold),family = binomial)
pred <- round(predict(mod, newdata = select( test, -c(costs_bin) ),type = "response" ))
  
model_info = tibble(.rows=1)
contingency = table(pred,test$costs_bin)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error_rate <- mean( (test$costs_bin != pred) )

testMSE_glm = model_info$error_rate
truepos_glm = model_info$truepos
trueneg_glm = model_info$trueneg
falsepos_glm = model_info$falsepos
falseneg_glm = model_info$falseneg
```

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

## LDA

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

test = df_test %>%
  select(-total_costs)

compute_error_lda <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  lda_model <- MASS::lda(costs_bin ~ ., data = select(current_train_folds,-fold))
  lda_pred <- predict(lda_model, newdata = select(current_test_fold, -c(fold,costs_bin)))$class
  
  contingency = table(lda_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != lda_pred)
  return (model_info)
}

kfold_cv_lda = map_df(1:10, ~compute_error_lda(train,.x))
cv_lda = (mean(kfold_cv_lda$error))

# table(c(1,2),c(3,4))
# table(current_test_fold$costs_bin,lda_pred)
```

Test error 
```{r}
lda_model <- MASS::lda(costs_bin ~ ., data = select(train,-fold))
lda_pred <- predict(lda_model, newdata = select(test, -c(costs_bin)))$class
  
model_info = tibble(.rows=1)
contingency = table(pred,test$costs_bin)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error_rate <- mean( (test$costs_bin != lda_pred) )

testMSE_lda = model_info$error_rate
truepos_lda = model_info$truepos
trueneg_lda = model_info$trueneg
falsepos_lda = model_info$falsepos
falseneg_lda = model_info$falseneg
```


## QDA

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

test = df_test %>%
  select(-total_costs)

compute_error_qda <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  qda_model <- MASS::qda(costs_bin ~ ., data = select(current_train_folds,-fold))
  qda_pred <- predict(qda_model, newdata = select(current_test_fold, -c(fold,costs_bin)))$class
  
  contingency = table(qda_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != qda_pred)
  return (model_info)
}

kfold_cv_qda = map_df(1:10, ~compute_error_qda(train,.x))
cv_qda = (mean(kfold_cv_qda$error))

# table(c(1,2),c(3,4))
# table(current_test_fold$costs_bin,lda_pred)
```

Test error 
```{r}

qda_model <- MASS::qda(costs_bin ~ ., data = select(train,-fold))
qda_pred <- predict(qda_model, newdata = select(test, -c(costs_bin)))$class

model_info = tibble(.rows=1)
contingency = table(pred,test$costs_bin)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error_rate <- mean( (test$costs_bin != lda_pred) )

testMSE_qda = model_info$error_rate
truepos_qda = model_info$truepos
trueneg_qda = model_info$trueneg
falsepos_qda = model_info$falsepos
falseneg_qda = model_info$falseneg
```

## Naive Bayes (at least two kernels)

```{r}
set.seed(1)

train = df_train %>%
  select(-total_costs)

test = df_test %>%
  select(-total_costs)

compute_error_nb <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  model_info = tibble(.rows=1)
  
  nb_model <- e1071::naiveBayes(costs_bin ~ ., data = select(current_train_folds,-fold))
  nb_pred <- predict(nb_model, newdata = select(current_test_fold, -c(fold,costs_bin)))
  
  contingency = table(nb_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos

  #Positive = above median 
  #Negative = below median 
  
  #sensitivity = true positive = prob that hospital correctly identified above median 188/258
  #specificity = true negative = prob that hospital correctly identified below median 236/251
  
  #false positive = prob that hospital get identified above median when it's below median 15/251
  #false negative = prob that hospital get identified below median when it's above median 70/258
  
  model_info$error = mean(current_test_fold$costs_bin != nb_pred)
  return (model_info)
}

kfold_cv_nb = map_df(1:10, ~compute_error_nb(train,.x))
cv_nb = (mean(kfold_cv_nb$error))
```

Test error 
```{r}
nb_model <- e1071::naiveBayes(costs_bin ~ ., data = select(train,-fold))
nb_pred <- predict(nb_model, newdata = select(test, -c(costs_bin)))

model_info = tibble(.rows=1)
contingency = table(pred,test$costs_bin)

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos

model_info$error_rate <- mean( (test$costs_bin != lda_pred) )

testMSE_nb = model_info$error_rate
truepos_nb = model_info$truepos
trueneg_nb = model_info$trueneg
falsepos_nb = model_info$falsepos
falseneg_nb = model_info$falseneg
```

2nd kernel

```{r}
set.seed(1)


train = df_train %>% 
  select(-total_costs)

test = df_test %>% 
  select(-total_costs)

compute_error_nb_kde <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
  
  current_train_matrix = current_train_folds %>%
    select(-c(fold,costs_bin)) %>%
    as.matrix()
  
  current_train_matrixy = current_train_folds %>%
    select(costs_bin) %>%
    as.matrix()
  
  current_test_matrix = current_test_fold %>%
    select(-c(fold,costs_bin)) %>%
    as.matrix()
  
  model_info = tibble(.rows=1)
  
  nb_kde_model <-  naivebayes::nonparametric_naive_bayes(y = as.factor(current_train_matrixy),
                                       x = current_train_matrix
    )
  nb_kde_pred <- predict(nb_kde_model, newdata = current_test_matrix )

  # nb_kde_model <- e1071::naiveBayes(costs_bin ~ ., data = select(current_train_folds,-fold))
  # nb_kde_pred <- predict(nb_kde_model, newdata = select(current_test_fold, -c(fold,costs_bin)))
  
  contingency = table(nb_kde_pred,current_test_fold$costs_bin)
  #           Actual:           
  #           below   above
  # Predicted:
  # below 
  # above
  
  model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
  model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
  model_info$falsepos = 1 - model_info$trueneg
  model_info$falseneg = 1 - model_info$truepos
  model_info$error = mean(current_test_fold$costs_bin != nb_kde_pred)
  return (model_info)
}

kfold_cv_nb_kde = map_df(1:10, ~compute_error_nb_kde(train,.x))
cv_nb_kde = (mean(kfold_cv_nb_kde$error))
```

Test error 
```{r}
current_train_matrix = train %>%
  select(-c(fold,costs_bin)) %>%
  as.matrix()

current_train_matrixy = train %>%
  select(costs_bin) %>%
  as.matrix()

current_test_matrix = test %>%
  select(-c(costs_bin)) %>%
  as.matrix()

model_info = tibble(.rows=1)

nb_kde_model <-  naivebayes::nonparametric_naive_bayes(y = as.factor(current_train_matrixy),
                                     x = current_train_matrix
  )
nb_kde_pred <- predict(nb_kde_model, newdata = current_test_matrix )

# nb_kde_model <- e1071::naiveBayes(costs_bin ~ ., data = select(current_train_folds,-fold))
# nb_kde_pred <- predict(nb_kde_model, newdata = select(current_test_fold, -c(fold,costs_bin)))

contingency = table(nb_kde_pred,test$costs_bin)
#           Actual:           
#           below   above
# Predicted:
# below 
# above

model_info$truepos = contingency[2,2] / (contingency[1,2] + contingency[2,2])
model_info$trueneg = contingency[1,1] / (contingency[2,1] + contingency[1,1])
model_info$falsepos = 1 - model_info$trueneg
model_info$falseneg = 1 - model_info$truepos
model_info$error = mean(test$costs_bin != nb_kde_pred)

testMSE_nb_kde = model_info$error
truepos_nb_kde = model_info$truepos
trueneg_nb_kde = model_info$trueneg
falsepos_nb_kde = model_info$falsepos
falseneg_nb_kde = model_info$falseneg
```

# Qualitative summary
```{r}

method <- c("KNN","Multiple Logistic Regression","LDA","QDA","Naive Bayes (Gaussian)",
            "Naive Bayes (KDE)")

cv_error <- c(cv_knn,cv_glm,cv_lda,cv_qda,cv_nb,cv_nb_kde)

error_rates <- c(testMSE_knn,testMSE_glm,testMSE_lda,testMSE_qda,testMSE_nb,
                testMSE_nb_kde)

true_pos <- c(truepos_knn,truepos_glm,truepos_lda,truepos_qda,truepos_nb,
              truepos_nb_kde)

true_neg <- c(trueneg_knn,trueneg_glm,trueneg_lda,trueneg_qda,trueneg_nb,
              trueneg_nb_kde)

false_pos <- c(falsepos_knn,falsepos_glm,falsepos_lda,falsepos_qda,falsepos_nb,
              falsepos_nb_kde)

false_neg <- c(falseneg_knn,falseneg_glm,falseneg_lda,falseneg_qda,falseneg_nb,
              falseneg_nb_kde)

error_summary <- tibble(method,cv_error,error_rates,true_pos,true_neg,
                        false_pos,false_neg)

knitr::kable(error_summary)
```

## Decision Tree (with pruning)

## Bagging (with variable importance)

## Random Forest (with variable importance)

## Boosting (including selecting the tuning parameter)

(For the above three methods, calculate the Gini index on each leaf of the final tree to examine the purity of the node.)

## Neural Network

## Calculate True/False Positive/Negative rates for each method

# Bootstrap CI

# Simulation Study
