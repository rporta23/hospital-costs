---
title: "Rose Analysis"
author: "Rose Porta"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(class)
library(glmnet)
library(naivebayes)
library(leaps)
library(pls)
```

To-dos 4/22/24:

- pick an interaction and transformation

- implement 10-fold cv for each method

- 10-fold cv for choosing best subset model

- repeat best subsets for categorical outcome

- compile results into a cohesive table: method - cv estimated error - true test error (one table for quant and one for binary)

- write down assumptions for each method

- idea for simulation study

- idea for bootstrap CI

Questions:

- how to interpret best subset for binary response?

- perform best subset first and proceed with only selected predictors?

# Data Cleaning

```{r}
hpc <- read_csv(here::here("hpc.csv"))
#summary(hpc)

hpcdf <- hpc |> 
  mutate(start = as.Date(`Fiscal Year Begin Date`),end = as.Date(`Fiscal Year End Date`)) |> 
  mutate(days = as.numeric(end - start)) |> 
  mutate(numBeds = `Total Bed Days Available`/days,id = row_number())
```

```{r}
hpc_clean <- hpcdf |> 
  janitor::clean_names() |> 
  select(provider_ccn, days, number_of_beds,
         total_costs, rural_versus_urban, provider_type, type_of_control,
         fte_employees_on_payroll, total_days = total_days_v_xviii_xix_unknown,
         total_discharges = total_discharges_v_xviii_xix_unknown,
         total_income, total_assets,
         salaries = total_salaries_from_worksheet_a) |> 
  na.omit() |> 
  mutate(rural = ifelse(rural_versus_urban == "R", 1, 0),
         control_bin = case_when(
           type_of_control < 3 ~ "Voluntary",
           (type_of_control >= 3 & type_of_control < 7) ~ "Proprietary",
           type_of_control >= 7 ~ "Governmental"
         ),
         provider_bin = ifelse(provider_type < 3 |
                                 provider_type == 6,
                               "General", "Specialized"
                               ),
         ) |> 
  select(- c(rural_versus_urban, type_of_control, provider_type))

duplicates <- hpc_clean |> 
  group_by(provider_ccn) |> 
  summarise(count = n()) |> 
  filter(count > 1)

dup <- map_lgl(hpc_clean$provider_ccn, `%in%`, duplicates$provider_ccn)

hpc1 <- hpc_clean |> 
  filter(!dup)

hpc2 <- hpc_clean |> 
  filter(dup) |> 
  group_by(provider_ccn) |> 
  summarise(
    days = sum(days),
    number_of_beds = mean(number_of_beds),
    total_costs = sum(total_costs),
    fte_employees_on_payroll = mean(fte_employees_on_payroll),
    total_days = sum(total_days),
    total_discharges = sum(total_discharges),
    total_income = sum(total_income),
    total_assets = mean(total_assets),
    salaries = sum(salaries),
    rural = max(rural),
    control_bin = max(control_bin),
    provider_bin = max(provider_bin)
  )

hpc_all <- bind_rows(hpc1, hpc2)

hpc_normalize <- hpc_all |> 
  mutate(
    total_costs = total_costs/days,
    inpatients = total_days/days,
    total_discharges = total_discharges/days,
    total_income = total_income/days,
    salaries = salaries/days
    )
```

```{r}
hpc_dummies <- hpc_normalize |> 
  select(-c(provider_ccn, days)) |> 
  mutate(
    costs_bin = ifelse(total_costs > median(total_costs), 1, 0)
    ) |> 
  tidytable::get_dummies(drop_first = TRUE) |> 
  select(where(is.numeric))
```

# Train-test Split

```{r}
set.seed(1)

train_prop <- 0.9

n <- nrow(hpc_dummies)
n_train <- train_prop*n
n_test <- n - n_train

hpc_dummies$set <- "Train"
hpc_dummies$set[sample(n, n_test, replace = FALSE)] <- "Test"
  
df_train <- hpc_dummies |>  filter(set == "Train") |> select(-set)
df_test <- hpc_dummies |> filter(set == "Test") |> select(-set)
```

# For all methods:

-   For each method you apply, use 10 fold cross-validation estimate for the test error.
-   discuss assumptions

# Quantitative Outcome Analyses

Set Up:

```{r}
predictors <- select(df_train, -c(total_costs, costs_bin))
```

Assign Folds for 10-fold CV

```{r}
n_train <- nrow(df_train)

k <- 10
k.o <- k
kvals <- unique(round(n_train/(1L:floor(n_train/2))))
temp <- abs(kvals - k)
if (!any(temp == 0))  { k <- kvals[temp == min(temp)][1L] }
f <- ceiling(n_train/k)

df_train$fold <- sample(rep(1L:k, f), n_train)
```

Remove categorical outcome and fold number from train and test data:

```{r}
train_data <- df_train |> select(-c(costs_bin))

test_data <- df_test |> select(-c(costs_bin))
```


## Marginal simple linear regressions

```{r}
# model summaries
map(predictors, ~summary(lm(total_costs ~ .x, data = df_train)))

simple_models <- map(predictors, ~lm(total_costs ~ .x, data = df_train))
```

10-fold cv

```{r}
# Estimate with k-fold:

predictor_names <- colnames(predictors)

formulas <- str_c("total_costs", " ~ ", predictor_names)


compute_MSEP_lm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
    
  mod <- lm(formula, data = current_train_folds)
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}


kfold_cv_lm <- function(train_data, formula, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
  
}

test_error_vec <- map_dbl( formulas, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

test_error_vec
```

True Test Error

```{r}
compute_MSEP_test <- function(formula, train_data, test_data){
  
  train_data <- train_data |> select(-fold)
  
  mod <- lm(formula, data = train_data)
  pred <- predict(mod, newdata = select( test_data, -c(total_costs) ) )
    
  MSEP <- mean( (test_data$total_costs - pred) ^ 2 )
  return(MSEP)
}

test_MSEP_vec <- map_dbl(formulas, ~compute_MSEP_test(.x, df_train, df_test))
test_MSEP_vec
```


## Multiple linear regression

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r}

formula <- as.formula("total_costs ~ .")

mod_mlr <- lm(formula, data = select(train_data, -fold) )

summary(mod_mlr)
```

```{r}

cv_error_lm <- kfold_cv_lm(train_data, formula)

cv_error_lm
```

```{r}
test_error_lm <- compute_MSEP_test(formula, train_data, df_test)

test_error_lm
```

## Multiple linear regression with interactions and transformations: ** NA coefficients

```{r}
formula <- 
as.formula("total_costs ~ . + poly(total_days, 2, raw = TRUE) + poly(salaries, 2, raw = TRUE) + fte_employees_on_payroll*total_income + fte_employees_on_payroll*provider_bin_Specialized")

mod_mlr <- lm(total_costs ~ . + poly(total_days, 2) + poly(salaries, 2) + fte_employees_on_payroll*total_income + fte_employees_on_payroll*provider_bin_Specialized, data = select(train_data, -fold) )

summary(mod_mlr)

```



## Regression Tree (with pruning)

## Bagging (with variable importance)

## Random Forest (with variable importance)

## Boosting (including selecting the tuning parameter)

## Neural Network

# Variable Selection

## Best Subset

```{r}
nvmax <- 12
```


```{r}
regfit_best <- regsubsets(total_costs ~ ., data = select(train_data, -fold),
                          nvmax = nvmax)
summary_regfit <- summary(regfit_best)
summary_regfit
```

10-fold CV to choose best model:

```{r}
response <- "total_costs"
m <- summary_regfit$which[,-1]
predictor_names <- colnames(predictors)
#colnames(m) <- predictor_names
rhs <- map_chr(1:12, ~str_c(predictor_names[m[.x, ]], collapse = " + "))
formulas <- str_c(response, " ~ ", rhs)

cv_error_vec <- map_dbl( formulas, ~kfold_cv_lm(train_data, formula = .x, k = 10) )
cv_error_vec

optimal_size <- which(cv_error_vec == min(cv_error_vec))
optimal_size

cv_error_bsub <- cv_error_vec[optimal_size]
cv_error_bsub
```

Optimal Model Size: 6

Summary of Best Model:

```{r}
best_formula <- formulas[optimal_size]
mod_best <- lm(best_formula, train_data)
summary(mod_best)
```


Test Error:

```{r}
test_error_bsub <- compute_MSEP_test(best_formula, train_data, test_data)

test_error_bsub
```


## Forward Stepwise

```{r}
# Forward stepwise selection -----
regfit_fwd <- regsubsets(total_costs ~ ., data = select(train_data, -fold),
                         nvmax = nvmax, method = "forward")
summary_regfit <- summary(regfit_fwd)
summary_regfit
```

10-fold CV to choose best model:

```{r}
response <- "total_costs"
m <- summary_regfit$which[,-1]
predictor_names <- colnames(predictors)
#colnames(m) <- predictor_names
rhs <- map_chr(1:12, ~str_c(predictor_names[m[.x, ]], collapse = " + "))
formulas <- str_c(response, " ~ ", rhs)

cv_error_vec <- map_dbl( formulas, ~kfold_cv_lm(train_data, formula = .x, k = 10) )
cv_error_vec

optimal_size <- which(cv_error_vec == min(cv_error_vec))
optimal_size

cv_error_fwd <- cv_error_vec[optimal_size]
cv_error_fwd
```

Optimal Model Size: 6

Summary of Best Model:

```{r}
best_formula <- formulas[optimal_size]
mod_best <- lm(best_formula, train_data)
summary(mod_best)
```

Test Error:

```{r}
test_error_fwd <- compute_MSEP_test(best_formula, train_data, test_data)

test_error_fwd
```


## Backward Stepwise

```{r}
# Backward stepwise selection -----
regfit_bwd <- regsubsets(total_costs ~ ., 
                         data = select(train_data, -fold), 
                         nvmax = nvmax, 
                         method = "backward"
                         )
summary_regfit <- summary(regfit_bwd)
summary_regfit
```

10-fold CV to choose best model:

```{r}
response <- "total_costs"
m <- summary_regfit$which[,-1]
predictor_names <- colnames(predictors)
#colnames(m) <- predictor_names
rhs <- map_chr(1:12, ~str_c(predictor_names[m[.x, ]], collapse = " + "))
formulas <- str_c(response, " ~ ", rhs)

cv_error_vec <- map_dbl( formulas, ~kfold_cv_lm(train_data, formula = .x, k = 10) )
cv_error_vec

optimal_size <- which(cv_error_vec == min(cv_error_vec))
optimal_size

cv_error_bwd <- cv_error_vec[optimal_size]
cv_error_bwd
```

Optimal Model Size: 6

Summary of Best Model:

```{r}
best_formula <- formulas[optimal_size]
mod_best <- lm(best_formula, train_data)
summary(mod_best)
```

Test Error:

```{r}
test_error_bwd <- compute_MSEP_test(best_formula, train_data, test_data)

test_error_bwd
```

(For the above 3 methods, determine which model of a given size is best by comparing the 10-fold cross validation estimate of the test error.) 

## Ridge regression (find the best tuning parameter using cross-validation)

```{r}
# scale predictors

quant_predictors_train <- train_data |> 
  select(-c(control_bin_Governmental, control_bin_Proprietary, 
            provider_bin_Specialized, fold)) |> 
  scale()

qual_predictors_train <- train_data |> 
  select(control_bin_Governmental, control_bin_Proprietary,
         provider_bin_Specialized, fold) 

hpc_scaled_train <- cbind(quant_predictors_train, qual_predictors_train) 

quant_predictors_test <- test_data |> 
  select(-c(control_bin_Governmental, control_bin_Proprietary, provider_bin_Specialized)) |> 
  scale()

qual_predictors_test <- test_data |> 
  select(control_bin_Governmental, control_bin_Proprietary, provider_bin_Specialized) 

hpc_scaled_test <- cbind(quant_predictors_test, qual_predictors_test)

x_train <- model.matrix(total_costs ~ . - fold, data = hpc_scaled_train)[ , -1]
y_train <- hpc_scaled_train$total_costs

x_test <- model.matrix(total_costs ~ ., data = hpc_scaled_test)[ , -1]
y_test <- hpc_scaled_test$total_costs
```


```{r}
set.seed(1)

lambda_grid <- 10 ^ seq(10, -2, length = 100)
cv_ridge <- 
  cv.glmnet(
    x_train,
    y_train,
    alpha = 0,
    lambda = lambda_grid,
    type.measure = "mse",
    nfolds = 10        # Set the number of folds to n to get LOOCV
    )

cv_ridge$lambda.min

lambda_min <- cv_ridge$lambda.min
```

```{r}
ridge_reg <- glmnet(x_train, y_train, alpha = 0, lambda = lambda_min)
coef(ridge_reg)
```

```{r}
pred_ridge <- predict(ridge_reg, newx = x_test)

test_error_ridge <- mean( (hpc_scaled_test$total_costs - pred_ridge) ^ 2 )

test_error_ridge
```

10-fold CV:

```{r}
compute_MSEP_ridge <- function(train_data, lambda, alpha, i){
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
  
  x_train <- model.matrix(total_costs ~ ., data = current_train_folds)[ , -1]
  y_train <- current_train_folds$total_costs

  x_test <- model.matrix(total_costs ~ ., data = current_test_fold)[ , -1]
  y_test <- current_test_fold$total_costs
    
  ridge_reg <- glmnet(x_train, y_train, alpha = alpha, lambda = lambda)
  pred_ridge <- predict(ridge_reg, newx = x_test)
    
  MSEP <- mean( (current_test_fold$total_costs - pred_ridge) ^ 2 )
  return(MSEP)
}

#compute_MSEP_lm(train_data, formula, i)

kfold_cv_ridge <- function(train_data, lambda, alpha, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_ridge(train_data, lambda, alpha, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
  
}

cv_error_ridge <- kfold_cv_ridge(hpc_scaled_train, lambda_min, alpha = 0)
cv_error_ridge
```


## Lasso (find the best tuning parameter using cross-validation)

```{r}

set.seed(1)

# fit lasso model
lambda_grid <- 10 ^ seq(10, -2, length = 100)

# 10-fold cross validation to choose tuning parameter
cv_lasso <- 
  cv.glmnet(
    x_train,
    y_train,
    alpha = 1,
    lambda = lambda_grid,
    type.measure = "mse",
    nfolds = 10        # Set the number of folds to n to get LOOCV
    )

lambda_min <- cv_lasso$lambda.min
lambda_min
```


```{r}
# fit lasso model with optimal tuning parameter
lasso_bestlambda <- glmnet(x_train, y_train, alpha = 1, lambda = lambda_min)
coef(lasso_bestlambda)
```

```{r}
pred_lasso <- predict(lasso_bestlambda, newx = x_test)

test_error_lasso <- mean( (hpc_scaled_test$total_costs - pred_lasso) ^ 2 )

test_error_lasso
```

10-fold CV:

```{r}
cv_error_lasso <- kfold_cv_ridge(hpc_scaled_train, lambda_min, alpha = 1)
cv_error_lasso
```


## Principal Components Regression (PCR)


```{r}
pcr_fit <- pcr(total_costs ~ ., 
               data = select(hpc_scaled_train, -fold),
               validation = "CV", 
               segments = 10
               )
summary(pcr_fit)
```

```{r}
validationplot(pcr_fit, val.type = "MSEP")
```

```{r}
pred_test <- predict(pcr_fit, hpc_scaled_test, ncomp = 5)

test_error_pcr = mean( (hpc_scaled_test$total_costs - pred_test)^2 )

test_error_pcr
```

10-fold CV:

```{r}
compute_MSEP_pcr <- function(train_data, i){
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
  
  pcr_fit <- pcr(total_costs ~ ., 
               data = current_train_folds,
               validation = "CV", 
               segments = 10
               )
  
  pred <- predict(pcr_fit, current_test_fold, ncomp = 5)
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

#compute_MSEP_lm(train_data, formula, i)

kfold_cv_pcr <- function(train_data, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_pcr(train_data, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
  
}

cv_error_pcr <- kfold_cv_pcr(hpc_scaled_train)
cv_error_pcr
```


# Summary Table Quant Outcome

```{r}
method <- c("Linear Regression (Main Effects)",
            "Best Subset", "Forward Stepwise", "Backward Stepwise", "Ridge", "Lasso", "PCR")

cv_error <- c(cv_error_lm, cv_error_bsub, cv_error_fwd, 
              cv_error_bwd, cv_error_ridge, cv_error_lasso, cv_error_pcr)

test_error <- c(test_error_lm, test_error_bsub, test_error_fwd, 
              test_error_bwd, test_error_ridge, test_error_lasso, test_error_pcr)

error_summary <- tibble(method, cv_error, test_error)

knitr::kable(error_summary)
```


# Qualitative Outcome Analyses

```{r}
train_data <- df_train |> select(-c(total_costs))

test_data <- df_test |> select(-c(total_costs))
```


## KNN

Choose optimal number of nearest neighbors, c, using CV: 

```{r}
compute_error_knn <- function(train_data, i, c){
  
  set.seed(i)
  
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
  
  knn_pred <- 
    knn(
      train = select(current_train_folds, -costs_bin),
      test = select(current_test_fold, -c(costs_bin)),
      cl = current_train_folds$costs_bin,
      k = c,
      prob = TRUE
    )
    
  error <- mean( (current_test_fold$costs_bin != knn_pred)  )
  return(error)
}




kfold_cv_knn <- function(train_data, k = 10, c){
  
  error_vec <- map_dbl(1:k, ~compute_error_knn(train_data, .x, c))
  avg_error <- mean(error_vec)
  return(avg_error)
  
}

c_values <- 1:100

cv_error_rates <- map_dbl(c_values, ~kfold_cv_knn(train_data, k = 10, c = .x))

optimal_c <- which(cv_error_rates == min(cv_error_rates))

optimal_c

cv_error_rate_optimal_c <- min(cv_error_rates)
cv_error_rate_optimal_c

cv_error_knn <- cv_error_rate_optimal_c
```

Fit model with optimal number of nearest neighbors, c = 17:

```{r}
set.seed(1)

knn_pred <- 
    knn(
      train = select(df_train, -c(total_costs, costs_bin, fold)),
      test = select(df_test, -c(total_costs, costs_bin)),
      cl = df_train$costs_bin,
      k = optimal_c,
      prob = TRUE
    )

test_error_knn <- mean(df_test$costs_bin != knn_pred)

test_error_knn

```


## Multiple logistic regression ** issue with cv

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r}

formula <- as.formula("costs_bin ~ .")

mod_mlogr <- glm(formula, 
                 data = train_data |> select(-fold), 
                 family = binomial(link = "logit")
                 )

summary(mod_mlogr)
```

```{r}
log_pred <- predict( mod_mlogr, 
                     newdata = select( test_data, -c(costs_bin) ), 
                     type = "response" 
                     )

pred_class <- ifelse(log_pred >= 0.5, 1, 0)

test_error_logistic <- mean(df_test$costs_bin != pred_class)

test_error_logistic
```

```{r}
compute_error_logistic <- function(train_data, formula, i){
  
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
  
  mod_mlogr <- glm(formula, 
                 data = current_train_folds, 
                 family = binomial(link = "logit")
                 )
  
  log_pred <- predict(mod_mlogr, 
                     newdata = current_test_fold |> select(-costs_bin) , 
                     type = "response" 
                     )

  pred_class <- ifelse(log_pred >= 0.5, 1, 0)
    
  error <- mean( (current_test_fold$costs_bin != pred_class)  )
  return(error)
}

compute_error_logistic(train_data, formula, 1)


#compute_MSEP_lm(train_data, formula, i)

kfold_cv_logistic <- function(train_data, formula, k = 10){
  
  error_vec <- map_dbl(1:k, ~compute_error_logistic(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
  
}

cv_error_logistic <- kfold_cv_logistic(train_data, formula)
```



## LDA

```{r}

mod_lda <- MASS::lda(costs_bin ~ ., data = select(train_data, -fold))

lda_pred <- predict( mod_lda, newdata = select( df_test, -c(costs_bin, total_costs) ) )$class

test_error_lda <- mean(df_test$costs_bin != lda_pred)

test_error_lda
```

10-fold CV: 

```{r}
compute_error_lda <- function(train_data, formula, i){
  
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
  
  mod_lda <- MASS::lda(formula, data = current_train_folds)
  
  lda_pred <- predict( mod_lda, newdata = select( current_test_fold, -c(costs_bin) ) )$class
    
  error <- mean( (current_test_fold$costs_bin != lda_pred)  )
  return(error)
}


kfold_cv_lda <- function(train_data, formula, k = 10){
  
  error_vec <- map_dbl(1:k, ~compute_error_lda(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
  
}

cv_error_lda <- kfold_cv_lda(train_data, formula)

cv_error_lda
```


## QDA

```{r}

mod_qda <- MASS::qda(costs_bin ~ ., data = select(train_data, -fold))

qda_pred <- predict( mod_qda, newdata = select( df_test, -c(costs_bin, total_costs) ) )$class

test_error_qda <- mean(df_test$costs_bin != qda_pred)

test_error_qda
```

10-fold CV: 

```{r}
compute_error_qda <- function(train_data, formula, i){
  
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
  
  mod_qda <- MASS::qda(formula, data = current_train_folds)
  
  qda_pred <- predict( mod_qda, newdata = select( current_test_fold, -c(costs_bin) ) )$class
    
  error <- mean( (current_test_fold$costs_bin != qda_pred)  )
  return(error)
}


kfold_cv_qda <- function(train_data, formula, k = 10){
  
  error_vec <- map_dbl(1:k, ~compute_error_qda(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
  
}

cv_error_qda <- kfold_cv_qda(train_data, formula)

cv_error_qda
```


## Naive Bayes (at least two kernels)

### Gaussian Kernel

```{r}
nb_gaussian <- e1071::naiveBayes(costs_bin ~ ., data = select(train_data, -fold))

nb_pred <- predict(nb_gaussian, newdata = select( df_test, -c(costs_bin, total_costs) ) )

test_error_nb <- mean(df_test$costs_bin != nb_pred)

test_error_nb
```

10-fold CV: 

```{r}
compute_error_nb <- function(train_data, formula, i){
  
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
  
  mod_nb <- e1071::naiveBayes(formula, data = current_train_folds)
  
  nb_pred <- predict( mod_nb, newdata = select( current_test_fold, -c(costs_bin) ) )
    
  error <- mean( (current_test_fold$costs_bin != nb_pred)  )
  return(error)
}


kfold_cv_nb <- function(train_data, formula, k = 10){
  
  error_vec <- map_dbl(1:k, ~compute_error_nb(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
  
}

cv_error_nb <- kfold_cv_nb(train_data, formula)

cv_error_nb
```


### Non Parametric Kernel Density Estimation

```{r}
predictor_matrix <- as.matrix(predictors)

test_matrix <- df_test |> 
  select( -c(costs_bin, total_costs) ) |> 
  as.matrix()

nb_KDE <-
  nonparametric_naive_bayes(
    y = as.factor(df_train %>% pull(costs_bin)),
    x = predictor_matrix
    )

nb_kde_pred <- predict(nb_KDE, newdata = test_matrix )

test_error_nb_kde <- mean(df_test$costs_bin != nb_kde_pred)

test_error_nb_kde

```

10-fold CV:

```{r}
compute_error_nb_kde <- function(train_data, formula, i){
  
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)
  
  current_train_x <- current_train_folds |> select(-costs_bin) |> as.matrix()
  
  current_train_y = current_train_folds$costs_bin
  
  current_test_x <- current_test_fold |> select(-costs_bin) |> as.matrix()
  
  current_test_y = current_test_fold$costs_bin
  
  nb_KDE <-
  nonparametric_naive_bayes(
    y = as.factor(current_train_y),
    x = current_train_x
    )
  
  nb_kde_pred <- predict(nb_KDE, newdata = current_test_x )
    
  error <- mean( (current_test_y != nb_kde_pred)  )
  return(error)
}


kfold_cv_nb_kde <- function(train_data, formula, k = 10){
  
  error_vec <- map_dbl(1:k, ~compute_error_nb_kde(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
  
}

cv_error_nb_kde <- kfold_cv_nb_kde(train_data, formula)

cv_error_nb_kde
```



## Decision Tree (with pruning)

## Bagging (with variable importance)

## Random Forest (with variable importance)

## Boosting (including selecting the tuning parameter)

(For the above three methods, calculate the Gini index on each leaf of the final tree to examine the purity of the node.)

## Neural Network

## Calculate True/False Positive/Negative rates for each method

# Summary Table Qualitative Outcome

```{r}
method <- c("Logistic Regression (Main Effects)", "KNN",
            "LDA", "QDA", "Naive Bayes- Gaussian", "Naive Bayes- KDE")

cv_error <- c(cv_error_logistic, cv_error_knn, cv_error_lda, cv_error_qda, 
              cv_error_nb, cv_error_nb_kde)

test_error <- c(test_error_logistic, test_error_knn, test_error_lda, test_error_qda, 
              test_error_nb, test_error_nb_kde)

error_summary <- tibble(method, cv_error, test_error)

knitr::kable(error_summary)
```

# Bootstrap CI

# Simulation Study
