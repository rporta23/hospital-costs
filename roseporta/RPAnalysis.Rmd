---
title: "Rose Analysis"
author: "Rose Porta"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(class)
library(glmnet)
library(naivebayes)
library(leaps)
library(pls)
library(tree)
library(randomForest)
library(gbm)
library(keras)
library(faux)
```

# Data Cleaning

```{r}
hpc <- read_csv(here::here("hpc.csv"))
# summary(hpc)

hpcdf <- hpc |>
  mutate(
    start = as.Date(`Fiscal Year Begin Date`), end = as.Date(`Fiscal Year End Date`)
  ) |>
  mutate(days = as.numeric(end - start)) |>
  mutate(numBeds = `Total Bed Days Available` / days, id = row_number())
```

```{r}
hpc_clean <- hpcdf |>
  janitor::clean_names() |>
  select(provider_ccn, days, number_of_beds,
    total_costs, rural_versus_urban, provider_type, type_of_control,
    fte_employees_on_payroll,
    total_days = total_days_v_xviii_xix_unknown,
    total_discharges = total_discharges_v_xviii_xix_unknown,
    total_income, total_assets,
    salaries = total_salaries_from_worksheet_a
  ) |>
  na.omit() |>
  mutate(
    rural = ifelse(rural_versus_urban == "R", 1, 0),
    control_bin = case_when(
      type_of_control < 3 ~ "Voluntary",
      (type_of_control >= 3 & type_of_control < 7) ~ "Proprietary",
      type_of_control >= 7 ~ "Governmental"
    ),
    provider_bin = ifelse(provider_type < 3 |
      provider_type == 6,
    "General", "Specialized"
    ),
  ) |>
  select(-c(rural_versus_urban, type_of_control, provider_type))

duplicates <- hpc_clean |>
  group_by(provider_ccn) |>
  summarise(count = n()) |>
  filter(count > 1)

dup <- map_lgl(hpc_clean$provider_ccn, `%in%`, duplicates$provider_ccn)

hpc_clean$duplicate <- as.numeric(dup)

hpc_normalize <- hpc_clean |>
  mutate(
    total_costs = total_costs / days,
    inpatients = total_days / days,
    total_discharges = total_discharges / days,
    total_income = total_income / days,
    salaries = salaries / days
  )
```

```{r}
hpc_dummies <- hpc_normalize |>
  select(-c(provider_ccn, days)) |>
  mutate(
    costs_bin = ifelse(total_costs > median(total_costs), 1, 0)
  ) |>
  tidytable::get_dummies() |>
  select(-c(control_bin_Voluntary, provider_bin_General)) |> 
  select(where(is.numeric))
```

```{r}
quant_scaled <- hpc_dummies |>
  select(-c(
    control_bin_Governmental, control_bin_Proprietary,
    provider_bin_Specialized, rural, costs_bin, duplicate
  )) |>
  scale()

qual <- hpc_dummies |>
  select(c(
    control_bin_Governmental, control_bin_Proprietary,
    provider_bin_Specialized, rural, costs_bin, duplicate
  ))

hpc_scaled <- cbind(quant_scaled, qual)
```


# Train-test Split

```{r}
set.seed(1)

train_prop <- 0.9

n <- nrow(hpc_scaled)
n_train <- train_prop * n
n_test <- n - n_train

hpc_scaled$set <- "Train"
hpc_scaled$set[sample(n, n_test, replace = FALSE)] <- "Test"

df_train <- hpc_scaled |>
  filter(set == "Train") |>
  select(-set)
df_test <- hpc_scaled |>
  filter(set == "Test") |>
  select(-set)
```


# For all methods:

-   For each method you apply, use 10 fold cross-validation estimate for the test error.
-   discuss assumptions

# Quantitative Outcome Analyses

Set Up:

```{r}
predictors <- select(df_train, -c(total_costs, costs_bin))
```

Assign Folds for 10-fold CV

```{r}
n_train <- nrow(df_train)

k <- 10
k.o <- k
kvals <- unique(round(n_train / (1L:floor(n_train / 2))))
temp <- abs(kvals - k)
if (!any(temp == 0)) {
  k <- kvals[temp == min(temp)][1L]
}
f <- ceiling(n_train / k)

df_train$fold <- sample(rep(1L:k, f), n_train)
```

Remove categorical outcome and fold number from train and test data:

```{r}
train_data <- df_train |> select(-c(costs_bin))

test_data <- df_test |> select(-c(costs_bin))
```


## Marginal simple linear regressions

```{r}
# model summaries
map(predictors, ~ summary(lm(total_costs ~ .x, data = df_train)))

simple_models <- map(predictors, ~ lm(total_costs ~ .x, data = df_train))
```

10-fold cv

```{r}
# Estimate with k-fold:

predictor_names <- colnames(predictors)

formulas <- str_c("total_costs", " ~ ", predictor_names)


compute_MSEP_lm <- function(train_data, formula, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  mod <- lm(formula, data = current_train_folds)
  pred <- predict(mod, newdata = select(current_test_fold, -c(total_costs)))

  MSEP <- mean((current_test_fold$total_costs - pred)^2)
  return(MSEP)
}


kfold_cv_lm <- function(train_data, formula, k = 10) {
  MSEP_vec <- map_dbl(1:k, ~ compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

test_error_vec <- map_dbl(formulas, ~ kfold_cv_lm(df_train, formula = .x, k = 10))

test_error_vec
```

True Test Error

```{r}
compute_MSEP_test <- function(formula, train_data, test_data) {
  train_data <- train_data |> select(-fold)

  mod <- lm(formula, data = train_data)
  pred <- predict(mod, newdata = select(test_data, -c(total_costs)))

  MSEP <- mean((test_data$total_costs - pred)^2)
  return(MSEP)
}

test_MSEP_vec <- map_dbl(formulas, ~ compute_MSEP_test(.x, df_train, df_test))
test_MSEP_vec
```

```{r}
method <- str_c("Marginal LR", " ", predictor_names)
```

```{r}
summary_table_simple <- tibble(
  method = method,
  cv_error = test_error_vec,
  test_error = test_MSEP_vec
)

summary_table_simple
```




## Multiple linear regression

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r}
formula <- as.formula("total_costs ~ .")

mod_mlr <- lm(formula, data = select(train_data, -fold))

summary(mod_mlr)
```

```{r}
cv_error_lm <- kfold_cv_lm(train_data, formula)

cv_error_lm
```

```{r}
test_error_lm <- compute_MSEP_test(formula, train_data, df_test)

test_error_lm
```

## Multiple linear regression with interactions and transformations: 

```{r}
formula <-
  as.formula("total_costs ~ . - total_days - salaries + poly(total_days, 2, raw = TRUE) + poly(salaries, 2, raw = TRUE) + fte_employees_on_payroll*total_income + fte_employees_on_payroll*provider_bin_Specialized")

mod_mlr <- lm(formula, data = select(train_data, -fold))

summary(mod_mlr)
```

```{r}
cv_error_lm2 <- kfold_cv_lm(train_data, formula)

cv_error_lm2
```

```{r}
test_error_lm2 <- compute_MSEP_test(formula, train_data, df_test)

test_error_lm2
```


## Regression Tree (with pruning)

```{r}
formula <- as.formula("total_costs ~ .")
tree_train <- tree(
  formula,
  select(train_data, -fold)
)

# pruning -- find best size with cv
set.seed(1)
cv_tree <- cv.tree(tree_train)
cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))]

# refit tree with pruning

tree_pruned <- prune.tree(tree_train, best = 9)

pred <- predict(tree_pruned, newdata = test_data)

test_error_regtree <- mean((test_data$total_costs - pred)^2)

test_error_regtree
```

10-fold CV: 

```{r}
compute_MSEP_regtree <- function(train_data, formula, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  tree_train <- tree(
    formula,
    current_train_folds,
  )

  tree_pruned <- prune.tree(tree_train, best = 9)

  pred <- predict(tree_pruned, newdata = current_test_fold)

  MSEP <- mean((current_test_fold$total_costs - pred)^2)
  return(MSEP)
}


kfold_cv_regtree <- function(train_data, formula, k = 10) {
  MSEP_vec <- map_dbl(1:k, ~ compute_MSEP_regtree(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

cv_error_regtree <- kfold_cv_regtree(train_data, formula)
cv_error_regtree
```


## Bagging (with variable importance)

```{r}
n_trees <- 300
p <- ncol(train_data) - 2

# Bagging
set.seed(1)
bag_train <-
  randomForest(
    formula,
    data = select(train_data, -fold),
    ntree = n_trees,
    mtry = p, # m = p for bagging.
    keep.forest = TRUE, # not necessary, using to compute test errors for all trees.
    importance = TRUE # not necessary, compute importance of variables.
  )

# Test errors:
bag_pred <- predict(bag_train, test_data, predict.all = TRUE)

bag_pred_avg <- map_dbl(
  1:nrow(test_data),
  ~ mean(bag_pred$individual[.x, ])
)

test_error_bag <- mean((bag_pred_avg - test_data$total_costs)^2)

test_error_bag
```

```{r}
importance(bag_train)
```

10-fold CV:

```{r}
compute_MSEP_bag <- function(train_data, formula, i) {
  set.seed(i)

  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  n_trees <- 300
  p <- ncol(current_train_folds) - 1

  bag_train <-
    randomForest(
      formula,
      data = current_train_folds,
      ntree = n_trees,
      mtry = p, # m = p for bagging.
      keep.forest = TRUE, # not necessary, using to compute test errors for all trees.
      importance = TRUE # not necessary, compute importance of variables.
    )

  bag_pred <- predict(bag_train, current_test_fold, predict.all = TRUE)

  bag_pred_avg <- map_dbl(
    1:nrow(current_test_fold),
    ~ mean(bag_pred$individual[.x, ])
  )

  MSEP <- mean((current_test_fold$total_costs - bag_pred_avg)^2)
  return(MSEP)
}


kfold_cv_bag <- function(train_data, formula, k = 10) {
  MSEP_vec <- map_dbl(1:k, ~ compute_MSEP_bag(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

#cv_error_bag <- kfold_cv_bag(train_data, formula)
cv_error_bag <- 0.07086356
```

## Random Forest (with variable importance)

```{r}
compute_error_rf <- function(train_data, formula, i) {
  set.seed(i)

  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  n_trees <- 300

  rf_train <-
    randomForest(
      formula,
      data = current_train_folds,
      ntree = n_trees,
      keep.forest = TRUE, # not necessary, using to compute test errors for all trees.
      importance = TRUE # not necessary, compute importance of variables.
    )

  rf_pred <- predict(bag_train, current_test_fold, predict.all = TRUE)

  rf_pred_avg <- map_dbl(
    1:nrow(current_test_fold),
    ~ mean(rf_pred$individual[.x, ])
  )

  MSEP <- mean((current_test_fold$total_costs - rf_pred_avg)^2)
  return(MSEP)
}


kfold_cv_rf <- function(train_data, formula, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_rf(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

#cv_error_rf <- kfold_cv_rf(train_data, formula)
cv_error_rf <- 0.01186113
```

```{r}
set.seed(1)
rf_train <-
  randomForest(
    formula,
    data = select(train_data, -fold),
    ntree = n_trees, # m = p for bagging.
    keep.forest = TRUE, # not necessary, using to compute test errors for all trees.
    importance = TRUE # not necessary, compute importance of variables.
  )

# Test errors:
rf_pred <- predict(rf_train, test_data, predict.all = TRUE)

rf_pred_avg <- map_dbl(
  1:nrow(test_data),
  ~ mean(rf_pred$individual[.x, ])
)

test_error_rf <- mean((rf_pred_avg - test_data$total_costs)^2)

test_error_rf
```

```{r}
importance(rf_train)
```


## Boosting (including selecting the tuning parameter)

Choose Tuning Paramter With CV:

```{r}
compute_error_boost <- function(train_data, formula, lambda, i) {
  set.seed(i)

  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  n_trees <- 300
  d1 <- 1

  boost_train <-
    gbm(
      formula,
      data = current_train_folds,
      n.trees = n_trees,
      interaction.depth = d1,
      shrinkage = lambda
    )

  boost_pred <- predict(boost_train, current_test_fold, n.trees = n_trees)

  MSEP <- mean((current_test_fold$total_costs - boost_pred)^2)
  return(MSEP)
}


kfold_cv_boost <- function(train_data, formula, lambda, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_boost(train_data, formula, lambda, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

lambda_grid <- seq(0, 3, by = 0.1)

#cv_error_rates <- map_dbl(lambda_grid, ~kfold_cv_boost(train_data, formula, .x, k = 10))

#optimal_lambda <- lambda_grid[which(cv_error_rates == min(cv_error_rates))]

optimal_lambda <- 0.1

#cv_error_rate_optimal_lambda <- min(cv_error_rates)
#cv_error_rate_optimal_lambda

cv_error_rate_optimal_lambda <- 0.09249563

cv_error_boost <- cv_error_rate_optimal_lambda
```


```{r}
d1 <- 1

boost_train <-
  gbm(
    formula,
    data = select(train_data, -fold),
    n.trees = n_trees,
    interaction.depth = d1,
    shrinkage = optimal_lambda
  )

# Test errors:
boost_pred <- predict(boost_train, test_data, n.trees = n_trees)

test_error_boost <- mean((boost_pred - test_data$total_costs)^2)

test_error_boost
```

## Neural Network

```{r}
cv_neural_network <- 0.0721646

testMSE_neural_net <- 0.08771483
```


# Variable Selection

## Best Subset

```{r}
nvmax <- 12
```


```{r}
regfit_best <- regsubsets(total_costs ~ .,
  data = select(train_data, -fold),
  nvmax = nvmax
)
summary_regfit <- summary(regfit_best)
summary_regfit
```

10-fold CV to choose best model:

```{r}
response <- "total_costs"
m <- summary_regfit$which[, -1]
predictor_names <- colnames(predictors)
# colnames(m) <- predictor_names
rhs <- map_chr(1:12, ~ str_c(predictor_names[m[.x, ]], collapse = " + "))
formulas <- str_c(response, " ~ ", rhs)

cv_error_vec <- map_dbl(formulas, ~ kfold_cv_lm(train_data, formula = .x, k = 10))
cv_error_vec

optimal_size <- which(cv_error_vec == min(cv_error_vec))
optimal_size

cv_error_bsub <- cv_error_vec[optimal_size]
cv_error_bsub
```

Optimal Model Size: 6

Summary of Best Model:

```{r}
best_formula <- formulas[optimal_size]
mod_best <- lm(best_formula, train_data)
summary(mod_best)
```


Test Error:

```{r}
test_error_bsub <- compute_MSEP_test(best_formula, train_data, test_data)

test_error_bsub
```


## Forward Stepwise

```{r}
# Forward stepwise selection -----
regfit_fwd <- regsubsets(total_costs ~ .,
  data = select(train_data, -fold),
  nvmax = nvmax, method = "forward"
)
summary_regfit <- summary(regfit_fwd)
summary_regfit
```

10-fold CV to choose best model:

```{r}
response <- "total_costs"
m <- summary_regfit$which[, -1]
predictor_names <- colnames(predictors)
# colnames(m) <- predictor_names
rhs <- map_chr(1:12, ~ str_c(predictor_names[m[.x, ]], collapse = " + "))
formulas <- str_c(response, " ~ ", rhs)

cv_error_vec <- map_dbl(formulas, ~ kfold_cv_lm(train_data, formula = .x, k = 10))
cv_error_vec

optimal_size <- which(cv_error_vec == min(cv_error_vec))
optimal_size

cv_error_fwd <- cv_error_vec[optimal_size]
cv_error_fwd
```

Optimal Model Size: 6

Summary of Best Model:

```{r}
best_formula <- formulas[optimal_size]
mod_best <- lm(best_formula, train_data)
summary(mod_best)
```

Test Error:

```{r}
test_error_fwd <- compute_MSEP_test(best_formula, train_data, test_data)

test_error_fwd
```


## Backward Stepwise

```{r}
# Backward stepwise selection -----
regfit_bwd <- regsubsets(total_costs ~ .,
  data = select(train_data, -fold),
  nvmax = nvmax,
  method = "backward"
)
summary_regfit <- summary(regfit_bwd)
summary_regfit
```

10-fold CV to choose best model:

```{r}
response <- "total_costs"
m <- summary_regfit$which[, -1]
predictor_names <- colnames(predictors)
# colnames(m) <- predictor_names
rhs <- map_chr(1:12, ~ str_c(predictor_names[m[.x, ]], collapse = " + "))
formulas <- str_c(response, " ~ ", rhs)

cv_error_vec <- map_dbl(formulas, ~ kfold_cv_lm(train_data, formula = .x, k = 10))
cv_error_vec

optimal_size <- which(cv_error_vec == min(cv_error_vec))
optimal_size

cv_error_bwd <- cv_error_vec[optimal_size]
cv_error_bwd
```

Optimal Model Size: 6

Summary of Best Model:

```{r}
best_formula <- formulas[optimal_size]
mod_best <- lm(best_formula, train_data)
summary(mod_best)
```

Test Error:

```{r}
test_error_bwd <- compute_MSEP_test(best_formula, train_data, test_data)

test_error_bwd
```

(For the above 3 methods, determine which model of a given size is best by comparing the 10-fold cross validation estimate of the test error.) 

## Ridge regression (find the best tuning parameter using cross-validation)

```{r}
x_train <- model.matrix(total_costs ~ . - fold, data = train_data)[, -1]
y_train <- train_data$total_costs

x_test <- model.matrix(total_costs ~ ., data = test_data)[, -1]
y_test <- test_data$total_costs
```


```{r}
set.seed(1)

lambda_grid <- 10^seq(10, -2, length = 100)
cv_ridge <-
  cv.glmnet(
    x_train,
    y_train,
    alpha = 0,
    lambda = lambda_grid,
    type.measure = "mse",
    nfolds = 10 # Set the number of folds to n to get LOOCV
  )

cv_ridge$lambda.min

lambda_min <- cv_ridge$lambda.min
```

```{r}
ridge_reg <- glmnet(x_train, y_train, alpha = 0, lambda = lambda_min)
coef(ridge_reg)
```

```{r}
pred_ridge <- predict(ridge_reg, newx = x_test)

test_error_ridge <- mean((y_test - pred_ridge)^2)

test_error_ridge
```

10-fold CV:

```{r}
compute_MSEP_ridge <- function(train_data, lambda, alpha, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  x_train <- model.matrix(total_costs ~ ., data = current_train_folds)[, -1]
  y_train <- current_train_folds$total_costs

  x_test <- model.matrix(total_costs ~ ., data = current_test_fold)[, -1]
  y_test <- current_test_fold$total_costs

  ridge_reg <- glmnet(x_train, y_train, alpha = alpha, lambda = lambda)
  pred_ridge <- predict(ridge_reg, newx = x_test)

  MSEP <- mean((current_test_fold$total_costs - pred_ridge)^2)
  return(MSEP)
}

# compute_MSEP_lm(train_data, formula, i)

kfold_cv_ridge <- function(train_data, lambda, alpha, k = 10) {
  MSEP_vec <- map_dbl(1:k, ~ compute_MSEP_ridge(train_data, lambda, alpha, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

cv_error_ridge <- kfold_cv_ridge(train_data, lambda_min, alpha = 0)
cv_error_ridge
```

## Bootstrap Estimate of Standard Errors for Ridge Regression

```{r}
# original training data
given_sample <- train_data |> select(-fold)

# function to compute bootstrap estimate of SE
compute_bootstrap_est <- function(given_sample, lambda_min, b) {
  set.seed(b)

  n <- nrow(given_sample)

  bootstrap_sample_idx <- sample(n, replace = TRUE)
  Z <- given_sample[bootstrap_sample_idx, , drop = F]
  x_b <- model.matrix(total_costs ~ ., data = Z)[, -1]
  y_b <- Z$total_costs
  modb <- glmnet(x_b, y_b, alpha = 0, lambda = lambda_min)
  est_coef_ridge <- coef(modb)
  est_coef_ridge_mat <- t(as.matrix(est_coef_ridge))[, -1]

  return(data.frame(as.list(est_coef_ridge_mat)))
}

# number of times to repeat bootstrap
B <- 1000

# compute 1000 bootstrap estimates
boot_ests <- map_dfr(1:B, ~ compute_bootstrap_est(given_sample, lambda_min, .x))

# compute standard error of bootstrap estimates
SE_boot <- map_dbl(boot_ests, sd)

SE_boot
```

```{r}
# compare to multiple regression
formula <- as.formula("total_costs ~ .")

mod_mlr <- lm(formula, data = select(train_data, -fold))

SE_ols <- coef(summary(mod_mlr))[, "Std. Error"][-1]

SE_ols
```

```{r}
# plot to compare ols to ridge
data_plot <- tibble(SE_boot, SE_ols)

ggplot(data_plot, aes(x = SE_boot, y = SE_ols)) +
  geom_point() +
  labs(
    title = "Ridge Versus OLS Standard Errors",
    x = "Ridge (bootstrap) SE",
    y = "OLS SE"
  )
```



## Lasso (find the best tuning parameter using cross-validation)

```{r}
set.seed(1)

# fit lasso model
lambda_grid <- 10^seq(10, -2, length = 100)

# 10-fold cross validation to choose tuning parameter
cv_lasso <-
  cv.glmnet(
    x_train,
    y_train,
    alpha = 1,
    lambda = lambda_grid,
    type.measure = "mse",
    nfolds = 10 # Set the number of folds to n to get LOOCV
  )

lambda_min <- cv_lasso$lambda.min
lambda_min
```


```{r}
# fit lasso model with optimal tuning parameter
lasso_bestlambda <- glmnet(x_train, y_train, alpha = 1, lambda = lambda_min)
coef(lasso_bestlambda)
```

```{r}
pred_lasso <- predict(lasso_bestlambda, newx = x_test)

test_error_lasso <- mean((y_test - pred_lasso)^2)

test_error_lasso
```

10-fold CV:

```{r}
cv_error_lasso <- kfold_cv_ridge(train_data, lambda_min, alpha = 1)
cv_error_lasso
```


## Principal Components Regression (PCR)


```{r}
pcr_fit <- pcr(total_costs ~ .,
  data = select(train_data, -fold),
  validation = "CV",
  segments = 10
)
summary(pcr_fit)
```

```{r}
validationplot(pcr_fit, val.type = "MSEP")
```

```{r}
pred_test <- predict(pcr_fit, test_data, ncomp = 5)

test_error_pcr <- mean((test_data$total_costs - pred_test)^2)

test_error_pcr
```

10-fold CV:

```{r}
compute_MSEP_pcr <- function(train_data, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  pcr_fit <- pcr(total_costs ~ .,
    data = current_train_folds,
    validation = "CV",
    segments = 10
  )

  pred <- predict(pcr_fit, current_test_fold, ncomp = 5)

  MSEP <- mean((current_test_fold$total_costs - pred)^2)
  return(MSEP)
}

# compute_MSEP_lm(train_data, formula, i)

kfold_cv_pcr <- function(train_data, k = 10) {
  MSEP_vec <- map_dbl(1:k, ~ compute_MSEP_pcr(train_data, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

cv_error_pcr <- kfold_cv_pcr(train_data)
cv_error_pcr
```


# Summary Table Quant Outcome

```{r}
method <- c(
  "Linear Regression (Main Effects)", "Linear Regression (Transformations)",
  "Best Subset", "Forward Stepwise", "Backward Stepwise", "Ridge", "Lasso", "PCR", "Regression Tree", "Bagging", "Random Forest", "Boosting"
)

cv_error <- c(
  cv_error_lm, cv_error_lm2, cv_error_bsub, cv_error_fwd,
  cv_error_bwd, cv_error_ridge, cv_error_lasso, cv_error_pcr,
  cv_error_regtree, cv_error_bag, cv_error_rf, cv_error_boost
)

test_error <- c(
  test_error_lm, test_error_lm2, test_error_bsub, test_error_fwd,
  test_error_bwd, test_error_ridge, test_error_lasso, test_error_pcr,
  test_error_regtree, test_error_bag, test_error_rf, test_error_boost
)

error_summary <- summary_table_simple |> 
  bind_rows( tibble(method, cv_error, test_error) ) |> 
  mutate_if(is.numeric, ~ round(.x, 3))

knitr::kable(error_summary)
```


# Qualitative Outcome Analyses


```{r}
confusion_matrix <- function(pred, true_values) {
  true_positives <- sum((pred == 1) & (true_values == 1))
  false_positives <- sum((pred == 1) & (true_values == 0))
  true_negatives <- sum((pred == 0) & (true_values == 0))
  false_negatives <- sum((pred == 0) & (true_values == 1))

  all_negatives <- sum(true_values == 0)
  all_positives <- sum(true_values == 1)

  fpr <- false_positives / all_negatives
  fnr <- false_negatives / all_positives
  tpr <- true_positives / all_positives
  tnr <- true_negatives / all_negatives

  result <- c(fpr, fnr, tpr, tnr)

  names(result) <- c("False Positive Rate", "False Negative Rate", "True Positive Rate", "True Negative Rate")

  return(result)
}
```


```{r}
train_data <- df_train |> select(-c(total_costs))

test_data <- df_test |> select(-c(total_costs))
```


## KNN

Choose optimal number of nearest neighbors, c, using CV: 

```{r}
compute_error_knn <- function(train_data, i, c) {
  set.seed(i)

  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  knn_pred <-
    knn(
      train = select(current_train_folds, -costs_bin),
      test = select(current_test_fold, -c(costs_bin)),
      cl = current_train_folds$costs_bin,
      k = c,
      prob = TRUE
    )

  error <- mean((current_test_fold$costs_bin != knn_pred))
  return(error)
}


kfold_cv_knn <- function(train_data, k = 10, c) {
  error_vec <- map_dbl(1:k, ~ compute_error_knn(train_data, .x, c))
  avg_error <- mean(error_vec)
  return(avg_error)
}

c_values <- 1:100

cv_error_rates <- map_dbl(c_values, ~ kfold_cv_knn(train_data, k = 10, c = .x))

optimal_c <- which(cv_error_rates == min(cv_error_rates))

optimal_c <- 17

cv_error_rate_optimal_c <- min(cv_error_rates)
cv_error_rate_optimal_c

cv_error_knn <- cv_error_rate_optimal_c
```

Fit model with optimal number of nearest neighbors, c = 17:

```{r}
set.seed(1)

knn_pred <-
  knn(
    train = select(df_train, -c(total_costs, costs_bin, fold)),
    test = select(df_test, -c(total_costs, costs_bin)),
    cl = df_train$costs_bin,
    k = optimal_c,
    prob = TRUE
  )

test_error_knn <- mean(test_data$costs_bin != knn_pred)

test_error_knn
```

```{r}
cm_knn <- confusion_matrix(knn_pred, test_data$costs_bin)
cm_knn
```


## Multiple logistic regression ** issue 0/1 probabilities

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r}
formula <- as.formula("costs_bin ~ .")

mod_mlogr <- glm(formula,
  data = train_data |> select(-fold),
  family = binomial(link = "logit")
)

summary(mod_mlogr)
```

```{r}
log_pred <- predict(mod_mlogr,
  newdata = select(test_data, -c(costs_bin)),
  type = "response"
)

pred_class <- ifelse(log_pred >= 0.5, 1, 0)

test_error_logistic <- mean(df_test$costs_bin != pred_class)

test_error_logistic
```

```{r}
compute_error_logistic <- function(train_data, formula, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  mod_mlogr <- glm(formula,
    data = current_train_folds,
    family = binomial(link = "logit")
  )

  log_pred <- predict(mod_mlogr,
    newdata = current_test_fold |> select(-costs_bin),
    type = "response"
  )

  pred_class <- ifelse(log_pred >= 0.5, 1, 0)

  error <- mean((current_test_fold$costs_bin != pred_class))
  return(error)
}

compute_error_logistic(train_data, formula, 1)


# compute_MSEP_lm(train_data, formula, i)

kfold_cv_logistic <- function(train_data, formula, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_logistic(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

cv_error_logistic <- kfold_cv_logistic(train_data, formula)
```

```{r}
cm_logistic <- confusion_matrix(pred_class, test_data$costs_bin)
cm_logistic
```


## Multiple logistic regression with Transformations and Interactions

```{r}
formula <-
  as.formula("costs_bin ~ . - total_days - salaries + poly(total_days, 2, raw = TRUE) + poly(salaries, 2, raw = TRUE) + fte_employees_on_payroll*total_income + fte_employees_on_payroll*provider_bin_Specialized")

mod_mlogr <- glm(formula,
  data = train_data |> select(-fold),
  family = binomial(link = "logit")
)

summary(mod_mlogr)
```


```{r}
log_pred <- predict(mod_mlogr,
  newdata = select(test_data, -c(costs_bin)),
  type = "response"
)

pred_class <- ifelse(log_pred >= 0.5, 1, 0)

test_error_logistic2 <- mean(df_test$costs_bin != pred_class)

test_error_logistic2
```

```{r}
cv_error_logistic2 <- kfold_cv_logistic(train_data, formula)
cv_error_logistic2
```

```{r}
cm_logistic2 <- confusion_matrix(pred_class, test_data$costs_bin)
cm_logistic2
```


## LDA

```{r}
formula <- as.formula("costs_bin ~ .")

mod_lda <- MASS::lda(costs_bin ~ ., data = select(train_data, -fold))

lda_pred <- predict(mod_lda, newdata = select(df_test, -c(costs_bin, total_costs)))$class

test_error_lda <- mean(df_test$costs_bin != lda_pred)

test_error_lda
```

10-fold CV: 

```{r}
compute_error_lda <- function(train_data, formula, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  mod_lda <- MASS::lda(formula, data = current_train_folds)

  lda_pred <- predict(mod_lda, newdata = select(current_test_fold, -c(costs_bin)))$class

  error <- mean((current_test_fold$costs_bin != lda_pred))
  return(error)
}


kfold_cv_lda <- function(train_data, formula, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_lda(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

cv_error_lda <- kfold_cv_lda(train_data, formula)

cv_error_lda
```

```{r}
cm_lda <- confusion_matrix(lda_pred, test_data$costs_bin)
cm_lda
```



## QDA

```{r}
mod_qda <- MASS::qda(costs_bin ~ ., data = select(train_data, -fold))

qda_pred <- predict(mod_qda, newdata = select(df_test, -c(costs_bin, total_costs)))$class

test_error_qda <- mean(df_test$costs_bin != qda_pred)

test_error_qda
```

10-fold CV: 

```{r}
compute_error_qda <- function(train_data, formula, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  mod_qda <- MASS::qda(formula, data = current_train_folds)

  qda_pred <- predict(mod_qda, newdata = select(current_test_fold, -c(costs_bin)))$class

  error <- mean((current_test_fold$costs_bin != qda_pred))
  return(error)
}


kfold_cv_qda <- function(train_data, formula, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_qda(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

cv_error_qda <- kfold_cv_qda(train_data, formula)

cv_error_qda
```

```{r}
cm_qda <- confusion_matrix(qda_pred, test_data$costs_bin)
cm_qda
```



## Naive Bayes (at least two kernels)

### Gaussian Kernel

```{r}
nb_gaussian <- e1071::naiveBayes(costs_bin ~ ., data = select(train_data, -fold))

nb_pred <- predict(nb_gaussian, newdata = select(df_test, -c(costs_bin, total_costs)))

test_error_nb <- mean(df_test$costs_bin != nb_pred)

test_error_nb
```

10-fold CV: 

```{r}
compute_error_nb <- function(train_data, formula, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  mod_nb <- e1071::naiveBayes(formula, data = current_train_folds)

  nb_pred <- predict(mod_nb, newdata = select(current_test_fold, -c(costs_bin)))

  error <- mean((current_test_fold$costs_bin != nb_pred))
  return(error)
}


kfold_cv_nb <- function(train_data, formula, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_nb(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

cv_error_nb <- kfold_cv_nb(train_data, formula)

cv_error_nb
```

```{r}
cm_nb <- confusion_matrix(nb_pred, test_data$costs_bin)
cm_nb
```


### Non Parametric Kernel Density Estimation

```{r}
predictor_matrix <- as.matrix(predictors)

test_matrix <- df_test |>
  select(-c(costs_bin, total_costs)) |>
  as.matrix()

nb_KDE <-
  nonparametric_naive_bayes(
    y = as.factor(df_train %>% pull(costs_bin)),
    x = predictor_matrix
  )

nb_kde_pred <- predict(nb_KDE, newdata = test_matrix)

test_error_nb_kde <- mean(df_test$costs_bin != nb_kde_pred)

test_error_nb_kde
```

10-fold CV:

```{r}
compute_error_nb_kde <- function(train_data, formula, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  current_train_x <- current_train_folds |>
    select(-costs_bin) |>
    as.matrix()

  current_train_y <- current_train_folds$costs_bin

  current_test_x <- current_test_fold |>
    select(-costs_bin) |>
    as.matrix()

  current_test_y <- current_test_fold$costs_bin

  nb_KDE <-
    nonparametric_naive_bayes(
      y = as.factor(current_train_y),
      x = current_train_x
    )

  nb_kde_pred <- predict(nb_KDE, newdata = current_test_x)

  error <- mean((current_test_y != nb_kde_pred))
  return(error)
}


kfold_cv_nb_kde <- function(train_data, formula, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_nb_kde(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

cv_error_nb_kde <- kfold_cv_nb_kde(train_data, formula)

cv_error_nb_kde
```

```{r}
cm_nb_kde <- confusion_matrix(nb_kde_pred, test_data$costs_bin)
cm_nb_kde
```

# Variable Selection- Qualitative Outcome

## Forward Stepwise

```{r}
rhs <- str_c(predictor_names, collapse = "+")

formula <- as.formula(str_c("costs_bin ~", rhs))

mod_null <- glm(costs_bin ~ 1,
  data = train_data |> select(-fold),
  family = binomial(link = "logit")
)

m.forward <- step(mod_null, scope = formula, direction = "forward")

summary(m.forward)
```

```{r}
fwd_pred <- predict(m.forward,
  newdata = select(test_data, -c(costs_bin)),
  type = "response"
)

pred_class <- ifelse(fwd_pred >= 0.5, 1, 0)

test_error_fwd_bin <- mean(df_test$costs_bin != pred_class)

test_error_fwd_bin
```

```{r}
fwd_formula <- m.forward$formula

cv_error_fwd_bin <- kfold_cv_logistic(train_data, fwd_formula)
cv_error_fwd_bin
```

```{r}
cm_fwd_bin <- confusion_matrix(pred_class, test_data$costs_bin)
cm_fwd_bin
```


## Backward Stepwise

```{r}
mod_full <- update(mod_null, formula)

m.backward <- step(mod_full, direction = "backward")

summary(m.backward)
```

```{r}
bwd_pred <- predict(m.backward,
  newdata = select(test_data, -c(costs_bin)),
  type = "response"
)

pred_class <- ifelse(bwd_pred >= 0.5, 1, 0)

test_error_bwd_bin <- mean(df_test$costs_bin != pred_class)

test_error_bwd_bin
```

```{r}
bwd_formula <- m.backward$formula

cv_error_bwd_bin <- kfold_cv_logistic(train_data, bwd_formula)
cv_error_bwd_bin
```

```{r}
cm_bwd_bin <- confusion_matrix(pred_class, test_data$costs_bin)
cm_bwd_bin
```


## Ridge regression (find the best tuning parameter using cross-validation)

```{r}
x_train <- model.matrix(costs_bin ~ . - fold, data = train_data)[, -1]
y_train <- train_data$costs_bin

x_test <- model.matrix(costs_bin ~ ., data = test_data)[, -1]
y_test <- test_data$costs_bin
```


```{r}
set.seed(1)

lambda_grid <- 10^seq(10, -2, length = 100)
cv_ridge <-
  cv.glmnet(
    x_train,
    y_train,
    alpha = 0,
    lambda = lambda_grid,
    family = "binomial",
    nfolds = 10
  )

cv_ridge$lambda.min

lambda_min <- cv_ridge$lambda.min
```

```{r}
ridge_reg <- glmnet(x_train, y_train, alpha = 0, family = "binomial", lambda = lambda_min)
coef(ridge_reg)
```

```{r}
pred_ridge <- predict(ridge_reg, newx = x_test, type = "response")

pred_class <- ifelse(pred_ridge >= 0.5, 1, 0)

test_error_ridge_bin <- mean(y_test != pred_class)

test_error_ridge_bin
```

10-fold CV:

```{r}
compute_error_ridge_bin <- function(train_data, lambda, alpha, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  x_train <- model.matrix(costs_bin ~ ., data = current_train_folds)[, -1]
  y_train <- current_train_folds$costs_bin

  x_test <- model.matrix(costs_bin ~ ., data = current_test_fold)[, -1]
  y_test <- current_test_fold$costs_bin

  ridge_reg <- glmnet(x_train, y_train, alpha = alpha, family = "binomial", lambda = lambda)
  pred_ridge <- predict(ridge_reg, newx = x_test, type = "response")

  pred_class <- ifelse(pred_ridge >= 0.5, 1, 0)

  error <- mean(current_test_fold$costs_bin != pred_class)
  return(error)
}

# compute_MSEP_lm(train_data, formula, i)

kfold_cv_ridge_bin <- function(train_data, lambda, alpha, k = 10) {
  MSEP_vec <- map_dbl(1:k, ~ compute_error_ridge_bin(train_data, lambda, alpha, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

cv_error_ridge_bin <- kfold_cv_ridge_bin(train_data, lambda_min, alpha = 0)
cv_error_ridge_bin
```

```{r}
cm_ridge_bin <- confusion_matrix(pred_class, test_data$costs_bin)
cm_ridge_bin
```

## Lasso (find the best tuning parameter using cross-validation)

```{r}
set.seed(1)

# fit lasso model
lambda_grid <- 10^seq(10, -2, length = 100)

# 10-fold cross validation to choose tuning parameter
cv_lasso <-
  cv.glmnet(
    x_train,
    y_train,
    alpha = 1,
    lambda = lambda_grid,
    family = "binomial",
    nfolds = 10 # Set the number of folds to n to get LOOCV
  )

lambda_min <- cv_lasso$lambda.min
lambda_min
```


```{r}
# fit lasso model with optimal tuning parameter
lasso_bestlambda <- glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = lambda_min)
coef(lasso_bestlambda)
```

```{r}
pred_lasso <- predict(lasso_bestlambda, newx = x_test, type = "response")

pred_class <- ifelse(pred_lasso >= 0.5, 1, 0)

test_error_lasso_bin <- mean(y_test != pred_class)

test_error_lasso_bin
```

10-fold CV:

```{r}
cv_error_lasso_bin <- kfold_cv_ridge_bin(train_data, lambda_min, alpha = 1)
cv_error_lasso_bin
```

```{r}
cm_lasso_bin <- confusion_matrix(pred_class, test_data$costs_bin)
cm_lasso_bin
```


## Decision Tree (with pruning)

```{r}
train_data <- train_data |>
  mutate(costs_bin = as.factor(costs_bin))

test_data <- test_data |>
  mutate(costs_bin = as.factor(costs_bin))

formula <- as.formula("costs_bin ~ .")
tree_train <- tree(formula,
  select(train_data, -fold),
  control = tree.control(1866496, mincut = 1000),
  split = "gini"
)

# pruning -- find best size with cv
set.seed(1)
cv_tree <- cv.tree(tree_train)
cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))]

# refit tree with pruning

tree_pruned <- prune.misclass(tree_train, best = 4)

pred <- predict(tree_pruned, newdata = test_data, type = "class")

test_error_classtree <- mean(test_data$costs_bin != pred)

test_error_classtree
```

```{r}
cm_classtree <- confusion_matrix(pred, test_data$costs_bin)
cm_classtree
```


10-fold CV: 

```{r}
compute_error_classtree <- function(train_data, formula, i) {
  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  tree_train <- tree(formula,
    current_train_folds,
    control = tree.control(1866496, mincut = 1000),
    split = "gini"
  )

  tree_pruned <- prune.tree(tree_train, best = 4)

  pred <- predict(tree_pruned, newdata = current_test_fold, type = "class")

  error <- mean(current_test_fold$costs_bin != pred)
  return(error)
}


kfold_cv_classtree <- function(train_data, formula, k = 10) {
  MSEP_vec <- map_dbl(1:k, ~ compute_error_classtree(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

cv_error_classtree <- kfold_cv_classtree(train_data, formula)
cv_error_classtree
```


## Bagging (with variable importance)

```{r}
n_trees <- 300
p <- ncol(train_data) - 2

# Bagging
set.seed(1)
bag_train <-
  randomForest(
    formula,
    data = select(train_data, -fold),
    ntree = n_trees,
    mtry = p, # m = p for bagging.
    keep.forest = TRUE, # not necessary, using to compute test errors for all trees.
    importance = TRUE # not necessary, compute importance of variables.
  )

# Test errors:
bag_pred <- predict(bag_train, test_data, type = "class")

test_error_bag_cl <- mean(bag_pred != test_data$costs_bin)

test_error_bag_cl
```

```{r}
importance(bag_train)
```

```{r}
cm_bag <- confusion_matrix(bag_pred, test_data$costs_bin)
cm_bag
```

```{r}
mean_decrease_Gini <- importance(bag_train, type = 2)
mean_decrease_Gini
```



10-fold CV:

```{r}
compute_error_bag_cl <- function(train_data, formula, i) {
  set.seed(i)

  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  n_trees <- 300
  p <- ncol(current_train_folds) - 1

  bag_train <-
    randomForest(
      formula,
      data = current_train_folds,
      ntree = n_trees,
      mtry = p, # m = p for bagging.
      keep.forest = TRUE, # not necessary, using to compute test errors for all trees.
      importance = TRUE # not necessary, compute importance of variables.
    )

  bag_pred <- predict(bag_train, current_test_fold, type = "class")

  error <- mean(bag_pred != current_test_fold$costs_bin)

  return(error)
}


kfold_cv_bag_cl <- function(train_data, formula, k = 10) {
  MSEP_vec <- map_dbl(1:k, ~ compute_error_bag_cl(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
}

cv_error_bag_cl <- kfold_cv_bag_cl(train_data, formula)
cv_error_bag_cl
```

## Random Forest (with variable importance)

```{r}
compute_error_rf_cl <- function(train_data, formula, i) {
  set.seed(i)

  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  n_trees <- 300
  p <- ncol(current_train_folds) - 1

  rf_train <-
    randomForest(
      formula,
      data = current_train_folds,
      ntree = n_trees,
      keep.forest = TRUE, # not necessary, using to compute test errors for all trees.
      importance = TRUE # not necessary, compute importance of variables.
    )

  rf_pred <- predict(rf_train, current_test_fold, type = "class")

  error <- mean(rf_pred != current_test_fold$costs_bin)

  return(error)
}


kfold_cv_rf_cl <- function(train_data, formula, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_rf_cl(train_data, formula, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

# cv_error_rf_cl <- kfold_cv_rf_cl(train_data, formula)
cv_error_rf_cl <- 0.03640008
```

```{r}
set.seed(1)
rf_train <-
  randomForest(
    formula,
    data = select(train_data, -fold),
    ntree = n_trees,
    keep.forest = TRUE, # not necessary, using to compute test errors for all trees.
    importance = TRUE # not necessary, compute importance of variables.
  )

# Test errors:
rf_pred <- predict(rf_train, test_data, type = "class")

test_error_rf_cl <- mean(rf_pred != test_data$costs_bin)
test_error_rf_cl
```

```{r}
importance(rf_train)
```


```{r}
cm_rf <- confusion_matrix(rf_pred, test_data$costs_bin)
cm_rf
```

```{r}
mean_decrease_Gini <- importance(rf_train, type = 2)
mean_decrease_Gini
```

## Boosting (including selecting the tuning parameter)

Choose Tuning Paramter With CV:

```{r}
train_data <- df_train |> select(-c(total_costs))

test_data <- df_test |> select(-c(total_costs))
```


```{r}
compute_error_boost <- function(train_data, formula, lambda, i) {
  set.seed(i)

  current_train_folds <- filter(train_data, fold != i) |> select(-fold)
  current_test_fold <- filter(train_data, fold == i) |> select(-fold)

  n_trees <- 300

  boost_train <-
    gbm(
      formula,
      data = current_train_folds,
      distribution = "bernoulli",
      n.trees = n_trees,
      shrinkage = lambda
    )

  boost_pred <- predict(boost_train, current_test_fold, n.trees = n_trees, type = "response")

  pred_class <- ifelse(boost_pred >= 0.5, 1, 0)

  error <- mean((pred_class != current_test_fold$costs_bin))

  return(error)
}


kfold_cv_boost <- function(train_data, formula, lambda, k = 10) {
  error_vec <- map_dbl(1:k, ~ compute_error_boost(train_data, formula, lambda, .x))
  avg_error <- mean(error_vec)
  return(avg_error)
}

lambda_grid <- seq(0, 3, by = 0.1)

# cv_error_rates <- map_dbl(lambda_grid, ~kfold_cv_boost(train_data, formula, .x, k = 10))

# optimal_lambda <- lambda_grid[which(cv_error_rates == min(cv_error_rates))]

optimal_lambda <- 0.2

# cv_error_rate_optimal_lambda <- min(cv_error_rates)
# cv_error_rate_optimal_lambda

cv_error_rate_optimal_lambda <- 0.03718827

cv_error_boost_cl <- cv_error_rate_optimal_lambda
```


```{r}
set.seed(1)
boost_train <-
  gbm(
    formula,
    data = select(train_data, -fold),
    distribution = "bernoulli",
    n.trees = n_trees,
    shrinkage = optimal_lambda,
  )

# Test errors:
boost_pred <- predict(boost_train, test_data, n.trees = n_trees, type = "response")

pred_class <- ifelse(boost_pred >= 0.5, 1, 0)

test_error_boost_cl <- mean((pred_class != test_data$costs_bin))

test_error_boost_cl
```

```{r}
cm_boost <- confusion_matrix(pred_class, test_data$costs_bin)
cm_boost
```



(For the above three methods, calculate the Gini index on each leaf of the final tree to examine the purity of the node.)

## Neural Network

```{r, eval = FALSE}
x_train <- train_data |> select(-c(costs_bin, fold))
y_train <- train_data$costs_bin

x_test <- test_data |> select(-costs_bin)
y_test <- test_data$costs_bin

modelnn <- keras_model_sequential()
modelnn %>%
  layer_dense(
    units = 256, activation = "relu",
    input_shape = 784
  ) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax")

modelnn %>%
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(),
    metrics = "accuracy"
  )

system.time(
  history <-
    modelnn %>%
    fit(
      x_train,
      y_train,
      epochs = 30,
      batch_size = 128,
      validation_split = 0.2
    )
)

pred_nn <- predict(x_test)
```


## Calculate True/False Positive/Negative rates for each method

# Summary Table Qualitative Outcome

```{r}
cm_summary <- bind_rows(
  cm_logistic, cm_logistic2, cm_fwd_bin,
  cm_bwd_bin, cm_ridge_bin, cm_lasso_bin,
  cm_knn, cm_lda, cm_qda,
  cm_nb, cm_nb_kde, cm_classtree, cm_bag,
  cm_rf, cm_boost
)

# cm_summary
```

\newpage


```{r}
method <- c(
  "Logistic Regression (Main Effects)", "Logistic Regression (Transformations)",
  "Forward Stepwise", "Backward Stepwise", "Ridge", "Lasso", "KNN",
  "LDA", "QDA", "Naive Bayes- Gaussian", "Naive Bayes- KDE", "Decision Tree",
  "Bagging", "Random Forest", "Boosting"
)

cv_error <- c(
  cv_error_logistic, cv_error_logistic2, cv_error_fwd_bin,
  cv_error_bwd_bin, cv_error_ridge_bin, cv_error_lasso_bin,
  cv_error_knn, cv_error_lda, cv_error_qda,
  cv_error_nb, cv_error_nb_kde, cv_error_classtree, cv_error_bag_cl,
  cv_error_rf_cl, cv_error_boost_cl
)

test_error <- c(
  test_error_logistic, test_error_logistic2, test_error_fwd_bin,
  test_error_bwd_bin, test_error_ridge_bin, test_error_lasso_bin,
  test_error_knn, test_error_lda, test_error_qda,
  test_error_nb, test_error_nb_kde, test_error_classtree, test_error_bag_cl,
  test_error_rf_cl, test_error_boost_cl
)

error_summary <- tibble(method, cv_error, test_error) |>
  bind_cols(cm_summary) |>
  mutate_if(is.numeric, ~ round(.x, 2))

knitr::kable(error_summary)
```


# Simulation Study

```{r}
```



```{r}
library(tidyverse)
library(faux)

seed = 1

train_data <- df_train |> select(-c(costs_bin))

formula <- as.formula("total_costs ~ .")

mod_mlr <- lm(formula, data = select(train_data, -fold))

compute_min_k <- function(df_train, seed) {
  set.seed(seed)

  df <- df_train |> select(-c(provider_bin_Specialized, control_bin_Governmental, control_bin_Proprietary, costs_bin, rural, fold,
                              duplicate, total_costs))

  sim <- sim_df(df, 1000)

  cat_df <- hpc_scaled |> select(c(provider_bin_Specialized, control_bin_Governmental, control_bin_Proprietary, rural, duplicate))

  p <- colMeans(cat_df)

  sim_cat <- data.frame(matrix(nrow = 1000, ncol = 0))

  for (i in colnames(cat_df)) {
    sim_cat[, i] <- rbinom(1000, 1, p = p[i])
  }
  
  simulated_x <- cbind(sim, sim_cat)

  simulated_true_fx <- predict(mod_mlr, newdata = simulated_x)
  sigma <- 0.25
  simulated_y <- simulated_true_fx + rnorm(length(simulated_true_fx), 0, sigma)
  
  sim <- simulated_x |>
    mutate(total_costs = simulated_y,
           costs_bin = ifelse(total_costs > median(total_costs), 1, 0)
           )

  train_prop <- 0.9

  n <- nrow(sim)
  n_train <- train_prop * n
  n_test <- n - n_train

  sim$set <- "Train"
  sim$set[sample(n, n_test, replace = FALSE)] <- "Test"

  sim <- sim |> select(-id)

  train.X <- sim |>
    filter(set == "Train") |>
    select(-c(costs_bin, total_costs, set))

  train.Y <- sim |>
    filter(set == "Train") |>
    pull(costs_bin)

  test.X <- sim |>
    filter(set == "Test") |>
    select(-c(costs_bin, total_costs, set))

  test.Y <- sim |>
    filter(set == "Test") |>
    pull(costs_bin)

  K <- 100

  erates <- rep(0, K)

  for (i in 1:K) {
    knn.pred <- knn(train.X, test.X, train.Y, k = i)
    erates[i] <- mean(knn.pred != test.Y)
  }

  k_optimal <- which.min(erates)

  min_erate <- min(erates)

  var_error <- var(erates)

  max_erate <- max(erates)

  result <- tibble(
    i = seed,
    k = k_optimal,
    error = min_erate,
    var = var_error,
    max_erate
  )

  return(result)
}

compute_min_k(df_train, 1)

results <- map_dfr(1:200, ~ compute_min_k(df_train, .x))

results
```

Plots: 

```{r}
ggplot(results, aes(x = k)) +
  geom_histogram()

ggplot(results, aes(x = k, y = error)) +
  geom_point()
```

