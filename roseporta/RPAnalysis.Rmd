---
title: "Rose Analysis"
author: "Rose Porta"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(class)
library(glmnet)
library(naivebayes)
library(leaps)
library(pls)
```

To-dos 4/22/24:

- pick an interaction and transformation

- implement 10-fold cv for each method

- 10-fold cv for choosing best subset model

- repeat best subsets for categorical outcome

- compile results into a cohesive table: method - cv estimated error - true test error (one table for quant and one for binary)

- write down assumptions for each method

- idea for simulation study

- idea for bootstrap CI

Questions:

- how to interpret best subset for binary response?

- perform best subset first and proceed with only selected predictors?

# Data Cleaning

```{r}
hpc <- read_csv(here::here("hpc.csv"))
#summary(hpc)

hpcdf <- hpc |> 
  mutate(start = as.Date(`Fiscal Year Begin Date`),end = as.Date(`Fiscal Year End Date`)) |> 
  mutate(days = as.numeric(end - start)) |> 
  mutate(numBeds = `Total Bed Days Available`/days,id = row_number())
```

```{r}
hpc_clean <- hpcdf |> 
  janitor::clean_names() |> 
  select(provider_ccn, days, number_of_beds,
         total_costs, rural_versus_urban, provider_type, type_of_control,
         fte_employees_on_payroll, total_days = total_days_v_xviii_xix_unknown,
         total_discharges = total_discharges_v_xviii_xix_unknown,
         total_income, total_assets,
         salaries = total_salaries_from_worksheet_a) |> 
  na.omit() |> 
  mutate(rural = ifelse(rural_versus_urban == "R", 1, 0),
         control_bin = case_when(
           type_of_control < 3 ~ "Voluntary",
           (type_of_control >= 3 & type_of_control < 7) ~ "Proprietary",
           type_of_control >= 7 ~ "Governmental"
         ),
         provider_bin = ifelse(provider_type < 3 |
                                 provider_type == 6,
                               "General", "Specialized"
                               ),
         ) |> 
  select(- c(rural_versus_urban, type_of_control, provider_type))

duplicates <- hpc_clean |> 
  group_by(provider_ccn) |> 
  summarise(count = n()) |> 
  filter(count > 1)

dup <- map_lgl(hpc_clean$provider_ccn, `%in%`, duplicates$provider_ccn)

hpc1 <- hpc_clean |> 
  filter(!dup)

hpc2 <- hpc_clean |> 
  filter(dup) |> 
  group_by(provider_ccn) |> 
  summarise(
    days = sum(days),
    number_of_beds = mean(number_of_beds),
    total_costs = sum(total_costs),
    fte_employees_on_payroll = mean(fte_employees_on_payroll),
    total_days = sum(total_days),
    total_discharges = sum(total_discharges),
    total_income = sum(total_income),
    total_assets = mean(total_assets),
    salaries = sum(salaries),
    rural = max(rural),
    control_bin = max(control_bin),
    provider_bin = max(provider_bin)
  )

hpc_all <- bind_rows(hpc1, hpc2)

hpc_normalize <- hpc_all |> 
  mutate(
    total_costs = total_costs/days,
    inpatients = total_days/days,
    total_discharges = total_discharges/days,
    total_income = total_income/days,
    salaries = salaries/days
    )
```

```{r}
hpc_dummies <- hpc_normalize |> 
  select(-c(provider_ccn, days)) |> 
  mutate(
    costs_bin = ifelse(total_costs > median(total_costs), 1, 0)
    ) |> 
  tidytable::get_dummies(drop_first = TRUE) |> 
  select(where(is.numeric))
```

# Train-test Split

```{r}
set.seed(1)

train_prop <- 0.9

n <- nrow(hpc_dummies)
n_train <- train_prop*n
n_test <- n - n_train

hpc_dummies$set <- "Train"
hpc_dummies$set[sample(n, n_test, replace = FALSE)] <- "Test"
  
df_train <- hpc_dummies |>  filter(set == "Train") |> select(-set)
df_test <- hpc_dummies |> filter(set == "Test") |> select(-set)
```

# For all methods:

-   For each method you apply, use 10 fold cross-validation estimate for the test error.
-   discuss assumptions

# Quantitative Outcome Analyses

Set Up:

```{r}
predictors <- select(df_train, -c(total_costs, costs_bin))
```

Assign Folds for 10-fold CV

```{r}
n_train <- nrow(df_train)

k <- 10
k.o <- k
kvals <- unique(round(n_train/(1L:floor(n_train/2))))
temp <- abs(kvals - k)
if (!any(temp == 0))  { k <- kvals[temp == min(temp)][1L] }
f <- ceiling(n_train/k)

df_train$fold <- sample(rep(1L:k, f), n_train)
```

Remove categorical outcome and fold number from train and test data:

```{r}
train_data <- df_train |> select(-c(costs_bin, fold))

test_data <- df_test |> select(-c(costs_bin))
```


## Marginal simple linear regressions

```{r}
# model summaries
map(predictors, ~summary(lm(total_costs ~ .x, data = df_train)))

simple_models <- map(predictors, ~lm(total_costs ~ .x, data = df_train))
```

10-fold cv

```{r}
# Estimate with k-fold:

predictor_names <- colnames(predictors)

formulas <- str_c("total_costs", " ~ ", predictor_names)


compute_MSEP_lm <- function(train_data, formula, i){
  current_train_folds <- filter(train_data, fold != i)
  current_test_fold <- filter(train_data, fold == i)
    
  mod <- lm(formula, data = current_train_folds)
  pred <- predict(mod, newdata = select( current_test_fold, -c(total_costs, costs_bin) ) )
    
  MSEP <- mean( (current_test_fold$total_costs - pred) ^ 2 )
  return(MSEP)
}

#compute_MSEP_lm(train_data, formula, i)

kfold_cv_lm <- function(train_data, formula, k = 10){
  
  MSEP_vec <- map_dbl(1:k, ~compute_MSEP_lm(train_data, formula, .x))
  avg_MSEP <- mean(MSEP_vec)
  return(avg_MSEP)
  
}

kfold_cv_lm(train_data, formula)

test_error_vec <- map_dbl( formulas, ~kfold_cv_lm(df_train, formula = .x, k = 10) )

test_error_vec
```

True Test Error

```{r}
compute_MSEP_test <- function(formula, train_data, test_data){
  
  mod <- lm(formula, data = train_data)
  pred <- predict(mod, newdata = select( test_data, -c(total_costs, costs_bin) ) )
    
  MSEP <- mean( (test_data$total_costs - pred) ^ 2 )
  return(MSEP)
}

test_MSEP_vec <- map_dbl(formulas, ~compute_MSEP_test(.x, df_train, df_test))
test_MSEP_vec
```


## Multiple linear regression

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r}
mod_mlr <- lm(total_costs ~ ., data = train_data )

summary(mod_mlr)
```

```{r}
formula <- as.formula("total_costs ~ .")

test_error <- compute_MSEP_test(formula, train_data, df_test)

test_error
```



## Regression Tree (with pruning)

## Bagging (with variable importance)

## Random Forest (with variable importance)

## Boosting (including selecting the tuning parameter)

## Neural Network

# Variable Selection

## Best Subset

```{r}
nvmax <- 12
```


```{r}
regfit_best <- regsubsets(total_costs ~ ., data = train_data, nvmax = nvmax)
summary_regfit <- summary(regfit_best)
summary_regfit
```

## Forward Stepwise

```{r}
# Forward stepwise selection -----
regfit_fwd <- regsubsets(total_costs ~ ., data = train_data, nvmax = nvmax, method = "forward")
summary_regfit <- summary(regfit_fwd)
summary_regfit
```


## Backward Stepwise

```{r}
# Backward stepwise selection -----
regfit_bwd <- regsubsets(total_costs ~ ., data = train_data, nvmax = nvmax, method = "backward")
summary_regfit <- summary(regfit_bwd)
summary_regfit
```


(For the above 3 methods, determine which model of a given size is best by comparing the 10-fold cross validation estimate of the test error.) ***

## Ridge regression (find the best tuning parameter using cross-validation)

```{r}
# scale predictors
x_train <- scale(model.matrix(total_costs ~ ., data = train_data)[ , -1])
y_train <- train_data$total_costs

x_test <- scale(model.matrix(total_costs ~ ., data = test_data)[ , -1])
y_test <- test_data$total_costs
```


```{r}
set.seed(1)

lambda_grid <- 10 ^ seq(10, -2, length = 100)
cv_ridge <- 
  cv.glmnet(
    x_train,
    y_train,
    alpha = 0,
    lambda = lambda_grid,
    type.measure = "mse",
    nfolds = 10        # Set the number of folds to n to get LOOCV
    )

cv_ridge$lambda.min

lambda_min <- cv_ridge$lambda.min
```

```{r}
ridge_reg <- glmnet(x_train, y_train, alpha = 0, lambda = cv_ridge$lambda.min)
coef(ridge_reg)
```

```{r}
pred_ridge <- predict(ridge_reg, newx = x_test)

error_ridge <- mean( (test_data$total_costs - pred_ridge) ^ 2 )

error_ridge
```



## Lasso (find the best tuning parameter using cross-validation)

```{r}

set.seed(1)

# fit lasso model
lambda_grid <- 10 ^ seq(10, -2, length = 100)

# 10-fold cross validation to choose tuning parameter
cv_lasso <- 
  cv.glmnet(
    x,
    y,
    alpha = 1,
    lambda = lambda_grid,
    type.measure = "mse",
    nfolds = 10        # Set the number of folds to n to get LOOCV
    )

cv_lasso$lambda.min
```


```{r}
# fit lasso model with optimal tuning parameter
lasso_bestlambda <- glmnet(x, y, alpha = 1, lambda = cv_lasso$lambda.min)
coef(lasso_bestlambda)
```

```{r}
pred_lasso <- predict(lasso_bestlambda, newx = x_test)

error_lasso <- mean( (test_data$total_costs - pred_lasso) ^ 2 )

error_lasso
```


## Principal Components Regression (PCR)

```{r}
scaled_df_train <- bind_cols(total_costs = y_train, x_train)
scaled_df_test <- bind_cols(total_costs = y_test, x_test)
```

```{r}
pcr_fit <- pcr(total_costs ~ ., data = scaled_df_train, validation = "CV", segments = 10)
summary(pcr_fit)
```

```{r}
validationplot(pcr_fit, val.type = "MSEP")
```

```{r}
pred_test <- predict(pcr_fit, scaled_df_test, ncomp = 8)

test_error = mean( (scaled_df_test$total_costs - pred_test)^2 )

test_error
```

# Qualitative Outcome Analyses

## KNN

```{r}
# sample knn code

knn_pred <- 
    knn(
      train = select(df_train, -c(total_costs, costs_bin, fold)),
      test = select(df_test, -c(total_costs, costs_bin)),
      cl = df_train$costs_bin,
      k = k,
      prob = TRUE
    )

test_error <- mean(df_test$costs_bin != knn_pred)

test_error

```

## Multiple logistic regression

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r}
train_data <- df_train |> select(-c(total_costs, fold))
mod_mlogr <- glm(costs_bin ~ ., data = train_data, family = binomial(link = "logit") )

summary(mod_mlogr)
```

```{r}
log_pred <- predict( mod_mlogr, 
                     newdata = select( df_test, -c(costs_bin, total_costs) ), 
                     type = "response" 
                     )
pred_class <- ifelse(log_pred >= 0.5, 1, 0)

test_error <- mean(df_test$costs_bin != pred_class)

test_error
```


## LDA

```{r}

mod_lda <- MASS::lda(costs_bin ~ ., data = train_data)

lda_pred <- predict( mod_lda, newdata = select( df_test, -c(costs_bin, total_costs) ) )$class

test_error <- mean(df_test$costs_bin != lda_pred)

test_error
```

## QDA

```{r}

mod_qda <- MASS::qda(costs_bin ~ ., data = train_data)

qda_pred <- predict( mod_qda, newdata = select( df_test, -c(costs_bin, total_costs) ) )$class

test_error <- mean(df_test$costs_bin != qda_pred)

test_error
```

## Naive Bayes (at least two kernels)

### Gaussian Kernel

```{r}
nb_gaussian <- e1071::naiveBayes(costs_bin ~ ., data = train_data)

nb_pred <- predict(nb_gaussian, newdata = select( df_test, -c(costs_bin, total_costs) ) )

test_error_nb <- mean(df_test$costs_bin != nb_pred)

test_error_nb
```

### Non Parametric Kernel Density Estimation

```{r}
predictor_matrix <- as.matrix(predictors)

test_matrix <- df_test |> 
  select( -c(costs_bin, total_costs) ) |> 
  as.matrix()

nb_KDE <-
  nonparametric_naive_bayes(
    y = as.factor(df_train %>% pull(costs_bin)),
    x = predictor_matrix
    )

nb_kde_pred <- predict(nb_KDE, newdata = test_matrix )

test_error_nb_kde_pred <- mean(df_test$costs_bin != nb_kde_pred)

test_error_nb_kde_pred

```


## Decision Tree (with pruning)

## Bagging (with variable importance)

## Random Forest (with variable importance)

## Boosting (including selecting the tuning parameter)

(For the above three methods, calculate the Gini index on each leaf of the final tree to examine the purity of the node.)

## Neural Network

## Calculate True/False Positive/Negative rates for each method

# Bootstrap CI

# Simulation Study
