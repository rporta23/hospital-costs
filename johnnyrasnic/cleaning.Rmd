---
title: "Data Cleaning"
author: "Johnny Rasnic"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(GGally)
```

```{r echo=FALSE}
hpc = read_csv(here::here("hpc.csv"))
#summary(hpc)

hpcdf = hpc |> 
  mutate(start = as.Date(`Fiscal Year Begin Date`),end = as.Date(`Fiscal Year End Date`)) |> 
  mutate(days = as.numeric(end - start)) |> 
  mutate(numBeds = `Total Bed Days Available`/days,id = row_number())
```

```{r echo=FALSE}
hpc_clean <- hpcdf |>
  janitor::clean_names() |>
  select(provider_ccn, days, number_of_beds,
    total_costs, rural_versus_urban, provider_type, type_of_control,
    fte_employees_on_payroll,
    total_days = total_days_v_xviii_xix_unknown,
    total_discharges = total_discharges_v_xviii_xix_unknown,
    total_income, total_assets,
    salaries = total_salaries_from_worksheet_a
  ) |>
  na.omit() |>
  mutate(
    rural = ifelse(rural_versus_urban == "R", 1, 0),
    control_bin = case_when(
      type_of_control < 3 ~ "Voluntary",
      (type_of_control >= 3 & type_of_control < 7) ~ "Proprietary",
      type_of_control >= 7 ~ "Governmental"
    ),
    provider_bin = ifelse(provider_type < 3 |
      provider_type == 6,
    "General", "Specialized"
    ),
  ) |>
  select(-c(rural_versus_urban, type_of_control, provider_type))

duplicates <- hpc_clean |>
  group_by(provider_ccn) |>
  summarise(count = n()) |>
  filter(count > 1)

dup <- map_lgl(hpc_clean$provider_ccn, `%in%`, duplicates$provider_ccn)

hpc_clean$duplicate <- as.numeric(dup)

hpc_normalize <- hpc_clean |>
  mutate(
    total_costs = total_costs / days,
    inpatients = total_days / days,
    total_discharges = total_discharges / days,
    total_income = total_income / days,
    salaries = salaries / days
  )
```

```{r echo=FALSE}
hpc_dummies = hpc_normalize |> 
  select(-c(provider_ccn, days)) |> 
  mutate(
    costs_bin = ifelse(total_costs > median(total_costs), 1, 0)
    ) |> 
  tidytable::get_dummies(drop_first = TRUE) |> 
  select(where(is.numeric))
```

Codes for type of Control:

1 = Voluntary Non‐Profit‐Church

2 = Voluntary Non‐Profit‐Other

3 = Proprietary‐Individual

4 = Proprietary‐Corporation

5 = Proprietary‐Partnership

6 = Proprietary‐Other

7 = Governmental‐Federal

8 = Governmental‐City‐County

9 = Governmental‐County

10 = Governmental‐State

11 = Governmental‐Hospital District

12 = Governmental‐City

13 = Governmental‐Other

Codes for Provider Type:

1 = General Short Term (includes CAHs)

2 = General Long Term

3 = Cancer

4 = Psychiatric

5 = Rehabilitation

6 = Religious Non‐Medical Health Care Institution

7 = Children

8 = Reserved for Future Use

9 = Other

10 = Extended Neoplastic Disease Care

11 = Indian Health Services

12 = Rural Emergency Hospital.

# Exploratory Analysis

```{r echo=FALSE}
ggplot(hpc_clean, aes(x = total_costs)) +
  geom_histogram() +
  geom_vline(xintercept = median(hpc_clean$total_costs), color = "red") + 
  geom_vline(xintercept = mean(hpc_clean$total_costs), color = "blue")

ggplot(hpc_clean, aes(x = log(total_costs))) +
  geom_histogram() +
  geom_vline(xintercept = median(log(hpc_clean$total_costs)), color = "red") + 
  geom_vline(xintercept = mean(log(hpc_clean$total_costs)), color = "blue")
```

Distribution of response total costs is extremely right-skewed.

Red line = median, blue line = mean

Summary Statistics:

```{r echo=FALSE}
summary(hpc_clean)
```

Pairs plot:

```{r echo=FALSE}
# # code to generate and save 4K resolution pairwise plot
# png("pairs.png", width=3840, height=2160)
# ggpairs(hpc_dummies)
# dev.off()

# Try pairs plot with log of total costs
data_pairs = hpc_dummies |>
  select(-c(rural, costs_bin, control_bin_Governmental,
            control_bin_Voluntary, provider_bin_Specialized)
         ) |>
  mutate(total_costs = log(total_costs))

ggpairs(data_pairs, progress = FALSE)
```

-   very high pairwise positive collinearity between:
    -   total_days and bed_days, and total_discharges
    -   salaries and employees on payroll
-   total_costs appears to have fairly strong positive linear relationships with:
    -   fte_employees_on_payroll
    -   total_days
    -   bed_days
    -   total_discharges
    -   salaries
-   total_costs appears to have weak or no relationship with:
    -   total_income
    -   total_assets
-   hard to discern any relationships for the categorical predictors
-   for binary response, the predictors which have positive relationships with total cost have a pattern such that response = 0 corresponds to a higher concentration of points with low values of the predictor response = 1 corresponds to a wider range of values for the predictor. This same pattern shows up also for total assets and total income, which did not look associated with continuous total cost.

# Train-test Split

```{r echo=FALSE}
set.seed(1)

train_prop = 0.9

n = nrow(hpc_dummies)
n_train = train_prop*n
n_test = n - n_train

hpc_dummies$set = "Train"
hpc_dummies$set[sample(n, n_test, replace = FALSE)] = "Test"
```

# 10-fold split

```{r echo=FALSE}
set.seed(1)
folds = floor(seq(1,11, length.out=nrow(hpc_dummies)+1))[1:nrow(hpc_dummies)]
folds = sample(folds, length(folds))

hpc_dummies$fold = folds
```

# Normalize quantitative variables

```{r echo=FALSE}
hpc_quant = hpc_dummies |> select(-c(duplicate, provider_bin_Specialized, control_bin_Governmental, control_bin_Voluntary, costs_bin, rural, set, fold))

hpc_qual = hpc_dummies |> select(c(duplicate, provider_bin_Specialized, control_bin_Governmental, control_bin_Voluntary, costs_bin, rural, set, fold))

hpc_scaled = as_tibble(scale(hpc_quant) |> cbind(hpc_qual))

df_train = hpc_scaled |>  filter(set == "Train") |> select(-c(set, fold))
df_test = hpc_scaled |> filter(set == "Test") |> select(-c(set, fold))
```

# For all methods:

-   For each method you apply, use 10 fold cross-validation estimate for the test error.
-   discuss assumptions

# Quantitative Outcome Analyses

## Marginal simple linear regressions

```{r echo=FALSE}
predictors = select(df_train, -c(total_costs, costs_bin))

# model summaries
map(predictors, ~summary(lm(total_costs ~ .x, data = df_train)))

simple_models = map(predictors, ~lm(total_costs ~ .x, data = df_train))
```

10-fold cv

```{r echo=FALSE}

coefficients_df = hpc_scaled |> select(-c(total_costs, costs_bin, set, fold)) |> mutate(across(everything(), as.numeric))
MSE_df = coefficients_df[1:10,]

for (i in 1:10) {
  df_train_i = hpc_scaled |> filter(fold == i) |> select(-c(fold, set))
  predictors = select(df_train_i, -c(total_costs, costs_bin))
  model = map(predictors, ~lm(total_costs ~ .x, data = df_train_i))
  for (j in colnames(predictors)) {
    MSE_df[i,j] = mean((predict(model[[j]]) - df_train_i$total_costs)^2)
  }
}

ten_fold_mean_MSE = MSE_df |> summarize(across(everything(), mean))

ten_fold_mean_MSE
```

Above is MSE averaged over ten fold cross validation for each marginal linear regression on each predictor.

## Multiple linear regression

```{r echo=FALSE}
train.X = df_train |> select(-c("costs_bin"))
test.X = df_test |> select(-c("costs_bin"))

lin_model = lm(total_costs ~ ., data=train.X)

summary(lin_model)

pred = predict(lin_model, newdata = test.X)

mean((pred - test.X$total_costs)^2)

erates = rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> 
    select(-c("costs_bin", "fold", "set"))
  
  test.X = hpc_scaled |> filter(fold == i) |> 
    select(-c("costs_bin", "fold", "set"))
  
  lin_model = lm(total_costs ~ ., data=train.X)
  
  pred = predict(lin_model, newdata = test.X)
  
  erates[i] = mean((pred - test.X$total_costs)^2)
}

erates
```

-   Add polynomial terms or transformations of some of the predictors

    ```{r echo=FALSE}
    train.X = df_train |> select(-c("costs_bin")) |> 
                          mutate(days2 = total_days^2, salaries2 = salaries^2)
    test.X = df_test |> select(-c("costs_bin")) |> 
                        mutate(days2 = total_days^2, salaries2 = salaries^2)

    lin_model = lm(total_costs ~ ., data=train.X)

    summary(lin_model)

    pred = predict(lin_model, newdata = test.X)

    mean((pred - test.X$total_costs)^2)

    erates = rep(0,10)

    for (i in 1:10) {
      train.X = hpc_scaled |> filter(fold != i) |> 
        select(-c("costs_bin", "fold", "set")) |> 
        mutate(days2 = total_days^2, salaries2 = salaries^2)
      
      test.X = hpc_scaled |> filter(fold == i) |> 
        select(-c("costs_bin", "fold", "set")) |> 
        mutate(days2 = total_days^2, salaries2 = salaries^2)
      
      lin_model = lm(total_costs ~ ., data=train.X)
      
      pred = predict(lin_model, newdata = test.X)
      
      erates[i] = mean((pred - test.X$total_costs)^2)
    }

    erates
    ```

-   Add at least two interaction terms that make sense to you

    ```{r echo=FALSE}
    train.X = df_train |> select(-c("costs_bin")) |> 
                          mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll)
    test.X = df_test |> select(-c("costs_bin")) |> 
                        mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll)

    lin_model = lm(total_costs ~ ., data=train.X)

    summary(lin_model)

    pred = predict(lin_model, newdata = test.X)

    mean((pred - test.X$total_costs)^2)

    erates = rep(0,10)

    for (i in 1:10) {
      train.X = hpc_scaled |> filter(fold != i) |> 
        select(-c("costs_bin", "fold", "set")) |> 
        mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll)
      
      test.X = hpc_scaled |> filter(fold == i) |> 
        select(-c("costs_bin", "fold", "set")) |> 
        mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll)
      
      lin_model = lm(total_costs ~ ., data=train.X)
      
      pred = predict(lin_model, newdata = test.X)
      
      erates[i] = mean((pred - test.X$total_costs)^2)
    }

    erates
    ```

Both transformations and interaction terms present

```{r echo=FALSE}
train.X = df_train |> select(-c("costs_bin")) |> 
                      mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll, days2 = total_days^2, salaries2 = salaries^2)
test.X = df_test |> select(-c("costs_bin")) |> 
                    mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll, days2 = total_days^2, salaries2 = salaries^2)

lin_model = lm(total_costs ~ ., data=train.X)

summary(lin_model)

pred = predict(lin_model, newdata = test.X)

mean((pred - test.X$total_costs)^2)

erates = rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> 
    select(-c("costs_bin", "fold", "set")) |> 
    mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll, days2 = total_days^2, salaries2 = salaries^2)
  
  test.X = hpc_scaled |> filter(fold == i) |> 
    select(-c("costs_bin", "fold", "set")) |> 
    mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll, days2 = total_days^2, salaries2 = salaries^2)
  
  lin_model = lm(total_costs ~ ., data=train.X)
  
  pred = predict(lin_model, newdata = test.X)
  
  erates[i] = mean((pred - test.X$total_costs)^2)
}

erates
```

## Regression Tree (with pruning)

```{r echo=FALSE}
library(tree)
set.seed(1)

train.X = df_train|> select(-c(costs_bin))
test.X = df_test |> select(-c(costs_bin))

tree_mod = tree(total_costs ~ ., data=train.X, control = tree.control(1866496, mincut = 1000),
                split = "gini")

cv_tree = cv.tree(tree_mod)

k = cv.tree(tree_mod)$size[which(cv_tree$dev == min(cv_tree$dev))][1]

print(k)

tree_pruned <- prune.tree(tree_mod, best = k)

pred <- predict(tree_pruned, newdata = test.X)

test_error_classtree <- mean( (test.X$total_costs - pred )^2)

test_error_classtree

erates = rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> mutate(costs_bin = as.factor(costs_bin)) |> filter(fold != i) |> select(-c(fold,set,costs_bin))
  test.X = hpc_scaled |> mutate(costs_bin = as.factor(costs_bin)) |> filter(fold == i) |> select(-c(fold,set,costs_bin))
  
  tree_mod = tree(total_costs ~ ., data=train.X, control = tree.control(1866496, mincut = 1000),
                split = "gini")
  
  tree_pruned <- prune.tree(tree_mod, best = k)
  
  pred <- predict(tree_pruned, newdata = test.X)
  
  erates[i] =  mean( (test.X$total_costs - pred )^2)
}

print(paste("Average MSE rate for 10 fold CV:", mean(erates)))
```

## Bagging (with variable importance)

```{r echo=FALSE}
library(randomForest)
set.seed(1)

train.X = df_train |> select(-costs_bin)
test.X = df_test |> select(-costs_bin)

bag.total_costs = randomForest(total_costs ~ ., data = train.X, mtry = length(train.X)-1)

pred = predict(bag.total_costs, newdata=test.X)

mean((pred - test.X$total_costs)^2)

importance(bag.total_costs)

erates=rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-costs_bin)
  test.X = hpc_scaled |> filter(fold == i) |> select(-costs_bin)

  bag.total_costs = randomForest(total_costs ~ ., data = train.X, mtry = length(train.X)-1)
  
  pred = predict(bag.total_costs, newdata=test.X)
  
  erates[i] = mean((pred - test.X$total_costs)^2)
}

erates

mean(erates)
```

## Random Forest (with variable importance)

```{r echo=FALSE}
library(randomForest)
set.seed(1)

train.X = df_train |> select(-costs_bin)
test.X = df_test |> select(-costs_bin)

bag.total_costs = randomForest(total_costs ~ ., data = train.X)

pred = predict(bag.total_costs, newdata=test.X)

mean((pred - test.X$total_costs)^2)

importance(bag.total_costs)

erates=rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-costs_bin)
  test.X = hpc_scaled |> filter(fold == i) |> select(-costs_bin)

  bag.total_costs = randomForest(total_costs ~ ., data = train.X)
  
  pred = predict(bag.total_costs, newdata=test.X)
  
  erates[i] = mean((pred - test.X$total_costs)^2)
}

erates

mean(erates)
```

## Boosting (including selecting the tuning parameter)

```{r echo=FALSE}
library(gbm)
set.seed(1)

train.X = df_train |> select(-costs_bin)
test.X = df_test |> select(-costs_bin)

boost.total_costs = gbm(total_costs ~ ., data = train.X, distribution = "gaussian", n.trees = 5000, interaction.depth = 4)

summary(boost.total_costs)

pred = predict(boost.total_costs, newdata=test.X)
```

## Neural Network

```{r echo=FALSE}
library(keras)
train = df_train %>%
  select(-total_costs)

test = df_test %>%
  select(-total_costs)


# Neural Network Model 
modelnn <- keras_model_sequential()
modelnn %>%
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = "softmax")

modelnn %>% 
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(learning_rate = 0.01), 
    metrics = "categorical_accuracy"
    )
```

# Variable Selection

## Best Subset

```{r, echo=FALSE}
library(leaps)

train.X = hpc_scaled |> filter(set == "Train") |> select(-c(fold,set,costs_bin))
test.X = hpc_scaled |> filter(set == "Test") |> select(-c(fold,set,costs_bin))

erate = rep(0, 13)

best_subset = regsubsets(total_costs ~ ., data = train.X, nvmax = ncol(train.X))

summ = summary(best_subset)

for (j in 1:ncol(train.X)-1) {
    selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][j,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }
    temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    erate[j] = mean((pred - test.X$total_costs)^2)
}

plot(best_subset, scale="r2")
plot(best_subset, scale="adjr2")
plot(best_subset, scale="Cp")
plot(best_subset, scale="bic")

erates = matrix(rep(0, 10*13), nrow = 10, ncol = 13)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-c(costs_bin, set, fold))
  test.X = hpc_scaled |> filter(fold == i) |> select(-c(costs_bin, set, fold))
  best_subset = regsubsets(total_costs ~ ., data = train.X, nvmax = ncol(train.X))
  summ = summary(best_subset)
  for (j in 1:ncol(train.X)-1) {
    selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][j,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }
    temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    erates[i,j] = mean((pred - test.X$total_costs)^2)
  }
}

erates = as_tibble(erates) |> summarize(across(V1:V13,mean))

plot(1:13, erates[1,], 
     col = ifelse(1:13 == which.min(erates), "red", "black"),
     xlab = "Model Size",
     ylab = "Average Test MSE")

title("Best Subset Selection")

data = hpc_scaled |> select(-c(fold,set,costs_bin))

best_subset = regsubsets(total_costs ~ ., data = data)

coeffs = as.matrix(coef(best_subset, 8))
colnames(coeffs) = c("Coefficient Estimate")

selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][8,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }

temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    cv_erate = mean((pred - test.X$total_costs)^2)

```

## Forward Stepwise

```{r, echo=FALSE}
library(leaps)

train.X = hpc_scaled |> filter(set == "Train") |> select(-c(fold,set,costs_bin))
test.X = hpc_scaled |> filter(set == "Test") |> select(-c(fold,set,costs_bin))

erate = rep(0, 13)

best_subset = regsubsets(total_costs ~ ., data = train.X, nvmax = ncol(train.X), method = "forward")

summ = summary(best_subset)

for (j in 1:ncol(train.X)-1) {
    selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][j,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }
    temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    erate[j] = mean((pred - test.X$total_costs)^2)
}

plot(best_subset, scale="r2")
plot(best_subset, scale="adjr2")
plot(best_subset, scale="Cp")
plot(best_subset, scale="bic")

erates = matrix(rep(0, 10*13), nrow = 10, ncol = 13)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-c(costs_bin, set, fold))
  test.X = hpc_scaled |> filter(fold == i) |> select(-c(costs_bin, set, fold))
  best_subset = regsubsets(total_costs ~ ., data = train.X, nvmax = ncol(train.X), method="forward")
  summ = summary(best_subset)
  for (j in 1:ncol(train.X)-1) {
    selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][j,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }
    temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    erates[i,j] = mean((pred - test.X$total_costs)^2)
  }
}

erates = as_tibble(erates) |> summarize(across(V1:V13,mean))

plot(1:13, erates[1,], 
     col = ifelse(1:13 == which.min(erates), "red", "black"),
     xlab = "Model Size",
     ylab = "Average Test MSE")

title("Forward Stepwise")

data = hpc_scaled |> select(-c(fold,set,costs_bin))

best_subset = regsubsets(total_costs ~ ., data = data)

coeffs = as.matrix(coef(best_subset, 8))
colnames(coeffs) = c("Coefficient Estimate")

selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][8,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }

temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    cv_erate = mean((pred - test.X$total_costs)^2)

```

## Backward Stepwise

```{r, echo=FALSE}
library(leaps)

train.X = hpc_scaled |> filter(set == "Train") |> select(-c(fold,set,costs_bin))
test.X = hpc_scaled |> filter(set == "Test") |> select(-c(fold,set,costs_bin))

erate = rep(0, 13)

best_subset = regsubsets(total_costs ~ ., data = train.X, nvmax = ncol(train.X), method="backward")

summ = summary(best_subset)

for (j in 1:ncol(train.X)-1) {
    selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][j,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }
    temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    erate[j] = mean((pred - test.X$total_costs)^2)
}

plot(best_subset, scale="r2")
plot(best_subset, scale="adjr2")
plot(best_subset, scale="Cp")
plot(best_subset, scale="bic")

erates = matrix(rep(0, 10*13), nrow = 10, ncol = 13)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-c(costs_bin, set, fold))
  test.X = hpc_scaled |> filter(fold == i) |> select(-c(costs_bin, set, fold))
  best_subset = regsubsets(total_costs ~ ., data = train.X, nvmax = ncol(train.X), method="backward")
  summ = summary(best_subset)
  for (j in 1:ncol(train.X)-1) {
    selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][j,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }
    temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    erates[i,j] = mean((pred - test.X$total_costs)^2)
  }
}

erates = as_tibble(erates) |> summarize(across(V1:V13,mean))

plot(1:13, erates[1,], 
     col = ifelse(1:13 == which.min(erates), "red", "black"),
     xlab = "Model Size",
     ylab = "Average Test MSE")

title("Backward Stepwise")

data = hpc_scaled |> select(-c(fold,set,costs_bin))

best_subset = regsubsets(total_costs ~ ., data = data)

coeffs = as.matrix(coef(best_subset, 8))
colnames(coeffs) = c("Coefficient Estimate")

selected_vars = c("total_costs")
    for (k in 1:ncol(train.X)-1) {
      cond = FALSE
      if (isTRUE(summ$which[,-1][8,k])) {cond = TRUE}
      if (cond == TRUE) {selected_vars = append(selected_vars, colnames(summ$which)[-1][k])}
    }

temp.X = train.X |> select(all_of(selected_vars))
    lin_model = lm(total_costs ~ ., data=temp.X)
    pred = predict(lin_model, newdata = test.X)
    cv_erate = mean((pred - test.X$total_costs)^2)

```

With best subset, forward stepwise, and backward stepwise selection, all choose a model size of 8 as the model with the lowest average test MSE across 10 fold cross validation.

## Ridge regression (find the best tuning parameter using cross-validation)

### Quantitative

```{r echo=FALSE}
library(glmnet)

set.seed(1)

train.X = hpc_scaled |> select(-c("costs_bin", "set", "fold"))

x = scale(model.matrix(total_costs ~ ., data = train.X)[ , -1])

lambda_grid = 10 ^ seq(10, -2, length = 100)
ridge_reg = cv.glmnet(x, train.X$total_costs, alpha = 0, type.measure = "mse", lambda = lambda_grid, nfolds=10)
lambda = ridge_reg$lambda.min

ridge_model = glmnet(x, train.X$total_costs,alpha = 0, lambda = lambda, nfolds=10)

coeffs = coef(ridge_model)

colnames(coeffs) = c("Coefficient Estimate")

coeffs

pred = predict(ridge_model, newx=x)

erate= mean((pred-train.X$total_costs)^2)

```

### Qualitative

```{r echo=FALSE}
library(glmnet)

set.seed(1)

train.X = hpc_scaled |> select(-c("total_costs", "set", "fold"))
x = scale(model.matrix(costs_bin ~ ., data = train.X)[ , -1])

lambda_grid = 10 ^ seq(10, -2, length = 100)
ridge_reg = cv.glmnet(x, hpc_scaled$costs_bin, alpha = 0, family="binomial", lambda = lambda_grid, nfolds=10)
lambda = ridge_reg$lambda.min

ridge_model = glmnet(x, hpc_scaled$costs_bin,alpha = 0, lambda = lambda, nfolds=10)

coeffs = coef(ridge_model)

colnames(coeffs) = c("Coefficient Estimate")

coeffs

pred = round(predict(ridge_model, newx=x, type = "response"))

erate= mean((pred != train.X$costs_bin))
```

## Lasso (find the best tuning parameter using cross-validation)

### Quantitative

Below we have the Lasso selection results for predicting total costs:

```{r echo=FALSE}
library(glmnet)

set.seed(1)

train.X = hpc_scaled |> select(-c("costs_bin", "set", "fold"))
x = scale(model.matrix(total_costs ~ ., data = train.X)[ , -1])

lambda_grid = 10 ^ seq(10, -2, length = 100)
lasso_reg = cv.glmnet(x, hpc_scaled$total_costs, alpha = 1, type.measure = "mse", lambda = lambda_grid, nfolds=10)

lambda = lasso_reg$lambda.min

lasso_model = glmnet(x, hpc_scaled$total_costs, alpha = 1, lambda = lambda, nfolds=10)

coeffs = coef(lasso_model)

colnames(coeffs) = c("Coefficient Estimate")

coeffs

pred = predict(ridge_model, newx=x)

erate= mean((pred-train.X$total_costs)^2)
```

### Qualitative

```{r echo=FALSE}
library(glmnet)

set.seed(1)

train.X = hpc_scaled |> select(-c("total_costs", "set", "fold"))
x = scale(model.matrix(costs_bin ~ ., data = train.X)[ , -1])

lambda_grid = 10 ^ seq(10, -2, length = 100)
lasso_reg = cv.glmnet(x, hpc_scaled$costs_bin, alpha = 1, family="binomial", lambda = lambda_grid, nfolds=10)

lambda = lasso_reg$lambda.min

lasso_model = glmnet(x, hpc_scaled$costs_bin, alpha = 1, lambda = lambda, nfolds=10)

coeffs = coef(lasso_model)

colnames(coeffs) = c("Coefficient Estimate")

coeffs

pred = round(predict(ridge_model, newx=x, type = "response"))

erate= mean((pred != train.X$costs_bin))
```

## Principal Components Regression (PCR)

```{r echo=FALSE}
library(pls)

train.X = df_train
test.X = df_test

pcr_model = pcr(total_costs ~ ., data = train.X)

pred = predict(pcr_model, newdata = test.X)

print(paste("Test MSE for train-test split:", mean((pred - test.X$total_costs)^2)))


erates = rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-c(set,fold,costs_bin))
  test.X = hpc_scaled |> filter(fold == i) |> select(-c(set,fold,costs_bin))

  pcr_model = pcr(total_costs ~ ., data = train.X)

  pred = predict(pcr_model, newdata = test.X)

  erates[i] = mean((pred - test.X$total_costs)^2)
}


print(paste("Average Test MSE across 10-folds:", mean(erates)))
```

# Qualitative Outcome Analyses

## KNN

```{r echo=FALSE}
library(class)
set.seed(1)

train.X = df_train |> select(-c(costs_bin,total_costs))
test.X = df_test |> select(-c(costs_bin, total_costs))

K = 50

false_neg = rep(0,K)

false_pos = rep(0,K)

for (i in 1:K) {
  knn.pred = knn(train.X, test.X, df_train$costs_bin, k = i)
  false_neg[i] = mean((knn.pred == 0) & (df_test$costs_bin == 1))
  false_pos[i] = mean((knn.pred == 1) & (df_test$costs_bin == 0))

}

erate = false_neg + false_pos

print(paste("False positive rate for best K (K = ", which.min(erate), "): ", false_pos[which.min(erate)], sep=""))
print(paste("False negative rate for best K (K = ", which.min(erate), "): ", false_neg[which.min(erate)], sep=""))

false_neg = matrix(rep(0, 10*K), nrow = 10, ncol =K)

false_pos = matrix(rep(0, 10*K), nrow = 10, ncol =K)

for (i in 1:10) {
  for (j in 1:K) {
    train.X = filter(hpc_scaled, fold != i) |> select(-c(fold, set,total_costs,costs_bin))
    test.X = filter(hpc_scaled, fold == i) |> select(-c(fold, set,total_costs,costs_bin))
    class = filter(hpc_scaled, fold != i)$costs_bin
    
    knn.pred = knn(train.X, test.X, class, k = j)
    test.X = filter(hpc_scaled, fold == i) |> select(-c(fold, set,total_costs))
    false_neg[[i,j]] = mean((knn.pred == 0) & (df_test$costs_bin == 1))[1]
    false_pos[[i,j]] = mean((knn.pred == 1) & (df_test$costs_bin == 0))[1]
  }
}

summarized_k_false_neg = as_tibble(false_neg) |> summarize(across(everything(), mean))
summarized_k_false_pos = as_tibble(false_pos) |> summarize(across(everything(), mean))

erates = false_neg + false_pos

summarized_k= as_tibble(erates) |> summarize(across(everything(), mean))

print(paste("False positive rate for best K (K = ", which.min(), "): ", false_pos[which.min(erate)], sep=""))
print(paste("False negative rate for best K (K = ", which.min(erate), "): ", false_neg[which.min(erate)], sep=""))
```

## Multiple logistic regression

-   Add polynomial terms or transformations of some of the predictors
-   Add at least two interaction terms that make sense to you

```{r echo=FALSE}
suppressWarnings({

train.X = df_train |> select(-c("total_costs"))
test.X = df_test |> select(-c("total_costs"))

logist_model = glm(costs_bin ~ ., data=train.X, family="binomial")

pred = round(predict(logist_model, newdata = test.X, type="response"))

erate = mean(pred != test.X$costs_bin)

erate

})

summary(logist_model)

suppressWarnings({

erates = rep(0, 10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-c("total_costs", "set", "fold"))
  test.X = hpc_scaled |> filter(fold == i) |> select(-c("total_costs", "set", "fold"))

  logist_model = glm(costs_bin ~ ., data=train.X, family="binomial")
  
  pred = round(predict(logist_model, newdata = test.X, type="response"))
  
  erates[i] = mean(pred != test.X$costs_bin)
}

erates
})

erates

mean(erates)
```

```{r echo=FALSE}
suppressWarnings({

train.X = df_train |> select(-c("total_costs")) |> 
                      mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll)

test.X = df_test |> select(-c("total_costs")) |> mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll)

logist_model = glm(costs_bin ~ ., data=train.X, family="binomial")

pred = round(predict(logist_model, newdata = test.X, type="response"))

erate = mean(pred != test.X$costs_bin)

erate

})

suppressWarnings({

erates = rep(0, 10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-c("total_costs", "set", "fold")) |> mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll)
  
  test.X = hpc_scaled |> filter(fold == i) |> select(-c("total_costs", "set", "fold")) |> mutate(employees_x_total_income = fte_employees_on_payroll * total_income, specialized_employees_interaction = provider_bin_Specialized * fte_employees_on_payroll)

  logist_model = glm(costs_bin ~ ., data=train.X, family="binomial")
  
  pred = round(predict(logist_model, newdata = test.X, type="response"))
  
  erates[i] = mean(pred != test.X$costs_bin)
}

erates

})


erates

mean(erates)
```

```{r echo=FALSE}

suppressWarnings({

train.X = df_train |> select(-c("total_costs")) |> 
                      mutate(days2 = total_days^2, salaries2 = salaries^2)
test.X = df_test |> select(-c("total_costs")) |> 
                    mutate(days2 = total_days^2, salaries2 = salaries^2)

logist_model = glm(costs_bin ~ ., data=train.X, family="binomial")

pred = round(predict(logist_model, newdata = test.X, type="response"))

erate = mean(pred != test.X$costs_bin)

erate

})

suppressWarnings({

erates = rep(0, 10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> 
                      select(-c("total_costs", "set", "fold")) |> 
                      mutate(days2 = total_days^2, salaries2 = salaries^2)
  test.X = hpc_scaled |> filter(fold == i) |> 
                      select(-c("total_costs", "set", "fold")) |> 
                    mutate(days2 = total_days^2, salaries2 = salaries^2)

  logist_model = glm(costs_bin ~ ., data=train.X, family="binomial")
  
  pred = round(predict(logist_model, newdata = test.X, type="response"))
  
  erates[i] = mean(pred != test.X$costs_bin)
}

erates

})

erates

mean(erates)
```

## LDA

```{r echo=FALSE}
# sample code for LDA

lda_costs = MASS::lda(costs_bin ~ ., data = df_train)

lda_costs_pred = predict(lda_costs, newdata = select(df_test, -costs_bin))$class

test_error = mean(df_test$costs_bin != lda_costs_pred)

test_error

erates = rep(0, 10)

for (i in 1:10) {
    train.X = select(filter(hpc_dummies, fold != i), -c("fold", "set"))
    test.X = select(filter(hpc_dummies, fold == i), -c("fold", "set"))
    
    lda_costs = MASS::lda(costs_bin ~ ., data = train.X)

    lda_costs_pred = predict(lda_costs, newdata = select(test.X, -c("costs_bin")))$class
    
    erates[i] = mean(lda_costs_pred != test.X$costs_bin)
}

erates

mean(erates)
```

## QDA

```{r echo=FALSE}

qda_costs = MASS::qda(costs_bin ~ ., data = df_train)

qda_costs_pred = predict(qda_costs, newdata = select(df_test, -costs_bin))$class

test_error = mean(df_test$costs_bin != qda_costs_pred)

test_error

erates = rep(0, 10)

for (i in 1:10) {
    train.X = select(filter(hpc_dummies, fold != i), -c("total_costs","fold", "set"))
    test.X = select(filter(hpc_dummies, fold == i), -c("total_costs","fold", "set"))
    
    qda_costs = MASS::qda(costs_bin ~ ., data = train.X)

    qda_costs_pred = predict(qda_costs, newdata = select(test.X, -c("costs_bin")))$class
    
    erates[i] = mean(qda_costs_pred != test.X$costs_bin)
}

erates

mean(erates)
```

## Naive Bayes (at least two kernels)

```{r echo=FALSE}
library(e1071)
library(naivebayes)

# sample code for naive Bayes gaussian
nb_gaussian = e1071::naiveBayes(costs_bin ~ ., data = df_train)

nb_pred = predict(nb_gaussian, newdata = df_test)

test_error_nb = mean(df_test$costs_bin != nb_pred)

test_error_nb
```

```{r echo=FALSE}
erates = rep(0, 10)

for (i in 1:10) {
  train.X = hpc_dummies |> filter(fold != i)
  test.X = hpc_dummies |> filter(fold == i)
  
  nb_gaussian = e1071::naiveBayes(costs_bin ~ ., data = train.X)

  nb_pred = predict(nb_gaussian, newdata = test.X)
  
  erates[i] = mean(test.X$costs_bin != nb_pred)
}

erates
```

```{r echo=FALSE}
predictors = select(df_train, -c(total_costs, costs_bin))

predictor_matrix = as.matrix(predictors)

test_matrix = df_test |> select( -c(costs_bin, total_costs) ) |> as.matrix()

nb_KDE =
  nonparametric_naive_bayes(
    y = as.factor(df_train %>% pull(costs_bin)),
    x = predictor_matrix
    )

nb_kde_pred = predict(nb_KDE, newdata = test_matrix )

test_error_nb_kde_pred = mean(df_test$costs_bin != nb_kde_pred)

test_error_nb_kde_pred
```

```{r echo=FALSE}

erates = rep(0, 10)

for (i in 1:10) {
  train.X = hpc_dummies |> filter(fold != i)
  test.X = hpc_dummies |> filter(fold == i)
  
  predictors = train.X |> select(-c(total_costs, costs_bin, set))
  
  predictor_matrix = as.matrix(predictors)
  
  test_matrix = test.X |> select(-c(total_costs, costs_bin, set)) |> as.matrix()
  
  nb_KDE =
    nonparametric_naive_bayes(
      y = as.factor(train.X %>% pull(costs_bin)),
      x = predictor_matrix
      )
  
  nb_kde_pred = predict(nb_KDE, newdata = test_matrix)
  
  erates[i] = mean(hpc_dummies[fold == i]$costs_bin != nb_kde_pred)
}

erates

mean(erates)
```

## Decision Tree (with pruning)

```{r echo=FALSE}
library(tree)
set.seed(1)

train.X = df_train |> mutate(costs_bin = as.factor(costs_bin)) |> select(-c(total_costs))
test.X = df_test |> mutate(costs_bin = as.factor(costs_bin)) |> select(-c(total_costs))

tree_mod = tree(costs_bin ~ ., data=train.X, control = tree.control(1866496, mincut = 1000),
                split = "gini")

cv_tree = cv.tree(tree_mod)

k = cv.tree(tree_mod)$size[which(cv_tree$dev == min(cv_tree$dev))][1]

print(k)

tree_pruned <- prune.misclass(tree_mod, best = k)

pred <- predict(tree_pruned, newdata = test.X, type = "class")

test_error_classtree <- mean( test.X$costs_bin != pred )

test_error_classtree

table(pred, test.X$costs_bin)

erates = rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> mutate(costs_bin = as.factor(costs_bin)) |> filter(fold != i) |> select(-c(fold,set,total_costs))
  test.X = hpc_scaled |> mutate(costs_bin = as.factor(costs_bin)) |> filter(fold == i) |> select(-c(fold,set,total_costs))
  
  tree_mod = tree(costs_bin ~ ., data=train.X, control = tree.control(1866496, mincut = 1000),
                split = "gini")
  
  tree_pruned <- prune.misclass(tree_mod, best = k)
  
  pred <- predict(tree_pruned, newdata = test.X, type = "class")
  
  erates[i] =  mean( test.X$costs_bin != pred )
}

erates

mean(erates)
```

## Bagging (with variable importance)

```{r echo=FALSE}
library(randomForest)
set.seed(1)

train.X = df_train |> select(-total_costs)
test.X = df_test |> select(-total_costs)

bag.costs_bin = randomForest(costs_bin ~ ., data = train.X, mtry = length(train.X)-1)

pred = predict(bag.costs_bin, newdata=test.X, type="class")

mean(pred != test.X$costs_bin)

importance(bag.costs_bin)

erates=rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-total_costs)
  test.X = hpc_scaled |> filter(fold == i) |> select(-total_costs)

  bag.costs_bin = randomForest(costs_bin ~ ., data = train.X, mtry = length(train.X)-1)
  
  pred = predict(bag.costs_bin, newdata=test.X, type="class")
  
  erates[i] = mean(pred != test.X$costs_bin)
}

erates

mean(erates)
```

## Random Forest (with variable importance)

```{r echo=FALSE}
library(randomForest)
set.seed(1)

train.X = df_train |> select(-total_costs)
test.X = df_test |> select(-total_costs)

bag.costs_bin = randomForest(costs_bin ~ ., data = train.X)

pred = predict(bag.costs_bin, newdata=test.X, type="class")

mean(pred != test.X$costs_bin)

importance(bag.costs_bin)

erates=rep(0,10)

for (i in 1:10) {
  train.X = hpc_scaled |> filter(fold != i) |> select(-total_costs)
  test.X = hpc_scaled |> filter(fold == i) |> select(-total_costs)

  bag.costs_bin = randomForest(costs_bin ~ ., data = train.X)
  
  pred = predict(bag.costs_bin, newdata=test.X, type="class")
  
  erates[i] = mean(pred != test.X$costs_bin)
}

erates

mean(erates)
```

## Boosting (including selecting the tuning parameter)

(For the above three methods, calculate the Gini index on each leaf of the final tree to examine the purity of the node.)

```{r echo=FALSE}
library(gbm)
set.seed(1)

train.X = hpc_scaled |> select(-total_costs)
test.X = hpc_scaled |> select(-total_costs)

boost.costs_bin = gbm(costs_bin ~ ., data = train.X, distribution = "bernoulli", n.trees = 5000, interaction.depth = 4)

pred = round(predict(boost.costs_bin, newdata=test.X, type="response"))
```

## Neural Network

```{r echo=TRUE}
library(keras)

modelnn <- keras_model_sequential()
modelnn %>%
  layer_dense(units = 256, activation = "relu",
              input_shape = 784) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax")
```

# Bootstrap SEs

Our bootstrap study looks at the standard errors of the coefficient estimates found through ridge regression, with 1000 bootstrap samples. Our data is scaled, which is why our standard errors are all approximately the same magnitude.

```{r echo=FALSE}
library(boot)
library(glmnet)

X = hpc_scaled |> select(-c("costs_bin", "set", "fold"))

B = 1000

coeffs = matrix(rep(0,B*13), nrow=B)

for (i in 1:B) {
  x = X[sample(nrow(x), nrow(x), replace = TRUE),]
  
  x = scale(model.matrix(total_costs ~ ., data = x)[ , -1])
  
  lambda_grid = 10 ^ seq(10, -2, length = 100)
  
  ridge_reg = cv.glmnet(x, hpc_scaled$total_costs,
                        alpha = 0, 
                        type.measure = "mse", 
                        lambda = lambda_grid, 
                        nfolds=10)
  
  lambda = ridge_reg$lambda.min
  
  ridge_model = glmnet(x, hpc_scaled$total_costs,
                       alpha = 0,
                       lambda = lambda,
                       nfolds=10)
  
  for (j in 1:13)
 coeffs[i,j] = coef(ridge_model)[j]
}

coeffs_mean = coeffs |> as_tibble() |> summarize(across(V1:V13,mean))

se = rep(0,13)

for (i in 1:13) {
  se[i] = sqrt((1/(B-1))*sum((coeffs[,1]) -coeffs_mean[i])^2)
}

se = as.matrix(se, ncol=1)

row.names(se) = dimnames(coef(ridge_model))[[1]]
colnames(se) = c("Bootstrap Standard Error Estimate")

se
```

# Simulation Study

```{r echo=FALSE}
set.seed(1)
library(tidyverse)
library(faux)

df =  hpc_scaled |> select(-c(provider_bin_Specialized, control_bin_Governmental, control_bin_Proprietary, costs_bin, rural, set, fold))

N = 10

min_K = c(N,0)

for (i in 1:N)
  sim = sim_df(df, 1000)
  
  cat_df =  hpc_scaled |> select(c(provider_bin_Specialized, control_bin_Governmental, control_bin_Proprietary, rural))
  
  p = colMeans(cat_df)
  
  sim_cat = data.frame(matrix(nrow = 1000, ncol = 0))
  
  for (i in colnames(cat_df)) {
    sim_cat[,i] = rbinom(1000, 1, p=p[i])
  }
  
  sim = cbind(sim,sim_cat)
  
  train_prop = 0.9
  
  n = nrow(sim)
  n_train = train_prop*n
  n_test = n - n_train
  
  sim$set = "Train"
  sim$set[sample(n, n_test, replace = FALSE)] = "Test"
  
  sim = sim |> select(-id)
  
  train.X = sim |> select(-c(costs_bin,total_costs))
  test.X = df_test |> select(-c(costs_bin, total_costs))
  
  K = 25
  
  erates = rep(0,K)
  
  for (i in 1:K) {
    knn.pred = knn(train.X, test.X, df_train$costs_bin, k = i)
    erates[i] = mean(knn.pred != df_test$costs_bin)
  }
  
  min_K[i] = which.min(erates)
}
  
min_K
```
